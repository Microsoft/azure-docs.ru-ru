---
title: Сбор событий потоковой передачи — Центры событий Azure | Документация Майкрософт
description: В этой статье описывается функция сбора, которая позволяет записывать события потоковой передачи из Центров событий Azure
ms.topic: article
ms.date: 02/16/2021
ms.openlocfilehash: 9f0ec1223c06b908a9aa9f3ac5c5b19ead2fe962
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/20/2021
ms.locfileid: "100595953"
---
# <a name="capture-events-through-azure-event-hubs-in-azure-blob-storage-or-azure-data-lake-storage"></a>Сбор событий из Центров событий Azure в хранилище BLOB-объектов Azure или Azure Data Lake Storage
Центры событий Azure позволяют автоматически записывать определенный объем потоковых данных из Центров событий в учетную запись [хранилища BLOB-объектов Azure](https://azure.microsoft.com/services/storage/blobs/) или [Azure Data Lake Storage Gen 1 или Gen 2](https://azure.microsoft.com/services/data-lake-store/) с указанным интервалом времени и размера. Настройка функции "Сбор" выполняется быстро, ее использование не влечет дополнительных административных расходов, а масштабирование осуществляется автоматически на основе [единиц пропускной способности](event-hubs-scalability.md#throughput-units) Центров событий. Функция "Сбор" в Центрах событий — это самый удобный способ передачи потоковых данных в Azure. Он позволяет сосредоточиться на обработке данных, а не на их записи.

> [!NOTE]
> Настройка записи центров событий для использования хранилища Azure Data Lake Storage **Gen 2** совпадает с настройкой для использования хранилища BLOB-объектов Azure. Дополнительные сведения см. в разделе [Настройка записи центров событий](event-hubs-capture-enable-through-portal.md). 

Кроме того, функция "Сбор" в Центрах событий обеспечивает обработку конвейеров в режиме реального времени и на основе пакетов в одном потоке, благодаря чему вы можете создавать решения, масштабируемые по мере необходимости. Независимо от того, что вам нужно (создать системы на основе пакетов с учетом будущих потребностей в обработке данных в режиме реального времени или добавить эффективный холодный путь к имеющемуся решению для обработки в режиме реального времени), функция "Сбор" в Центрах событий упрощает работу с потоковыми данными.

> [!IMPORTANT]
> Учетная запись целевой службы хранилища (Служба хранилища Azure или Azure Data Lake Storage) должна находиться в той же подписке, что и концентратор событий. 

## <a name="how-event-hubs-capture-works"></a>Как работает функция "Сбор" в Центрах событий

Центры событий — это устойчивый буфер для хранения входящих данных телеметрии в течение определенного времени, подобный распределенному журналу. Масштабирование в Центрах событий выполняется в рамках [модели секционированных потребителей](event-hubs-scalability.md#partitions). Каждая секция — это независимый сегмент данных, потребление которого осуществляется отдельно. По истечении настроенного срока хранения эти данные устаревают, поэтому определенный концентратор событий никогда не заполняется полностью.

Функция "Сбор" в Центрах событий позволяет указать собственную учетную запись хранилища BLOB-объектов Azure и контейнер, или учетную запись Azure Data Lake Store, используемые для хранения собранных данных. Эти учетные записи могут находиться в том же регионе, что и концентратор событий, или в другом. Это дает гибкие возможности использования функции "Сбор" в Центрах событий.

Собранные данные записываются в формате [Apache Avro][Apache Avro] — сжатый быстрый двоичный формат, обеспечивающий эффективную структуру данных за счет встроенной схемы. Этот формат широко используется в экосистеме Hadoop, Stream Analytics и фабрике данных Azure. Работа с Avro более подробно описана далее в этой статье.

### <a name="capture-windowing"></a>Управление окнами в записи

В функции "Сбор" в Центрах событий можно настроить окно управления сбором. Это окно с минимальным размером и продолжительностью, для которого предусмотрена политика "побеждает первый". Это означает, что первый обнаруженный триггер активирует операцию записи. При наличии окна записи размером в 100 МБ и продолжительностью 15 минут для отправки данных со скоростью 1 МБ/с сначала используется окно размера, а затем — окно времени. Запись каждой секции выполняется отдельно, а запись выполненного блочного BLOB-объекта осуществляется в процессе записи. Имя блочного BLOB-объекта зависит от времени создания записи. Соглашение об именовании хранилища выглядит следующим образом:

```
{Namespace}/{EventHub}/{PartitionId}/{Year}/{Month}/{Day}/{Hour}/{Minute}/{Second}
```

Значения даты дополняются нулями. Пример имени файла может выглядеть так:

```
https://mystorageaccount.blob.core.windows.net/mycontainer/mynamespace/myeventhub/0/2017/12/08/03/03/17.avro
```

В случае если двоичный объект из хранилища Azure временно недоступен, запись центров событий сохраняет данные в течение срока хранения данных, настроенного в центре событий, и заполняет данные задним числом, когда учетная запись хранения снова станет доступной.

### <a name="scaling-to-throughput-units"></a>Масштабирование единиц пропускной способности

Трафик Центров событий контролируется с помощью [единиц пропускной способности](event-hubs-scalability.md#throughput-units). Одна единица пропускной способности разрешает передачу до 1 МБ/с или 1000 событий/с для входящих данных или до 2 МБ/с или 2000 событий/с для исходящих данных. Для Центров событий (цен. категория "Стандартный") можно настроить от 1 до 20 единиц пропускной способности. Кроме того, можно отправить запрос на увеличение квоты в [службу поддержки][support request]. Использование единиц пропускной способности свыше приобретенного количества регулируется. Функция "Сбор" в Центрах событий копирует данные непосредственно из внутреннего хранилища Центров событий. При этом выполняется обход квоты на единицы пропускной способности для исходящего трафика, а этот трафик сохраняется для других средств обработки, например Stream Analytics или Spark.

После настройки функция "Сбор" в Центрах событий автоматически запускается при отправке первого события и продолжает работать. Чтобы позволить операции последующей обработки установить, что процесс выполняется, при отсутствии данных Центры событий записывают пустые файлы. Этот процесс обеспечивает прогнозируемую периодичность и позволяет получить маркер, необходимый для пакетных обработчиков.

## <a name="setting-up-event-hubs-capture"></a>Настройка функции "Сбор" в Центрах событий

Запись можно настроить при создании концентратора событий с помощью [портала Azure](https://portal.azure.com) или с помощью шаблонов Azure Resource Manager. Дополнительные сведения см. в следующих статьях:

- [Включение функции "Сбор" в Центрах событий с помощью портала Azure](event-hubs-capture-enable-through-portal.md)
- [Создание пространства имен Центров событий с концентратором событий и включение записи с помощью шаблона Azure Resource Manager](event-hubs-resource-manager-namespace-event-hub-enable-capture.md)

> [!NOTE]
> Если включить функцию записи для существующего концентратора событий, компонент захватывает события, поступающие в концентратор событий **после** включения компонента. Он не фиксирует события, которые существовали в концентраторе событий до включения компонента. 

## <a name="exploring-the-captured-files-and-working-with-avro"></a>Просмотр собранных файлов и работа с Avro

Функция "Сбор" в Центрах событий создает файлы в формате Avro, как указано в настроенном окне времени. Эти файлы можно просмотреть в любом средстве, например в [обозревателе хранилищ Azure][Azure Storage Explorer]. Чтобы выполнить определенные действия с этими файлами, их можно скачать локально.

Файлы, созданные записью с помощью функции "Сбор" в Центрах событий, имеют следующую схему Avro.

![Схема Avro][3]

Файлы Avro можно легко просмотреть с помощью [средств Avro][Avro Tools] (JAR-файл) из Apache. Вы можете также использовать [Apache Drill][Apache Drill] для облегченного взаимодействия с SQL или [Apache Spark][Apache Spark], чтобы выполнить сложную распределенную обработку принятых данных. 

### <a name="use-apache-drill"></a>Использование Apache Drill

[Apache Drill][Apache Drill] — "механизм SQL-запросов с открытым исходным кодом для исследования больших данных", который может запрашивать структурированные и полуструктурированные данные, независимо от расположения. Этот механизм может работать как отдельный узел или как огромный кластер для высокой производительности.

Доступна встроенная поддержка хранилища BLOB-объектов Azure, которая упрощает запрос данных в файле Avro, как описано в документации:

[Apache Drill. Подключаемый модуль хранилища BLOB-объектов Azure][Apache Drill: Azure Blob Storage Plugin]

Чтобы легко запросить захваченные файлы, можно создать и выполнить виртуальную машину с поддержкой Apache с помощью контейнера для доступа к хранилищу BLOB-объектов Azure. См. Следующий пример: [потоковая передача в масштабе с записью концентраторов событий](https://github.com/Azure-Samples/streaming-at-scale/tree/main/eventhubs-capture).

### <a name="use-apache-spark"></a>Использование Apache Spark

[Apache Spark][Apache Spark] — это "единый аналитический механизм для крупномасштабной обработки данных". Он поддерживает разные языки, включая SQL, и может легко связываться с хранилищем BLOB-объектов Azure. Существует несколько вариантов для запуска Apache Spark в Azure, и каждый обеспечивает легкий доступ к хранилищу BLOB-объектов Azure:

- [HDInsight. Обращение к файлам в службе хранилища Azure][HDInsight: Address files in Azure storage]
- [Azure Databricks. Хранилище BLOB-объектов Azure][Azure Databricks: Azure Blob Storage]
- [Служба Azure Kubernetes (AKS)](../aks/spark-job.md) 

### <a name="use-avro-tools"></a>Использование средств Avro

[Средства Avro][Avro Tools] доступны в виде пакета JAR. После того как вы загрузили этот JAR-файл, чтобы просмотреть схему определенного файла Avro, выполните следующую команду:

```shell
java -jar avro-tools-1.9.1.jar getschema <name of capture file>
```

Эта команда возвращает следующее:

```json
{

    "type":"record",
    "name":"EventData",
    "namespace":"Microsoft.ServiceBus.Messaging",
    "fields":[
                 {"name":"SequenceNumber","type":"long"},
                 {"name":"Offset","type":"string"},
                 {"name":"EnqueuedTimeUtc","type":"string"},
                 {"name":"SystemProperties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Properties","type":{"type":"map","values":["long","double","string","bytes"]}},
                 {"name":"Body","type":["null","bytes"]}
             ]
}
```

Средства Avro можно также использовать для преобразования файлов в формат JSON и выполнения других задач обработки.

Чтобы выполнить более расширенную обработку, скачайте и установите Avro для определенной платформы. На момент написания статьи средства Avro доступны для следующих платформ: C, C++, C\#, Java, NodeJS, Perl, PHP, Python и Ruby.

Apache Avro предоставляет руководства по началу работы для платформ [Java][Java] и [Python][Python]. Дополнительные сведения см. в статье [Пошаговое руководство. Использование функции "Сбор" в Центрах событий с Python](event-hubs-capture-python.md).

## <a name="how-event-hubs-capture-is-charged"></a>Выставление счета за использование функции "Сбор" в Центрах событий

Выставление счета за использование функции "Сбор" в Центрах событий осуществляется подобно тарификации за единицы пропускной способности, то есть каждый час. Размер платы прямо пропорционален количеству единиц пропускной способности, приобретенных для пространства имен. Так же как и с единицами пропускной способности, единицы измерения при использовании функции "Сбор" в Центрах событий можно регулировать, чтобы обеспечить соответствующую производительность. Единицы измерения действуют совместно. Дополнительные сведения о ценах см. на странице цен на [Центры событий](https://azure.microsoft.com/pricing/details/event-hubs/). 

Запись не потребляет квоту исходящего трафика, так как она оплачивается отдельно. 

## <a name="integration-with-event-grid"></a>Интеграция со службой "Сетка событий" 

Можно создать подписку на Сетку событий Azure с пространством имен Центров событий в качестве источника. В следующем руководстве показано, как создать подписку службы "Сетка событий" с концентратором событий в качестве источника и приложение "функции Azure" в качестве приемника: [обработать и перенести захваченные данные концентраторов событий в Azure синапсе Analytics с помощью службы "Сетка событий" и функций Azure](store-captured-data-data-warehouse.md).

## <a name="next-steps"></a>Дальнейшие действия
Функция "Сбор" в Центрах событий — это самый быстрый способ передать данные в Azure. С помощью знакомых средств и платформ (Azure Data Lake, фабрики данных Azure и Azure HDInsight) можно выполнять необходимую пакетную обработку и другие операции анализа в любом масштабе.

Узнайте, как включить эту функцию с помощью портала Azure и шаблона Azure Resource Manager:

- [Использование портала Azure для включения функции "Сбор" в Центрах событий](event-hubs-capture-enable-through-portal.md)
- [Использование шаблона Resource Manager Azure для включения функции "Сбор" в Центрах событий](event-hubs-resource-manager-namespace-event-hub-enable-capture.md)


[Apache Avro]: https://avro.apache.org/
[Apache Drill]: https://drill.apache.org/
[Apache Spark]: https://spark.apache.org/
[support request]: https://portal.azure.com/?#blade/Microsoft_Azure_Support/HelpAndSupportBlade
[Azure Storage Explorer]: https://github.com/microsoft/AzureStorageExplorer/releases
[3]: ./media/event-hubs-capture-overview/event-hubs-capture3.png
[Avro Tools]: https://downloads.apache.org/avro/stable/java/
[Java]: https://avro.apache.org/docs/current/gettingstartedjava.html
[Python]: https://avro.apache.org/docs/current/gettingstartedpython.html
[Event Hubs overview]: ./event-hubs-about.md
[HDInsight: Address files in Azure storage]:https://docs.microsoft.com/azure/hdinsight/hdinsight-hadoop-use-blob-storage
[Azure Databricks: Azure Blob Storage]:https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[Apache Drill: Azure Blob Storage Plugin]:https://drill.apache.org/docs/azure-blob-storage-plugin/
[Streaming at Scale: Event Hubs Capture]:https://github.com/yorek/streaming-at-scale/tree/master/event-hubs-capture
