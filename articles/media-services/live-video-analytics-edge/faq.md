---
title: Интерактивная аналитика видео на IoT Edge часто задаваемые вопросы по Azure
description: В этой статье содержатся ответы на часто задаваемые вопросы о службе Live Video Analytics на IoT Edge.
ms.topic: conceptual
ms.date: 12/01/2020
ms.openlocfilehash: 0cb378bf614582070dd1bdd0a11706b26437af53
ms.sourcegitcommit: aaa65bd769eb2e234e42cfb07d7d459a2cc273ab
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/27/2021
ms.locfileid: "98880056"
---
# <a name="live-video-analytics-on-iot-edge-faq"></a>Интерактивная аналитика видео на IoT Edge часто задаваемые вопросы

В этой статье содержатся ответы на часто задаваемые вопросы о службе Live Video Analytics на Azure IoT Edge.

## <a name="general"></a>Общие сведения

**Какие системные переменные можно использовать в определении топологии графа?**

| Переменная   |  Описание  | 
| --- | --- | 
| [System.DateTime](/dotnet/framework/data/adonet/sql/linq/system-datetime-methods) | Представляет время в формате UTC, обычно выраженное в виде даты и времени суток в следующем формате:<br>*ииииммддсхммссз* | 
| System. ПреЦиседатетиме | Представляет экземпляр времени в формате UTC (Дата-время) в формате, совместимом с файлом ISO8601, с миллисекундами в следующем формате:<br>*ГГГГММДДTччммсс недоступен. fffZ* | 
| System. Графтопологинаме   | Представляет топологию графа мультимедиа и содержит схему графа. | 
| System. Графинстанценаме |    Представляет экземпляр графа мультимедиа, содержит значения параметров и ссылается на топологию. | 

## <a name="configuration-and-deployment"></a>Конфигурация и развертывание

**Можно ли развернуть модуль Media ребра на устройстве с Windows 10?**

Да. Дополнительные сведения см. [в разделе контейнеры Linux в Windows 10](/virtualization/windowscontainers/deploy-containers/linux-containers).

## <a name="capture-from-ip-camera-and-rtsp-settings"></a>Запись из IP-камеры и параметров RTSP

**Нужно ли использовать специальный пакет SDK на устройстве для отправки видеопотока?**

Нет, интерактивная аналитика видео на IoT Edge поддерживает запись носителей с помощью протокола передачи данных (потоковая передача в реальном времени) для потоковой передачи видео, который поддерживается большинством IP-камер.

**Можно ли передавать мультимедиа в Live Video Analytics на IoT Edge с помощью протокола Real-Timeа обмена сообщениями (RTMP) или протокола Smooth Streaming (например, события служб мультимедиа Live)?**

Нет, в реальном времени Video Analytics поддерживается только протокол RTSP для записи видео с IP-камер. Должна работать любая камера, поддерживающая потоковую передачу RTSP по протоколу TCP/HTTP. 

**Можно ли сбросить или обновить URL-адрес источника RTSP в экземпляре Graph?**

Да, если экземпляр графа находится в *неактивном* состоянии.  

**Доступен ли симулятор RTSP для использования во время тестирования и разработки?**

Да, модуль пограничной [симулятора RTSP](https://github.com/Azure/live-video-analytics/tree/master/utilities/rtspsim-live555) можно использовать в кратких руководствах и учебных курсах для поддержки процесса обучения. Этот модуль предоставляется как оптимальный и может быть не всегда доступен. Мы рекомендуем *не* использовать симулятор в течение нескольких часов. Перед планированием рабочего развертывания следует протестировать тестирование с использованием действительного источника RTSP.

**Поддерживается ли обнаружение IP-камер на пограничном устройстве с помощью ONVIF?**

Нет, мы не поддерживаем обнаружение устройств на пограничных устройствах с открытым сетевым видео (ОНВИФ).

## <a name="streaming-and-playback"></a>Потоковая передача и воспроизведение

**Можно ли воспроизвести ресурсы, записанные в службы мультимедиа Azure из пограничных технологий, используя такие технологии потоковой передачи, как HLS или ТИРЕ?**

Да. Вы можете выполнять потоковую передачу записанных ресурсов, как и любой другой ресурс в службах мультимедиа Azure. Чтобы выполнить потоковую передачу содержимого, необходимо создать конечную точку потоковой передачи и в состоянии выполнения. При использовании стандартного процесса создания указателя потоковой передачи вы получите доступ к HTTP Live Streamingу Apple (HLS) или динамической адаптивной потоковой передачи по протоколу HTTP (ТИРЕ, также известному как MPEG-ТИРЕ) для потоковой передачи на любую поддерживающую платформу проигрывателя. Дополнительные сведения о создании и публикации манифестов HLS или ТИРЕ см. в разделе [Динамическая упаковка](../latest/dynamic-packaging-overview.md).

**Можно ли использовать стандартные функции защиты содержимого и DRM для служб мультимедиа в архивном ресурсе?**

Да. Все стандартные функции защиты содержимого динамического шифрования и управления цифровыми правами (DRM) доступны для использования в ресурсах, записанных в графе мультимедиа.

**Какие игроки можно использовать для просмотра содержимого из записанных ресурсов?**

Поддерживаются все стандартные проигрыватели, поддерживающие совместимые HLS версии 3 или 4. Кроме того, поддерживается любой игрок, поддерживающий воспроизведение с поддержкой MPEG-ТИРЕ.

Рекомендуемые игроки для тестирования включают:

* [Проигрыватель мультимедиа Azure](../latest/use-azure-media-player.md)
* [HLS.js](https://hls-js.netlify.app/demo/)
* [Video.js](https://videojs.com/)
* [Dash.js](https://github.com/Dash-Industry-Forum/dash.js/wiki)
* [Проигрыватель Шака](https://github.com/google/shaka-player)
* [ExoPlayer](https://github.com/google/ExoPlayer)
* [Собственный HTTP Live Streaming Apple](https://developer.apple.com/streaming/)
* Встроенный видеопроигрыватель HTML5, Chrome или Safari
* Коммерческие игроки, поддерживающие воспроизведение HLS или ТИРЕ

**Каковы ограничения на потоковую передачу ресурса графа мультимедиа?**

Потоковая передача активного или записанного ресурса из графа мультимедиа использует ту же высокомасштабируемый инфраструктурой и конечную точку потоковой передачи, которую службы мультимедиа поддерживает по запросу и потоковую передачу мультимедиа & развлечениям, а также по началу (OTT) и рассылке клиентов. Это означает, что вы можете быстро и просто включить сеть доставки содержимого Azure, Verizon или Akamai, чтобы передавать содержимое в аудиторию как небольшое число посетителей или до миллионов в зависимости от вашего сценария.

Вы можете доставлять содержимое с помощью Apple HLS или MPEG-ТИРЕ.

## <a name="design-your-ai-model"></a>Проектирование модели искусственного интеллекта 

**В контейнере DOCKER есть несколько моделей искусственного интеллекта. Как использовать их с помощью функции Live Video Analytics?** 

Решения могут различаться в зависимости от протокола связи, используемого сервером для обмена данными с функцией Live Video Analytics. В следующих разделах описывается, как работает каждый протокол.

*Использовать протокол HTTP*:

* Один контейнер (один Лваекстенсион):  

   На используемом сервере можно использовать один порт, но разные конечные точки для различных моделей искусственного интеллекта. Например, для примера Python можно использовать разные `route` s для каждой модели, как показано ниже: 

   ```
   @app.route('/score/face_detection', methods=['POST']) 
   … 
   Your code specific to face detection model… 

   @app.route('/score/vehicle_detection', methods=['POST']) 
   … 
   Your code specific to vehicle detection model 
   … 
   ```

   Затем в развертывании Live Video Analytics при создании экземпляра графов задайте URL-адрес сервера вывода для каждого экземпляра, как показано ниже: 

   Первый экземпляр: URL-адрес сервера вывода =`http://lvaExtension:44000/score/face_detection`<br/>
   второй экземпляр: URL-адрес сервера вывода =`http://lvaExtension:44000/score/vehicle_detection`  
   
    > [!NOTE]
    > Кроме того, можно предоставлять модели искусственного интеллекта на разных портах и вызывать их при создании экземпляров графов.  

* Несколько контейнеров: 

   Каждый контейнер развертывается с другим именем. Ранее в наборе документации по Video Analytics мы показали, как развернуть расширение с именем *лваекстенсион*. Теперь можно разрабатывать два разных контейнера с одним и тем же интерфейсом HTTP, что означает, что они имеют одну и ту же `/score` конечную точку. Разверните эти два контейнера с разными именами и убедитесь, что оба они прослушивают *разные порты*. 

   Например, один контейнер с именем `lvaExtension1` прослушивает порт `44000` , а второй контейнер с именем `lvaExtension2` прослушивает порт `44001` . 

   В топологии Live Video Analytics создаются два графа с разными URL-адресами вывода, как показано ниже: 

   Первый экземпляр: URL-адрес сервера вывода = `http://lvaExtension1:44001/score`    
   Второй экземпляр: URL-адрес сервера вывода = `http://lvaExtension2:44001/score`
   
*Используйте протокол gRPC*: 

* Если используется модуль Live Video Analytics 1,0, то при использовании протокола общего назначения удаленной процедуры (gRPC) единственным способом сделать это является то, что сервер gRPC предоставляет различные модели AI через разные порты. В [этом примере кода](https://raw.githubusercontent.com/Azure/live-video-analytics/master/MediaGraph/topologies/grpcExtension/topology.json)один порт 44000 предоставляет доступ ко всем моделям Йоло. Теоретически сервер Йоло gRPC мог быть переписан, чтобы предоставить некоторые модели через порт 44000, а другие — через порт 45000. 

* При использовании модуля Live Video Analytics 2,0 в узел расширения gRPC добавляется новое свойство. Это свойство, **екстенсионконфигуратион**, является необязательной строкой, которая может использоваться как часть контракта gRPC. При наличии нескольких моделей искусственного интеллекта, упакованных в один сервер вывода, не нужно предоставлять узел для каждой модели искусственного интеллекта. Вместо этого для экземпляра Graph вы, как поставщик расширений, может определить способ выбора различных моделей AI с помощью свойства **екстенсионконфигуратион** . Во время выполнения Live Video Analytics передает эту строку на сервер, который может использовать его для вызова требуемой модели искусственного интеллекта. 

**Я создаю сервер gRPC на основе модели искусственного интеллекта и хочу поддерживать ее использование несколькими камерами или экземплярами графов. Как создать сервер?** 

 Во-первых, убедитесь, что сервер может обрабатывать более одного запроса за раз или работать в параллельных потоках. 

Например, в следующем [примере gRPC для анализа видео в реальном времени](https://github.com/Azure/live-video-analytics/blob/master/utilities/video-analysis/notebooks/Yolo/yolov3/yolov3-grpc-icpu-onnx/lvaextension/server/server.py)задано количество параллельных каналов по умолчанию: 

```
server = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) 
```

В предыдущем экземпляре сервера gRPC сервер может открывать только три канала за раз на камеру или на экземпляр топологии графа. Не пытайтесь подключить к серверу более трех экземпляров. Если вы попытаетесь открыть более трех каналов, запросы будут ожидать до тех пор, пока существующий канал не будет сброшен.  

Предыдущая реализация сервера gRPC используется в примерах Python. Разработчик может реализовать собственный сервер или использовать предыдущую реализацию по умолчанию, чтобы увеличить номер рабочего процесса, для которого задано количество камер, используемых для видеоканалов. 

Чтобы настроить и использовать несколько камер, можно создать экземпляры нескольких экземпляров топологии графов, каждый из которых указывает на один и тот же или другой сервер вывода (например, сервер, упомянутый в предыдущем абзаце). 

**Я хочу иметь возможность получать несколько кадров из вышестоящей части, прежде чем принимать решение о принятии решения. Как это можно сделать?** 

Наши текущие [образцы по умолчанию](https://github.com/Azure/live-video-analytics/tree/master/utilities/video-analysis) работают в режиме *без отслеживания состояния* . Они не сохраняют состояние предыдущих вызовов или даже те, кто вызвал. Это означает, что несколько экземпляров топологии могут вызывать один и тот же сервер вывода, но сервер не может отличать, кто вызывает, или состояние на каждого вызывающего. 

*Использовать протокол HTTP*:

Чтобы обеспечить состояние, каждый вызывающий объект или экземпляр топологии графа, вызывает сервер для обращения с помощью параметра HTTP-запроса, уникального для вызывающего. Например, здесь показаны URL-адреса сервера вывода для каждого экземпляра:  

экземпляр 1-го топологии = `http://lvaExtension:44000/score?id=1`<br/>
экземпляр второй топологии = `http://lvaExtension:44000/score?id=2`

… 

На стороне сервера маршрут оценки знает, кто вызывает. Если ID = 1, он может синхронизировать состояние отдельно для этого вызывающего объекта или экземпляра топологии графа. Затем полученные видеоматериалы можно разместить в буфере. Например, используйте массив или словарь с ключом DateTime, а значение — кадром. После этого можно определить сервер для обработки (Infer) после того, как будет получено *x* -число кадров. 

*Используйте протокол gRPC*: 

С расширением gRPC каждый сеанс предназначен для одного канала камеры, поэтому нет необходимости указывать идентификатор. Теперь с помощью свойства Екстенсионконфигуратион можно сохранить видеокадры в буфере и определить сервер для обработки (Infer) после того, как будет получено *x* -число кадров. 

**Все Процессмедиастреамс в определенном контейнере выполняют одну и ту же модель искусственного интеллекта?** 

Нет. Вызовы запуска или отмены от конечного пользователя в экземпляре Graph составляют сеанс, или может быть отключена или пересоединена камера. Цель состоит в том, чтобы сохранить один сеанс, если камера является потоковым видео. 

* Две камеры, отправляющие видео для обработки, создают два сеанса. 
* Одна камера, которая переходит на диаграмму с двумя узлами расширения gRPC, создает два сеанса. 

Каждый сеанс является полным дуплексным подключением между интерактивной видеоаналитикой и сервером gRPC, и каждый сеанс может иметь другую модель или конвейер. 

> [!NOTE]
> В случае отключения или повторного подключения камеры, когда камера переходит в автономный режим за период, превышающий допустимые пределы, в реальном времени для видео будет открыт новый сеанс с сервером gRPC. На сервере нет необходимости отслеживанию состояния в рамках этих сеансов. 

В реальном времени Video Analytics также добавлена поддержка нескольких расширений gRPC для одной камеры в экземпляре Graph. Эти расширения gRPC можно использовать для последовательного выполнения обработки искусственного интеллекта, параллельно или в виде их комбинации. 

> [!NOTE]
> Параллельное выполнение нескольких расширений повлияет на ресурсы оборудования. Помните об этом, так как вы выбрали оборудование, которое подходит для вычислительных потребностей. 

**Каково максимальное число одновременных Процессмедиастреамс?** 

В реальном времени для видео не применяются ограничения на этот номер.  

**Как определить, следует ли использовать ЦП или GPU или любой другой ускоритель оборудования?** 

Ваше решение зависит от сложности разработанной модели искусственного интеллекта и от того, как вы хотите использовать ускорители ЦП и оборудования. При разработке модели искусственного интеллекта можно указать, какие ресурсы должна использовать модель и какие действия она должна выполнять. 

**Разделы справки сохранять изображения с ограничивающими прямоугольниками после обработки?** 

Сейчас мы предоставляем координаты ограничивающего прямоугольника только в сообщениях вывода. Вы можете создать пользовательский поток МЖПЕГ, который сможет использовать эти сообщения и наложение ограничивающих прямоугольников в видеокадрах.  

## <a name="grpc-compatibility"></a>совместимость gRPC 

**Как определить обязательные поля для дескриптора потока мультимедиа?** 

Любому полю, которому не предоставляется значение, присваивается [значение по умолчанию, как указано в gRPC](https://developers.google.com/protocol-buffers/docs/proto3#default).  

Функция Live Video Analytics использует *proto3* версию языка буфера протокола. Все данные буфера протокола, используемые контрактами в реальном времени, доступны в [буферных файлах протокола](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc). 

**Как убедиться, что используются последние файлы буфера протокола?** 

Последние файлы буфера протокола можно получить на [сайте файлов контрактов](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc). При обновлении файлов контракта они будут находиться в этом расположении. Нет немедленного планового обновления файлов протокола, поэтому найдите имя пакета в верхней части файлов, чтобы узнать версию. Должно быть: 

```
microsoft.azure.media.live_video_analytics.extensibility.grpc.v1 
```

Все обновления этих файлов увеличивают "v-value" в конце имени. 

> [!NOTE]
> Так как в реальном видео-аналитике используется proto3 версия языка, поля являются необязательными, а версия совместима с обратной и прямой совместимостью. 

**Какие функции gRPC доступны для использования в службе Live Video Analytics? Какие функции являются обязательными, а какие — необязательными?** 

Вы можете использовать любые серверные функции gRPC при условии, что контракт «буферы протоколов» (protobuf) выполнен. 

## <a name="monitoring-and-metrics"></a>Мониторинг и метрики

**Можно ли отслеживать граф мультимедиа на пограничном устройстве с помощью службы "Сетка событий Azure"?**

Да. Вы можете использовать метрики Prometheus и публиковать их в сетке событий. 

**Можно ли использовать Azure Monitor для просмотра работоспособности, метрик и производительности графов мультимедиа в облаке или на границе?**

Да, мы поддерживаем этот подход. Дополнительные сведения см. в разделе [Общие сведения о метриках Azure Monitor](../../azure-monitor/platform/data-platform-metrics.md).

**Существуют ли инструменты, облегчающие мониторинг модуля IoT Edge служб мультимедиа?**

Visual Studio Code поддерживает расширение средств Azure IoT, с помощью которого можно легко отслеживать конечные точки модуля Лваедже. С помощью этого средства можно быстро начать наблюдение за встроенной конечной точкой центра Интернета вещей для "событий" и просмотреть сообщения о выводе, направляемые с пограничным устройством в облако. 

Кроме того, это расширение можно использовать для редактирования модуля двойника для модуля Лваедже, чтобы изменить параметры графа мультимедиа.

Дополнительные сведения см. в статье [мониторинг и ведение журнала](monitoring-logging.md) .

## <a name="billing-and-availability"></a>Выставление счетов и доступность

**Как выставляются счета за использование службы "Анализ видео в реальном времени" IoT Edge?**

Сведения о выставлении счетов см. на странице [цен на службы мультимедиа](https://azure.microsoft.com/pricing/details/media-services/).

## <a name="next-steps"></a>Дальнейшие действия

[Краткое руководство. Начало работы с аналитикой в реальном времени на IoT Edge](get-started-detect-motion-emit-events-quickstart.md)