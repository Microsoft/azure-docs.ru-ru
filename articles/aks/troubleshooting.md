---
title: Устранение распространенных проблем со Службой Azure Kubernetes
description: Узнайте, как устранить распространенные проблемы при использовании Службы Azure Kubernetes (AKS).
services: container-service
ms.topic: troubleshooting
ms.date: 06/20/2020
ms.openlocfilehash: 5a0e907ef27f125a9903b3d9e6079e3c8a288a97
ms.sourcegitcommit: c27a20b278f2ac758447418ea4c8c61e27927d6a
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/03/2021
ms.locfileid: "101714533"
---
# <a name="aks-troubleshooting"></a>Устранение неполадок с AKS

При создании кластеров Azure Kubernetes Service (AKS) или управлении ими иногда могут возникать проблемы. В этой статье описаны некоторые распространенные проблемы и действия по устранению неполадок.

## <a name="in-general-where-do-i-find-information-about-debugging-kubernetes-problems"></a>Где найти общие сведения об отладке проблем Kubernetes?

[Вот официальное руководство по устранению неполадок с кластерами Kubernetes](https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/).
[Это руководство по устранению неполадок](https://github.com/feiskyer/kubernetes-handbook/blob/master/en/troubleshooting/index.md) модулей pod, узлов, кластеров и т. д., опубликованное инженером Майкрософт.

## <a name="im-getting-a-quota-exceeded-error-during-creation-or-upgrade-what-should-i-do"></a>`quota exceeded`Произошла ошибка во время создания или обновления. Что делать? 

 [Запросить еще ядер](../azure-portal/supportability/resource-manager-core-quotas-request.md).

## <a name="im-getting-an-insufficientsubnetsize-error-while-deploying-an-aks-cluster-with-advanced-networking-what-should-i-do"></a>Произошла `insufficientSubnetSize` Ошибка при развертывании кластера AKS с расширенными сетевыми возможностями. Что следует делать?

Эта ошибка указывает, что подсеть, используемая для кластера, больше не имеет доступных IP-адресов в пределах CIDR для успешного назначения ресурсов. Для кластеров Кубенет требуется достаточное пространство IP-адресов для каждого узла в кластере. Для кластеров Azure CNI требуется достаточное пространство IP-адресов для каждого узла и Pod в кластере.
Узнайте больше о [проектировании Azure CNI для назначения IP-адресов для модулей](configure-azure-cni.md#plan-ip-addressing-for-your-cluster)Pod.

Эти ошибки также отображаются в средстве [диагностики AKS](concepts-diagnostics.md), которое заранее выводит на себя проблемы, например недостаточный размер подсети.

Следующие три варианта (3) вызывают ошибку недостаточного размера подсети:

1. Масштабирование пула узлов AKS или AKS
   1. При использовании Кубенет, если `number of free IPs in the subnet` меньше, **чем** `number of new nodes requested` .
   1. При использовании Azure CNI, если значение `number of free IPs in the subnet` **меньше, чем** `number of nodes requested times (*) the node pool's --max-pod value` .

1. Обновление пула узлов AKS или AKS
   1. При использовании Кубенет, если `number of free IPs in the subnet` меньше, **чем** `number of buffer nodes needed to upgrade` .
   1. При использовании Azure CNI, если значение `number of free IPs in the subnet` **меньше, чем** `number of buffer nodes needed to upgrade times (*) the node pool's --max-pod value` .
   
   По умолчанию кластеры AKS устанавливают значение максимального всплеска напряжения (в буфере обновления), равное одному (1), но это поведение можно настроить, задав [максимальное значение всплеска напряжения пула узлов, что увеличит количество доступных IP-адресов, необходимых для завершения обновления.

1. Добавление пула узлов AKS или AKS
   1. При использовании Кубенет, если `number of free IPs in the subnet` меньше, **чем** `number of nodes requested for the node pool` .
   1. При использовании Azure CNI, если значение `number of free IPs in the subnet` **меньше, чем** `number of nodes requested times (*) the node pool's --max-pod value` .

Для создания новых подсетей можно использовать следующие способы устранения рисков. Разрешение на создание новой подсети требуется для устранения рисков из-за невозможности обновить диапазон CIDR существующей подсети.

1. Перестройте новую подсеть с более крупным диапазоном CIDR, достаточным для целей операций:
   1. Создайте новую подсеть с новым нужным диапазоном, отличным от перекрытия.
   1. Создайте новый пул узлов в новой подсети.
   1. Очистка модулей Pod из старого пула узлов, находящегося в старой подсети для замены.
   1. Удалите старую подсеть и пул старых узлов.

## <a name="my-pod-is-stuck-in-crashloopbackoff-mode-what-should-i-do"></a>Мой модуль pod завис в режиме CrashLoopBackOff. Что делать?

Модуль pod может зависнуть в этом режиме по различным причинам. Вы можете просмотреть:

* сведения о самом модуле pod с помощью команды `kubectl describe pod <pod-name>`;
* сведения в журналах с помощью команды `kubectl logs <pod-name>`.

Дополнительные сведения об устранении неполадок модуля pod см. в статье об [отладке приложений](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods).

## <a name="im-receiving-tcp-timeouts-when-using-kubectl-or-other-third-party-tools-connecting-to-the-api-server"></a>Я получаю `TCP timeouts` при использовании `kubectl` или других сторонних средств, подключающихся к серверу API
AKS содержит плоскости управления высокой доступности, которые масштабируются по вертикали в соответствии с количеством ядер, чтобы гарантировать его цели уровня обслуживания (SLO) и соглашения об уровне обслуживания (SLA). Если время ожидания подключений истекло, проверьте следующее:

- **Истечет ли время ожидания команд API или только некоторые из них?** Если это всего лишь несколько, то есть `tunnelfront` модуль или модуль `aks-link` , отвечающий за обмен данными с плоскостью управления узлами >, может быть не в состоянии выполнения. Убедитесь, что узлы, на которых размещается этот модуль, не чрезмерно загружены или перегружены. Рассмотрите возможность перемещения их в собственный [ `system` пул узлов](use-system-pools.md).
- **Вы открыли все необходимые порты, полные доменные имена и IP-адреса, указанные в [документах AKS restrict исходящего трафика](limit-egress-traffic.md)?** В противном случае вызов нескольких команд может завершиться ошибкой.
- **Находится ли текущий IP-адрес, охваченный [IP-адресами разрешенных диапазонов API](api-server-authorized-ip-ranges.md)?** Если вы используете эту функцию и ваш IP-адрес не включен в диапазоны, ваши вызовы будут заблокированы. 
- **Имеются ли у клиента или приложения утечки вызовов к серверу API?** Обязательно используйте контрольные значения вместо частых вызовов Get, а сторонние приложения не вызывают такие вызовы. Например, ошибка в Istio микшере приводит к тому, что новое подключение к серверу API будет создаваться каждый раз при внутреннем чтении секрета. Так как такое поведение происходит через обычный интервал, Просмотр подключений быстро накапливается и, в конечном итоге, приводит к перегрузке сервера API независимо от шаблона масштабирования. https://github.com/istio/istio/issues/19481
- **У вас есть много выпусков в развертываниях Helm?** Этот сценарий может привести к тому, что оба сервера используют слишком много памяти на узлах, а также большой объем `configmaps` , что может привести к ненужным пиковым нагрузкам на сервере API. Рекомендуется настроить `--history-max` в `helm init` и использовать новый Helm 3. Дополнительные сведения о следующих проблемах: 
    - https://github.com/helm/helm/issues/4821
    - https://github.com/helm/helm/issues/3500
    - https://github.com/helm/helm/issues/4543
- **[Блокируется ли внутренний трафик между узлами?](#im-receiving-tcp-timeouts-such-as-dial-tcp-node_ip10250-io-timeout)**

## <a name="im-receiving-tcp-timeouts-such-as-dial-tcp-node_ip10250-io-timeout"></a>Я получаю `TCP timeouts` , например `dial tcp <Node_IP>:10250: i/o timeout`

Эти времена ожидания могут быть связаны с внутренним трафиком между заблокированными узлами. Убедитесь, что этот трафик не блокируется, например, по [группам безопасности сети](concepts-security.md#azure-network-security-groups) в подсети для узлов кластера.

## <a name="im-trying-to-enable-kubernetes-role-based-access-control-kubernetes-rbac-on-an-existing-cluster-how-can-i-do-that"></a>Я пытаюсь включить управление доступом на основе ролей Kubernetes (Kubernetes RBAC) в существующем кластере. Как это сделать?

Включение управления доступом на основе ролей Kubernetes (Kubernetes RBAC) в существующих кластерах сейчас не поддерживается, оно должно быть задано при создании новых кластеров. Kubernetes RBAC включен по умолчанию при использовании интерфейса командной строки, портала или версии API, более поздней, чем `2020-03-01` .

## <a name="i-cant-get-logs-by-using-kubectl-logs-or-i-cant-connect-to-the-api-server-im-getting-error-from-server-error-dialing-backend-dial-tcp-what-should-i-do"></a>Мне не удается получить журналы kubectl, или подключение к серверу API завершается сбоем. Выводится ошибка "Error from server: error dialing backend: dial tcp…" (На сервере возникла ошибка: ошибка вызова серверной части: вызывается протокол tcp...). Что делать?

Убедитесь, что порты 22, 9000 и 1194 открыты для подключения к серверу API. Проверьте, выполняется ли модуль `tunnelfront` или `aks-link` в пространстве имен *kube-system* с помощью команды `kubectl get pods --namespace kube-system`. Если он не запущен, принудительно удалите его, и он перезапустится.

## <a name="im-getting-tls-client-offered-only-unsupported-versions-from-my-client-when-connecting-to-aks-api-what-should-i-do"></a>Я получаю `"tls: client offered only unsupported versions"` клиент при подключении к API AKS. Что следует делать?

Минимальная поддерживаемая версия TLS в AKS — TLS 1,2.

## <a name="im-trying-to-upgrade-or-scale-and-am-getting-a-changing-property-imagereference-is-not-allowed-error-how-do-i-fix-this-problem"></a>При попытке обновления или масштабирования я получаю ошибку `"Changing property 'imageReference' is not allowed"`. Как устранить эту проблему?

Возможно, вы получаете эту ошибку, потому что изменили теги в узлах агента внутри кластера AKS. Изменение и удаление тегов и других свойств ресурсов в группе ресурсов MC_* может привести к непредвиденным результатам. Изменение ресурсов в группе MC_* кластера AKS нарушает цель уровня обслуживания (SLO).

## <a name="im-receiving-errors-that-my-cluster-is-in-failed-state-and-upgrading-or-scaling-will-not-work-until-it-is-fixed"></a>Я получаю ошибки, указывающие, что кластер находится в состоянии сбоя и что обновление или масштабирование не состоятся, пока это не будет исправлено

*Эта справка по устранению неполадок берется из https://aka.ms/aks-cluster-failed*

Эта ошибка возникает, когда кластеры переходят в состояние сбоя по нескольким причинам. Выполните следующие действия, чтобы устранить сбой кластера перед повторным выполнением операции, которая ранее не удалась.

1. Пока кластер не выйдет из состояния `failed`, операции `upgrade` и `scale` не будут выполняться. Ниже приведены общие основные проблемы и способы их устранения.
    * Масштабирование с **недостаточной квотой вычислений (CRP)** . Чтобы устранить эту проблему, сначала выполните масштабирование кластера до стабильного целевого состояния в пределах квоты. Затем выполните следующие [действия, чтобы запросить увеличение квоты вычислений](../azure-portal/supportability/resource-manager-core-quotas-request.md) перед попыткой увеличения масштаба после превышения первоначальных пределов квоты.
    * Масштабирование кластера с расширенными сетевым взаимодействием и **недостаточными ресурсами подсети (сетевого взаимодействия)** . Чтобы устранить эту проблему, сначала выполните масштабирование кластера до стабильного целевого состояния в пределах квоты. Затем выполните следующие [действия, чтобы запросить увеличение квоты ресурсов](../azure-resource-manager/templates/error-resource-quota.md#solution) перед попыткой увеличения масштаба после превышения первоначальных пределов квоты.
2. После устранения корневой проблемы с обновлением кластер должен находиться в состоянии "успешно". После проверки состояния "успешно" повторите исходную операцию.

## <a name="im-receiving-errors-when-trying-to-upgrade-or-scale-that-state-my-cluster-is-being-upgraded-or-has-failed-upgrade"></a>При попытке обновления или масштабирования возникли ошибки, указывающие, что выполняется обновление кластера или произошел сбой обновления

*Эта справка по устранению неполадок берется из https://aka.ms/aks-pending-upgrade*

 Кластер или пул узлов нельзя обновлять и масштабировать одновременно. Вместо этого каждый тип операции должен быть завершен на целевом ресурсе перед следующим запросом к этому же ресурсу. В результате операции ограничены при выполнении активных операций обновления, масштабирования или попытках выполнения таких операций. 

Чтобы помочь в диагностике проблемы, запустите `az aks show -g myResourceGroup -n myAKSCluster -o table` для получения подробных сведений о состоянии кластера. В зависимости от результата:

* Если кластер активно обновляется, дождитесь завершения операции. В случае успеха обновления повторите операцию, которая была выполнена ранее.
* Если произошел сбой обновления кластера, выполните действия, описанные в предыдущем разделе.

## <a name="can-i-move-my-cluster-to-a-different-subscription-or-my-subscription-with-my-cluster-to-a-new-tenant"></a>Можно ли переместить кластер в другую подписку или подписку с кластером к новому клиенту?

Если вы переместили кластер AKS в другую подписку или подписку кластера к новому клиенту, кластер не будет работать из-за отсутствия разрешений на удостоверение кластера. **AKS не поддерживает перемещение кластеров между подписками или клиентами** из-за этого ограничения.

## <a name="im-receiving-errors-trying-to-use-features-that-require-virtual-machine-scale-sets"></a>При попытке использования функций, требующих масштабируемых наборов виртуальных машин, возникают ошибки

*Эта справка по устранению неполадок берется из aka.ms/aks-vmss-enablement*

Могут возникать ошибки, указывающие, что кластер AKS не находится в масштабируемом наборе виртуальных машин, как показано в следующем примере:

**AgentPool `<agentpoolname>` включил автомасштабирование, однако не в Масштабируемых наборах виртуальных машин**

Для таких функций, как автомасштабирование кластера или пулы с несколькими узлами, в качестве `vm-set-type` требуются масштабируемые наборы виртуальных машин.

Выполните действия *Перед началом работы* из соответствующего документа, чтобы правильно создать кластер AKS:

* [Использование средства автомасштабирования кластера](cluster-autoscaler.md)
* [Использование нескольких пулов узлов](use-multiple-node-pools.md)
 
## <a name="what-naming-restrictions-are-enforced-for-aks-resources-and-parameters"></a>Какие ограничения именования применяются к ресурсам и параметрам AKS?

*Эта справка по устранению неполадок берется из aka.ms/aks-naming-rules*

Ограничения именования реализуются как платформой Azure, так и AKS. Если имя или параметр ресурса нарушает одно из этих ограничений, возвращается сообщение об ошибке, предлагающее указать другие входные данные. Применяются следующие общие рекомендации по именованию.

* Имена кластеров должны содержать 1–63 символа. Допустимыми являются только буквы, цифры, тире и символ подчеркивания. Первый и последний символ должны быть буквами или цифрами.
* Имя группы ресурсов "Узел AKS/*MC_* " объединяет имя группы ресурсов и имя ресурса. Автоматически сформированный синтаксис `MC_resourceGroupName_resourceName_AzureRegion` должен быть не длиннее 80 символов. При необходимости сократите длину имени группы ресурсов или имени кластера AKS. Вы также можете [модифицировать имя группы ресурсов узла.](cluster-configuration.md#custom-resource-group-name)
* *dnsPrefix* должны начинаться и заканчиваться буквенно-цифровыми значениями и содержать 1–54 символа. Допустимые символы включают в себя буквы, цифры и дефисы (-). *dnsPrefix* не может содержать специальные символы, такие как точка (.).
* Имена пулов узлов AKS должны быть целиком в нижнем регистре и состоять из 1–11 символов для пулов узлов Linux либо 1–6 символов для пулов узлов Windows. Имя должно начинаться с буквы, а допустимыми символами являются буквы и цифры.
* Имя *администратора, которое* задает имя администратора для узлов Linux, должно начинаться с буквы, может содержать только буквы, цифры, дефисы и символы подчеркивания и имеет максимальную длину 64 символов.

## <a name="im-receiving-errors-when-trying-to-create-update-scale-delete-or-upgrade-cluster-that-operation-is-not-allowed-as-another-operation-is-in-progress"></a>При попытке создать, обновить, масштабировать, удалить или обновить кластер возникают ошибки, указывающие, что операция запрещена, так как выполняется другая операция.

*Эта справка по устранению неполадок берется из aka.ms/aks-pending-operation*

Операции с кластером ограничены, если предыдущая операция все еще выполняется. Чтобы получить подробные сведения о состоянии кластера, используйте команду `az aks show -g myResourceGroup -n myAKSCluster -o table`. При необходимости укажите собственную группу ресурсов и имя кластера AKS.

На основе выходных данных состояния кластера:

* Если кластер находится в любом состоянии подготовки, кроме *Успешно* или *Сбой*, дождитесь завершения операции (*Обновление версии/Обновление/Создание/Масштабирование/Удаление/Миграция*). После завершения предыдущей операции повторите свою последнюю операцию с кластером.

* Если имел место сбой обновления кластера, выполните действия, обрисованные как [Я получаю ошибки, указывающие, что кластер находится в состоянии сбоя, и обновление или масштабирование не состоятся, пока это не будет исправлено](#im-receiving-errors-that-my-cluster-is-in-failed-state-and-upgrading-or-scaling-will-not-work-until-it-is-fixed).

## <a name="received-an-error-saying-my-service-principal-wasnt-found-or-is-invalid-when-i-try-to-create-a-new-cluster"></a>При попытке создать новый кластер выдается сообщение об ошибке, сообщающее, что субъект-служба не найдена или недопустима.

При создании кластера AKS для создания ресурсов от вашего имени требуются субъект-служба или управляемое удостоверение. AKS может автоматически создать новый субъект-службу во время создания кластера или получить существующий. При использовании автоматического создания Azure Active Directory необходимо распространить его на каждый регион, чтобы создание прошло успешно. Если распространение занимает слишком много времени, кластер не сможет выполнить проверку при создании, так как не сможет найти доступный субъект-службу. 

Для решения этой проблемы используйте следующие обходные пути.
* Используйте существующий субъект-службу, который уже распространен между регионами и существует для передачи в AKS во время создания кластера.
* При использовании скриптов автоматизации добавьте временные задержки между созданием субъекта-службы и созданием кластера AKS.
* Если используется портал Azure, вернитесь к параметрам кластера во время создания и обновите страницу проверки через несколько минут.

## <a name="im-getting-aadsts7000215-invalid-client-secret-is-provided-when-using-aks-api-what-should-i-do"></a>Я получаюсь `"AADSTS7000215: Invalid client secret is provided."` при использовании API AKS. Что следует делать?

Эта проблема вызвана истечением срока действия учетных данных субъекта-службы. [Обновите учетные данные для кластера AKS.](update-credentials.md)

## <a name="i-cant-access-my-cluster-api-from-my-automationdev-machinetooling-when-using-api-server-authorized-ip-ranges-how-do-i-fix-this-problem"></a>Я не могу получить доступ к API кластера из моей машины или инструментария службы автоматизации, а при использовании диапазонов IP-адресов, прошедших авторизацию сервера API. Как устранить эту проблему?

Чтобы устранить эту проблему, убедитесь, что `--api-server-authorized-ip-ranges` указаны IP-адреса или диапазоны IP-адресов для используемых систем автоматизации, разработки и инструментария. См. раздел "как найти мой IP-адрес" в [безопасном доступе к серверу API с помощью диапазонов IP-адресов с правом доступа](api-server-authorized-ip-ranges.md).

## <a name="im-unable-to-view-resources-in-kubernetes-resource-viewer-in-azure-portal-for-my-cluster-configured-with-api-server-authorized-ip-ranges-how-do-i-fix-this-problem"></a>Не удается просмотреть ресурсы в средстве просмотра ресурсов Kubernetes в портал Azure для кластера, настроенного с диапазонами IP-адресов, полномочными сервером API. Как устранить эту проблему?

[Средство просмотра ресурсов Kubernetes](kubernetes-portal.md) требует `--api-server-authorized-ip-ranges` включения доступа для локального клиентского компьютера или диапазона IP-адресов (из которого осуществляется просмотр портала). См. раздел "как найти мой IP-адрес" в [безопасном доступе к серверу API с помощью диапазонов IP-адресов с правом доступа](api-server-authorized-ip-ranges.md).

## <a name="im-receiving-errors-after-restricting-egress-traffic"></a>После ограничения исходящего трафика поступают ошибки

При ограничении исходящего трафика из кластера AKS существуют [обязательные и необязательные рекомендованные](limit-egress-traffic.md) исходящие порты/правила сети и правила FQDN/приложений для AKS. Если ваши параметры конфликтуют с любым из этих правил, некоторые команды `kubectl` будут работать неправильно. При создании кластера AKS также могут появиться ошибки.

Убедитесь, что параметры не конфликтуют с какими-либо из обязательных или необязательных рекомендуемых правил исходящих портов/сети и правил полного доменного имени/приложения.

## <a name="im-receiving-429---too-many-requests-errors"></a>Я получаю сообщение об ошибке "429-слишком много запросов"

Если кластер kubernetes в Azure (AKS или No) часто масштабируется или использует Автомасштабирование кластера (CA), эти операции могут привести к большому числу HTTP-вызовов, которые, в свою очередь, превышают квоты назначенной подписки, ведущая к сбою. Ошибки будут выглядеть следующим образом:

```
Service returned an error. Status=429 Code=\"OperationNotAllowed\" Message=\"The server rejected the request because too many requests have been received for this subscription.\" Details=[{\"code\":\"TooManyRequests\",\"message\":\"{\\\"operationGroup\\\":\\\"HighCostGetVMScaleSet30Min\\\",\\\"startTime\\\":\\\"2020-09-20T07:13:55.2177346+00:00\\\",\\\"endTime\\\":\\\"2020-09-20T07:28:55.2177346+00:00\\\",\\\"allowedRequestCount\\\":1800,\\\"measuredRequestCount\\\":2208}\",\"target\":\"HighCostGetVMScaleSet30Min\"}] InnerError={\"internalErrorCode\":\"TooManyRequestsReceived\"}"}
```

Эти ошибки регулирования подробно описаны [здесь](../azure-resource-manager/management/request-limits-and-throttling.md) и [здесь](../virtual-machines/troubleshooting/troubleshooting-throttling-errors.md) .

Рекомендация от группы инженеров AKS — убедиться, что вы используете версию не ниже 18E. x, которая содержит множество улучшений. Дополнительные сведения можно найти в [этих усовершенствованиях](https://github.com/Azure/AKS/issues/1413) и [здесь](https://github.com/kubernetes-sigs/cloud-provider-azure/issues/247).

Учитывая, что эти ошибки регулирования измеряются на уровне подписки, они могут по-прежнему выполняться в следующих случаях:
- Существуют сторонние приложения, выполняющие запросы GET (например, мониторинг приложений и т. д.). Рекомендуется уменьшить частоту этих вызовов.
- Существует множество кластеров или пулов узлов AKS, использующих масштабируемые наборы виртуальных машин. Попробуйте разделить количество кластеров на разные подписки, в частности, если вы ожидаете, что они будут очень активными (например, автомасштабированием активного кластера) или имеют несколько клиентов (например, Ранчер, terraform и т. д.).

## <a name="my-clusters-provisioning-status-changed-from-ready-to-failed-with-or-without-me-performing-an-operation-what-should-i-do"></a>Состояние подготовки кластера изменено с "Готово" на "выполнено" или без меня выполнения операции. Что следует делать?

Если состояние подготовки кластера изменяется с *готовности* на *сбой* с или без выполнения каких либо каких либо операций, но приложения в кластере продолжают выполняться, эта проблема может быть устранена автоматически службой, и ваши приложения не должны быть затронуты.

Если состояние подготовки кластера остается *неудачным* или приложения в кластере перестают работать, [отправьте запрос в службу поддержки](https://azure.microsoft.com/support/options/#submit).

## <a name="my-watch-is-stale-or-azure-ad-pod-identity-nmi-is-returning-status-500"></a>Мое контрольное значение устарело, или для удостоверения Pod Azure AD возвращено состояние 500

Если вы используете брандмауэр Azure, как в этом [примере](limit-egress-traffic.md#restrict-egress-traffic-using-azure-firewall), эта проблема может возникать, так как длительные TCP-подключения через брандмауэр, использующие правила приложений, в настоящее время имеют ошибку (которая разрешается в Q1CY21), что приводит к `keepalives` прекращению работы в брандмауэре. До устранения этой проблемы можно устранить проблему, добавив сетевое правило (вместо правила приложения) к IP-адресу сервера API AKS.

## <a name="azure-storage-and-aks-troubleshooting"></a>Устранение неполадок AKS и службы хранилища Azure

### <a name="failure-when-setting-uid-and-gid-in-mountoptions-for-azure-disk"></a>Сбой при настройке UID и `GID` в маунтоптионс для диска Azure

По умолчанию диск Azure использует ext4,xfs filesystem, а mountOptions, например uid=x,gid=x, не могут быть заданы во время подключения. Например, если вы попытались задать mountOptions uid=999,gid=999, отобразится следующее сообщение об ошибке:

```console
Warning  FailedMount             63s                  kubelet, aks-nodepool1-29460110-0  MountVolume.MountDevice failed for volume "pvc-d783d0e4-85a1-11e9-8a90-369885447933" : azureDisk - mountDevice:FormatAndMount failed with mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 --scope -- mount -t xfs -o dir_mode=0777,file_mode=0777,uid=1000,gid=1000,defaults /dev/disk/azure/scsi1/lun2 /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985
Output: Running scope as unit run-rb21966413ab449b3a242ae9b0fbc9398.scope.
mount: wrong fs type, bad option, bad superblock on /dev/sde,
       missing codepage or helper program, or other error
```

Вы можете устранить эту ошибку, выполнив одно из следующих действий.

* [Настройте контекст безопасности для pod](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/), задав UID в runAsUser и GID в fsGroup. Например, следующий параметр задает запуск модуля pod как root и делает его доступным для любого файла:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:
    runAsUser: 0
    fsGroup: 0
```

  >[!NOTE]
  > Поскольку GID и UID подключаются по умолчанию как root или 0. Если GID или UID задан отличным от root, например 1000, Kubernetes будет использовать `chown` для изменения всех каталогов и файлов на этом диске. Эта операция может занять много времени и привести к очень медленному подключению диска.

* Используйте `chown` в инитконтаинерс, чтобы установить `GID` и `UID` . Пример:

```yaml
initContainers:
- name: volume-mount
  image: busybox
  command: ["sh", "-c", "chown -R 100:100 /data"]
  volumeMounts:
  - name: <your data volume>
    mountPath: /data
```

### <a name="azure-disk-detach-failure-leading-to-potential-race-condition-issue-and-invalid-data-disk-list"></a>Сбой отключения диска Azure, ведущий к потенциальной проблеме состояния гонки и недопустимому списку дисков данных.

При сбое отключения диска Azure будет выполнено до шести повторных попыток, чтобы отключить диск с помощью экспоненциального выключения. Он также будет удерживать блокировку на уровне узла в списке дисков данных примерно в течение 3 минут. Если список дисков обновить вручную в течение этого времени, то список дисков, сохраняемый блокировкой на уровне узла, станет устаревшим, что приведет к нестабильной работе узла.

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.9 и более поздние |
| 1.13 | 1.13.6 и более поздние |
| 1,14 | 1.14.2 и более поздние |
| 1.15 и более поздние | Недоступно |

Если вы используете версию Kubernetes, которая не включает исправления для этой проблемы, и у узла имеется устаревший список дисков, вы можете исправить проблему, отключив все несуществующие диски от виртуальной машины в рамках групповой операции. **Отключение несуществующих дисков по отдельности может завершиться ошибкой.**

### <a name="large-number-of-azure-disks-causes-slow-attachdetach"></a>Большое количество дисков Azure приводит к снижению скорости их подключения или отключения.

Если количество операций подключения и отключения дисков Azure, нацеленных на виртуальную машину с одним узлом, превышает 10 или превышает 3 при выборе одного пула масштабируемых наборов виртуальных машин, операции могут быть медленнее, чем ожидалось, так как они выполняются последовательно. Эта проблема является известным ограничением, и в настоящее время у нее нет решений. [Пользовательский голосовой элемент для поддержки параллельного подключения и отключения за пределами числа.](https://feedback.azure.com/forums/216843-virtual-machines/suggestions/40444528-vmss-support-for-parallel-disk-attach-detach-for).

### <a name="azure-disk-detach-failure-leading-to-potential-node-vm-in-failed-state"></a>Сбой отключения диска Azure, приводящий к потенциальной виртуальной машине узла в состоянии сбоя

В некоторых случаях отключение диска Azure может завершиться частичным сбоем и оставить виртуальную машину узла в состоянии сбоя.

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.10 и более поздние |
| 1.13 | 1.13.8 и более поздние |
| 1,14 | 1.14.4 и более поздние |
| 1.15 и более поздние | Недоступно |

Если вы применяете версию Kubernetes, которая не включает исправления для этой проблемы, а узел находится в состоянии сбоя, можно устранить неполадки, обновив состояние виртуальной машины вручную, используя один из следующих способов.

* Для кластера на основе группы доступности:
    ```azurecli
    az vm update -n <VM_NAME> -g <RESOURCE_GROUP_NAME>
    ```

* Для кластера на основе VMSS:
    ```azurecli
    az vmss update-instances -g <RESOURCE_GROUP_NAME> --name <VMSS_NAME> --instance-id <ID>
    ```

## <a name="azure-files-and-aks-troubleshooting"></a>Устранение неполадок Файлов Azure и AKS

### <a name="what-are-the-recommended-stable-versions-of-kubernetes-for-azure-files"></a>Какие стабильные версии Kubernetes рекомендуются для Файлов Azure?
 
| Версия Kubernetes | Рекомендуемая версия |
|--|:--:|
| 1.12 | 1.12.6 и более поздние |
| 1.13 | 1.13.4 и более поздние |
| 1,14 | 1.14.0 и более поздние |

### <a name="what-are-the-default-mountoptions-when-using-azure-files"></a>Каковы mountOptions по умолчанию при использовании Файлов Azure?

Рекомендуемые параметры:

| Версия Kubernetes | значение fileMode и dirMode|
|--|:--:|
| 1.12.0–1.12.1 | 0755 |
| 1.12.2 и более поздние | 0777 |

Параметры подключения можно указать в объекте класса хранения. В следующем примере задается значение *0777*.

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: azurefile
provisioner: kubernetes.io/azure-file
mountOptions:
  - dir_mode=0777
  - file_mode=0777
  - uid=1000
  - gid=1000
  - mfsymlinks
  - nobrl
  - cache=none
parameters:
  skuName: Standard_LRS
```

Некоторые дополнительные полезные параметры *mountOptions*:

* `mfsymlinks` обеспечивает поддержку символических ссылок в службе подключения файлов Azure (CIFS)
* `nobrl` предотвратит отправку запросов на блокировку диапазона байтов на сервер. Этот параметр необходим для некоторых приложений, которые не используют обязательные блокировки диапазона байтов в стиле CIFS. Большинство серверов CIFS пока не поддерживают рекомендательные запросы на блокировку диапазона байтов. Если не использовать *nobrl*, приложения, которые не применяют обязательные блокировки диапазона байтов в стиле CIFS, могут вызвать сообщения об ошибках следующего вида:
    ```console
    Error: SQLITE_BUSY: database is locked
    ```

### <a name="error-could-not-change-permissions-when-using-azure-files"></a>Ошибка "не удалось изменить разрешения" при использовании Файлов Azure.

При запуске PostgreSQL в подключаемом модуле Файлов Azure может появиться сообщение об ошибке следующего вида:

```console
initdb: could not change permissions of directory "/var/lib/postgresql/data": Operation not permitted
fixing permissions on existing directory /var/lib/postgresql/data
```

Эта ошибка вызвана тем, что подключаемый модуль Файлов Azure использует протокол CIFS/SMB. При использовании протокола CIFS/SMB разрешения на доступ к файлам и каталогам не могут быть изменены после подключения.

Чтобы устранить эту проблему, используйте `subPath` вместе с подключаемым модулем диска Azure. 

> [!NOTE] 
> Для диска типа ext3/4 возникает потерянный и найденный каталог после форматирования диска.

### <a name="azure-files-has-high-latency-compared-to-azure-disk-when-handling-many-small-files"></a>Высокая задержка в Файлах Azure по сравнению с диском Azure при обработке большого количества небольших файлов

В некоторых случаях, например при обработке большого количества маленьких файлов, при использовании Файлов Azure может возникнуть высокая задержка, по сравнению с диском Azure.

### <a name="error-when-enabling-allow-access-allow-access-from-selected-network-setting-on-storage-account"></a>Ошибка при включении параметра "Разрешить доступ из выбранной сети" в учетной записи хранения

Если вы включаете *разрешить доступ из выбранной сети* в учетной записи хранения, которая используется для динамической подготовки в AKS, при создании общей папки AKS возникает ошибка:

```console
persistentvolume-controller (combined from similar events): Failed to provision volume with StorageClass "azurefile": failed to create share kubernetes-dynamic-pvc-xxx in account xxx: failed to create file share, err: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation.
```

Эта ошибка вызвана тем, что *persistentvolume-controller* Kubernetes не находится в сети, выбранной при настройке *разрешить доступ из выбранной сети*.

Вы можете устранить эту проблему, используя [статическую подготовку с помощью Файлов Azure](azure-files-volume.md).

### <a name="azure-files-fails-to-remount-in-windows-pod"></a>Не удается повторно подключить Файлы Azure в модуле Windows

Если удаляется модуль Windows с подключением к Файлам Azure и запланировано его повторное создание на том же узле, подключение завершится ошибкой. Это происходит из-за сбоя команды `New-SmbGlobalMapping`, так как служба Файлы Azure уже подключена к узлу.

Например, может появиться сообщение об ошибке следующего вида:

```console
E0118 08:15:52.041014    2112 nestedpendingoperations.go:267] Operation for "\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\" (\"42c0ea39-1af9-11e9-8941-000d3af95268\")" failed. No retries permitted until 2019-01-18 08:15:53.0410149 +0000 GMT m=+732.446642701 (durationBeforeRetry 1s). Error: "MountVolume.SetUp failed for volume \"pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\" (UniqueName: \"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\") pod \"deployment-azurefile-697f98d559-6zrlf\" (UID: \"42c0ea39-1af9-11e9-8941-000d3af95268\") : azureMount: SmbGlobalMapping failed: exit status 1, only SMB mount is supported now, output: \"New-SmbGlobalMapping : Generic failure \\r\\nAt line:1 char:190\\r\\n+ ... ser, $PWord;New-SmbGlobalMapping -RemotePath $Env:smbremotepath -Cred ...\\r\\n+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\r\\n    + CategoryInfo          : NotSpecified: (MSFT_SmbGlobalMapping:ROOT/Microsoft/...mbGlobalMapping) [New-SmbGlobalMa \\r\\n   pping], CimException\\r\\n    + FullyQualifiedErrorId : HRESULT 0x80041001,New-SmbGlobalMapping\\r\\n \\r\\n\""
```

Эта проблема исправлена в следующих версиях Kubernetes:

| Версия Kubernetes | Исправленная версия |
|--|:--:|
| 1.12 | 1.12.6 и более поздние |
| 1.13 | 1.13.4 и более поздние |
| 1.14 и более поздние | Недоступно |

### <a name="azure-files-mount-fails-because-of-storage-account-key-changed"></a>Сбой подключения к Файлам Azure из-за изменения ключа учетной записи хранения

Если ключ учетной записи хранения изменился, вы можете столкнуться со сбоями при подключении Файлов Azure.

Чтобы устранить ошибку, вручную обновите поле `azurestorageaccountkey` в секрете файла Azure с помощью ключа учетной записи хранения в кодировке Base64.

Чтобы закодировать ключ учетной записи хранения в Base64, можно использовать `base64`. Пример:

```console
echo X+ALAAUgMhWHL7QmQ87E1kSfIqLKfgC03Guy7/xk9MyIg2w4Jzqeu60CVw2r/dm6v6E0DWHTnJUEJGVQAoPaBc== | base64
```

Чтобы обновить секретный файл Azure, используйте `kubectl edit secret`. Пример:

```console
kubectl edit secret azure-storage-account-{storage-account-name}-secret
```

Через несколько минут узел агента повторит попытку подключения Файла Azure к обновленному ключу хранилища.


### <a name="cluster-autoscaler-fails-to-scale-with-error-failed-to-fix-node-group-sizes"></a>Сбой автомасштабирования кластера с ошибкой "не удалось исправить размеры групп узлов"

Если автомасштабирование кластера в ту или иную сторону не выполняется и вы видите ошибку, подобную приведенной ниже, в [ журналах автомасштабирования кластера][view-master-logs].

```console
E1114 09:58:55.367731 1 static_autoscaler.go:239] Failed to fix node group sizes: failed to decrease aks-default-35246781-vmss: attempt to delete existing nodes
```

Эта ошибка возникла из-за состояния гонки автомасштабирования вышестоящего кластера. В этом случае автомасштабирование кластера заканчивается значением, отличным от того, которое фактически находится в кластере. Чтобы выйти из этого состояния, отключите и снова включите [автомасштабирование кластера][cluster-autoscaler].

### <a name="slow-disk-attachment-getazuredisklun-takes-10-to-15-minutes-and-you-receive-an-error"></a>Очень большое дисковое вложение `GetAzureDiskLun` занимает от 10 до 15 минут, и появляется сообщение об ошибке

В версиях Kubernetes, **предшествовавших 1.15.0**, может появиться сообщение об ошибке, например **Ошибка WaitForAttach, не удается найти LUN для диска**.  Чтобы решить эту проблему, подождите примерно 15 минут и повторите попытку.


### <a name="why-do-upgrades-to-kubernetes-116-fail-when-using-node-labels-with-a-kubernetesio-prefix"></a>Почему при использовании меток узлов с префиксом kubernetes.io происходит сбой обновления до Kubernetes 1,16

Начиная с Kubernetes [1,16](https://v1-16.docs.kubernetes.io/docs/setup/release/notes/) , kubelet к узлам могут быть применены [только определенные подмножества меток с префиксом kubernetes.IO](https://v1-18.docs.kubernetes.io/docs/concepts/overview/working-with-objects/labels/) . AKS не может удалить активные метки от вашего имени без согласия, так как это может привести к простою рабочих нагрузок.

В результате, чтобы устранить эту ошибку, можно выполнить следующие действия.

1. Обновите плоскость управления кластером до 1,16 или более поздней версии.
2. Добавление нового нодепуол в 1,16 или более поздней версии без неподдерживаемых меток kubernetes.io
3. Удаление пула старых узлов

AKS изучает возможность изменения активных меток в пуле узлов, чтобы улучшить это снижение.



<!-- LINKS - internal -->
[view-master-logs]: ./view-control-plane-logs.md
[cluster-autoscaler]: cluster-autoscaler.md