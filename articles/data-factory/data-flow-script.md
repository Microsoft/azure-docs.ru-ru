---
title: Сценарий потока данных сопоставления
description: Общие сведения о языке кода программной части сценария потока данных фабрики данных
author: kromerm
ms.author: nimoolen
ms.service: data-factory
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 02/15/2021
ms.openlocfilehash: 7dd58a7d4a94b832e52930f8ac6507cdd8f7a20e
ms.sourcegitcommit: 867cb1b7a1f3a1f0b427282c648d411d0ca4f81f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "100534827"
---
# <a name="data-flow-script-dfs"></a>Сценарий потока данных (DFS)

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Сценарий потока данных (DFS) — это базовые метаданные, аналогичные языку программирования, используемому для выполнения преобразований, включенных в поток данных сопоставления. Каждое преобразование представлено рядом свойств, которые предоставляют необходимые сведения для правильного выполнения задания. Сценарий отображается и редактируется из ADF. для этого нажмите кнопку "Скрипт" на верхней ленте пользовательского интерфейса браузера.

![Кнопка скрипта](media/data-flow/scriptbutton.png "Кнопка скрипта")

Например, `allowSchemaDrift: true,` в преобразовании «источник» указывает службе включать все столбцы из исходного набора данных в потоке данных, даже если они не включены в проекцию схемы.

## <a name="use-cases"></a>Варианты использования
DFS автоматически создается интерфейсом пользователя. Можно нажать кнопку Скрипт, чтобы просмотреть и настроить скрипт. Кроме того, можно создавать скрипты за пределами пользовательского интерфейса ADF, а затем передавать их в командлет PowerShell. При отладке сложных потоков данных может оказаться проще проверять код программной части сценария, а не проверять представление графа пользовательского интерфейса для потоков.

Ниже приведены примеры использования.
- Программное создание многих похожих потоков данных, т. е. потоков данных с отметкой.
- Сложные выражения, которые трудно управлять в пользовательском интерфейсе или которые могут приводить к проблемам проверки.
- Отладка и более эффективное понимание различных ошибок, возвращаемых во время выполнения.

При создании скрипта потока данных для использования с PowerShell или API необходимо свернуть форматированный текст в одну строку. Символы табуляции и новой строки можно размещать в виде escape-символов. Но текст должен быть отформатирован в соответствии со свойством JSON. В нижней части пользовательского интерфейса редактора сценариев есть кнопка, которая будет форматировать сценарий как единую строку.

![Кнопка "Копировать"](media/data-flow/copybutton.png "Кнопка "Копировать"")

## <a name="how-to-add-transforms"></a>Добавление преобразований
Добавление преобразований требует выполнения трех основных шагов: Добавление основных данных преобразования, перенаправление входного потока и перенаправление потока вывода. В качестве примера это можно увидеть проще всего.
Предположим, что начнем с простого источника данных приемника, как в следующем примере:

```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Если мы решили добавить преобразование «Производный», сначала необходимо создать основной текст преобразования, имеющий простое выражение для добавления нового столбца в верхнем регистре `upperCaseTitle` :
```
derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
```

Затем мы принимаем существующую DFS и добавим преобразование:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Теперь мы перенаправляем входящий поток, определив преобразование, которое должно быть после (в данном случае) новым преобразованием, `source1` и скопировав имя потока в новое преобразование:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
source1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

Наконец, мы выведем преобразование, которое мы хотим сделать после этого нового преобразования, и замените входной поток (в данном случае `sink1` ) на имя выходного потока нашего нового преобразования:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
source1 derive(upperCaseTitle = upper(title)) ~> deriveTransformationName
deriveTransformationName sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

## <a name="dfs-fundamentals"></a>Основы DFS
DFS состоит из ряда подключенных преобразований, включая источники, приемники и различные другие, которые могут добавлять новые столбцы, фильтровать данные, объединять данные и многое другое. Обычно сценарий начинается с одного или нескольких источников, за которым следует много преобразований и заканчивается одним или несколькими приемниками.

Все источники имеют одинаковую базовую конструкцию:
```
source(
  source properties
) ~> source_name
```

Например, простой источник с тремя столбцами (Мовиеид, Title, жанры) будет выглядеть следующим образом:
```
source(output(
        movieId as string,
        title as string,
        genres as string
    ),
    allowSchemaDrift: true,
    validateSchema: false) ~> source1
```

Все преобразования, отличные от источников, имеют одну базовую конструкцию:
```
name_of_incoming_stream transformation_type(
  properties
) ~> new_stream_name
```

Например, простое производное преобразование, которое принимает столбец (заголовок) и перезаписывает его версией в верхнем регистре, будет выглядеть следующим образом:
```
source1 derive(
  title = upper(title)
) ~> derive1
```

И приемник без схемы будет просто:
```
derive1 sink(allowSchemaDrift: true,
    validateSchema: false) ~> sink1
```

## <a name="script-snippets"></a>Фрагменты сценариев

Фрагменты сценариев — это общий код сценария потока данных, который можно использовать для совместного использования в потоках данных. В этом видеоролике рассказывается о том, как использовать фрагменты сценариев и использование сценария потока данных для копирования и вставки частей сценария, расположенных в графах потоков данных:

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE4tA9b]


### <a name="aggregated-summary-stats"></a>Агрегированная сводная статистика
Добавьте преобразование «Статистическая обработка» в поток данных с именем «Суммаристатс», а затем вставьте этот код ниже для агрегатной функции в скрипте, заменив существующий Суммаристатс. Это обеспечит общий шаблон статистической статистики для профиля данных.

```
aggregate(each(match(true()), $$+'_NotNull' = countIf(!isNull($$)), $$ + '_Null' = countIf(isNull($$))),
        each(match(type=='double'||type=='integer'||type=='short'||type=='decimal'), $$+'_stddev' = round(stddev($$),2), $$ + '_min' = min ($$), $$ + '_max' = max($$), $$ + '_average' = round(avg($$),2), $$ + '_variance' = round(variance($$),2)),
        each(match(type=='string'), $$+'_maxLength' = max(length($$)))) ~> SummaryStats
```
Приведенный ниже пример также можно использовать для подсчета числа уникальных и количества уникальных строк в данных. Приведенный ниже пример можно вставлять в поток данных с преобразованием «Статистическая обработка» с именем Валуедистагг. В этом примере используется столбец с именем Title. Обязательно замените "Title" строковым столбцом данных, который вы хотите использовать для получения счетчиков значений.

```
aggregate(groupBy(title),
    countunique = count()) ~> ValueDistAgg
ValueDistAgg aggregate(numofunique = countIf(countunique==1),
        numofdistinct = countDistinct(title)) ~> UniqDist
```

### <a name="include-all-columns-in-an-aggregate"></a>Включить все столбцы в статистическое выражение
Это универсальный шаблон статистического выражения, демонстрирующий, как можно обеспечить возможность сохранения оставшихся столбцов в выходных метаданных при построении статистических выражений. В этом случае мы используем функцию, ```first()``` чтобы выбрать первое значение в каждом столбце, имя которого не является «Movie». Чтобы использовать эту функцию, создайте преобразование «Статистическая обработка» с именем Дистинктровс и вставьте его в скрипт поверх существующего статистического сценария Дистинктровс.

```
aggregate(groupBy(movie),
    each(match(name!='movie'), $$ = first($$))) ~> DistinctRows
```

### <a name="create-row-hash-fingerprint"></a>Создать отпечаток хэша строки 
Используйте этот код в скрипте потока данных для создания нового производного столбца ```DWhash``` с именем, создающего ```sha1``` хэш трех столбцов.

```
derive(DWhash = sha1(Name,ProductNumber,Color)) ~> DWHash
```

Этот скрипт также можно использовать для создания хэша строки с использованием всех столбцов, имеющихся в вашем потоке, без необходимости присвоить каждому столбцу имя.

```
derive(DWhash = sha1(columns())) ~> DWHash
```

### <a name="string_agg-equivalent"></a>Эквивалент String_agg
Этот код будет действовать как функция T-SQL ```string_agg()``` и будет объединять строковые значения в массив. Затем этот массив можно привести в строку для использования с назначениями SQL.

```
source1 aggregate(groupBy(year),
    string_agg = collect(title)) ~> Aggregate1
Aggregate1 derive(string_agg = toString(string_agg)) ~> StringAgg
```

### <a name="count-number-of-updates-upserts-inserts-deletes"></a>Число обновлений, операции Upsert, вставок, удалений
При использовании преобразования ALTER Row может потребоваться подсчитать количество операций Updates, операции Upsert, Inserts, которые являются результатом политик изменения строк. Добавьте преобразование «Статистическая обработка» после изменения строки и вставьте этот скрипт потока данных в определение агрегата для этих счетчиков.

```
aggregate(updates = countIf(isUpdate(), 1),
        inserts = countIf(isInsert(), 1),
        upserts = countIf(isUpsert(), 1),
        deletes = countIf(isDelete(),1)) ~> RowCount
```

### <a name="distinct-row-using-all-columns"></a>Уникальная строка с использованием всех столбцов
Этот фрагмент кода добавит в поток данных новое преобразование «Статистическая обработка», которое будет принимать все входящие столбцы, формировать хэш, используемый для группирования, чтобы исключить дубликаты, а затем предоставить первое вхождение каждого дубликата в качестве выходных данных. Явное имя столбцов не требуется, они будут автоматически создаваться из входящего потока данных.

```
aggregate(groupBy(mycols = sha2(256,columns())),
    each(match(true()), $$ = first($$))) ~> DistinctRows
```

### <a name="check-for-nulls-in-all-columns"></a>Проверять наличие значений NULL во всех столбцах
Это фрагмент кода, который можно вставить в поток данных для универсальной проверки всех столбцов на наличие значений NULL. Этот метод использует смещение схемы, чтобы просмотреть все столбцы во всех строках и использовать условное разбиение для разделения строк значениями NULL из строк без значений NULL. 

```
split(contains(array(columns()),isNull(#item)),
    disjoint: false) ~> LookForNULLs@(hasNULLs, noNULLs)
```

### <a name="automap-schema-drift-with-a-select"></a>Автоматическое сопоставление смещение схемы с помощью SELECT
Если необходимо загрузить существующую схему базы данных из неизвестного или динамического набора входящих столбцов, необходимо соотнести правый столбец в преобразовании приемника. Это необходимо только при загрузке существующей таблицы. Добавьте этот фрагмент перед приемником, чтобы создать выбор, который будет выполнять автоматическое сопоставление столбцов. Оставьте сопоставление приемника для автоматического сопоставления.

```
select(mapColumn(
        each(match(true()))
    ),
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~> automap
```

### <a name="persist-column-data-types"></a>Сохранение типов данных столбцов
Добавьте этот скрипт в определение производного столбца, чтобы сохранить имена столбцов и типы данных из потока данных в постоянное хранилище с помощью приемника.

```
derive(each(match(type=='string'), $$ = 'string'),
    each(match(type=='integer'), $$ = 'integer'),
    each(match(type=='short'), $$ = 'short'),
    each(match(type=='complex'), $$ = 'complex'),
    each(match(type=='array'), $$ = 'array'),
    each(match(type=='float'), $$ = 'float'),
    each(match(type=='date'), $$ = 'date'),
    each(match(type=='timestamp'), $$ = 'timestamp'),
    each(match(type=='boolean'), $$ = 'boolean'),
    each(match(type=='long'), $$ = 'long'),
    each(match(type=='double'), $$ = 'double')) ~> DerivedColumn1
```

### <a name="fill-down"></a>Заполнение по направлению вниз
Ниже показано, как реализовать общую проблему «заполнение» с наборами данных, если требуется заменить значения NULL значениями из предыдущего значения, отличного от NULL, в последовательности. Обратите внимание, что эта операция может отрицательно сказаться на производительности, поскольку необходимо создать искусственное окно во всем наборе данных со значением категории "фиктивный". Кроме того, необходимо выполнить сортировку по значению, чтобы создать правильную последовательность данных для поиска предыдущего значения, отличного от NULL. Следующий фрагмент кода создает искусственную категорию как "фиктивную" и сортирует ее по суррогатному ключу. Вы можете удалить суррогатный ключ и использовать собственный ключ сортировки, относящийся к данным. В этом фрагменте кода предполагается, что вы уже добавили Преобразование источника с именем ```source1```

```
source1 derive(dummy = 1) ~> DerivedColumn
DerivedColumn keyGenerate(output(sk as long),
    startAt: 1L) ~> SurrogateKey
SurrogateKey window(over(dummy),
    asc(sk, true),
    Rating2 = coalesce(Rating, last(Rating, true()))) ~> Window1
```

### <a name="moving-average"></a>Скользящее среднее
Скользящее среднее можно легко реализовать в потоках данных с помощью преобразования Windows. В следующем примере создается 15-дневное скользящее среднее для цен на акции для Майкрософт.

```
window(over(stocksymbol),
    asc(Date, true),
    startRowOffset: -7L,
    endRowOffset: 7L,
    FifteenDayMovingAvg = round(avg(Close),2)) ~> Window1
```

## <a name="next-steps"></a>Дальнейшие действия

Проанализируйте потоки данных, начав с помощью [статьи общие сведения о потоках данных](concepts-data-flow-overview.md)
