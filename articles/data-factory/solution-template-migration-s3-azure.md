---
title: Перенос данных из Amazon S3 в Azure Data Lake Storage 2-го поколения
description: Узнайте, как использовать шаблон решения для переноса данных из Amazon S3 с помощью внешней таблицы управления для хранения списка секций в AWS S3 с фабрикой данных Azure.
author: dearandyxu
ms.author: yexu
ms.service: data-factory
ms.topic: conceptual
ms.custom: seo-lt-2019
ms.date: 09/07/2019
ms.openlocfilehash: c1fd4cb248abdc219c6ee5d098e10c329826c160
ms.sourcegitcommit: 867cb1b7a1f3a1f0b427282c648d411d0ca4f81f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "100362001"
---
# <a name="migrate-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Перенос данных из Amazon S3 в Azure Data Lake Storage 2-го поколения

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

Используйте шаблоны для переноса петабайтов данных, состоящих из сотен миллионов файлов из Amazon S3, в Azure Data Lake Storage 2-го поколения. 

 > [!NOTE]
 > Если вы хотите скопировать небольшой объем данных из AWS S3 в Azure (например, менее 10 ТБ), более эффективно и удобно использовать [средство копирование данных фабрики данных Azure](copy-data-tool.md). Шаблон, описанный в этой статье, больше, чем требуется.

## <a name="about-the-solution-templates"></a>О шаблонах решений

Рекомендуется использовать секцию данных, особенно при переносе более 10 ТБ данных. Чтобы секционировать данные, используйте параметр "префикс" для фильтрации папок и файлов в Amazon S3 по имени, а затем каждое задание копирования ADF может копировать по одной секции за раз. Для повышения пропускной способности можно одновременно запустить несколько заданий копирования в Фабрике данных Azure.

Для переноса данных обычно требуется однократная миграция данных, а также периодическая синхронизация изменений из AWS S3 с Azure. Ниже приведено два шаблона, в которых один шаблон охватывает однократную миграцию данных, а другой шаблон охватывает синхронизацию изменений из AWS S3 с Azure.

### <a name="for-the-template-to-migrate-historical-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Для шаблона, чтобы перенести исторические данные из Amazon S3 в Azure Data Lake Storage 2-го поколения

Этот шаблон (*имя шаблона. Перенос исторических данных из AWS S3 в Azure Data Lake Storage 2-го поколения*) предполагает, что вы написали список секций во внешней таблице управления в базе данных SQL Azure. Поэтому он будет использовать действие *уточняющего запроса* для получения списка секций из внешней таблицы элементов управления, итерации по каждой секции и создания каждой копии задания копирования ADF по одной секции за раз. После завершения задания копирования оно использует действие *хранимой процедуры* для обновления состояния копирования каждой секции в таблице управления.

Шаблон содержит пять действий:
- **Уточняющий запрос** извлекает секции, которые не были скопированы в Azure Data Lake Storage 2-го поколения из внешней таблицы управления. Имя таблицы *s3_partition_control_table* , а запрос для загрузки данных из таблицы — *SELECT ПАРТИТИОНПРЕФИКС from s3_partition_control_table WHERE сукцессорфаилуре = 0*.
- **Foreach** получает список секций из действия *уточняющего запроса* и перебирает каждую секцию в действие *тригжеркопи* . Можно настроить *батчкаунт* для одновременного запуска нескольких заданий копирования ADF. Мы установили 2 в этом шаблоне.
- **ExecutePipeline** выполняет конвейер *CopyFolderPartitionFromS3* . Причина, по которой мы создаем другой конвейер для копирования каждого задания копирования в секцию, заключается в том, что это облегчит повторное выполнение неудачного задания копирования для повторной загрузки этой секции из AWS S3. Остальные задания копирования, которые загружают другие секции, не будут затронуты.
- **Copy** копирует каждую секцию из AWS S3 в Azure Data Lake Storage 2-го поколения.
- **SqlServerStoredProcedure** обновление состояния копирования каждой секции в таблице управления.

Шаблон содержит два параметра:
- **AWS_S3_bucketName** — это имя сегмента в AWS S3, в котором нужно перенести данные. Если вы хотите перенести данные из нескольких контейнеров в AWS S3, можно добавить еще один столбец во внешнюю таблицу управления для хранения имени контейнера для каждой секции, а также обновить конвейер для получения данных из этого столбца соответствующим образом.
- **Azure_Storage_fileSystem** — это имя файловой системы на Azure Data Lake Storage 2-го поколения, куда нужно перенести данные.

### <a name="for-the-template-to-copy-changed-files-only-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Чтобы шаблон скопировал измененные файлы только из Amazon S3 в Azure Data Lake Storage 2-го поколения

Этот шаблон (*имя шаблона: копирование Дельта-данных из AWS S3 в Azure Data Lake Storage 2-го поколения*) использует время каждого файла для копирования новых или обновленных файлов только из AWS S3 в Azure. Имейте в виду, что если файлы или папки уже были секционированы с помощью сведений тимеслице в имени файла или папки в AWS S3 (например,/ИИИИ/мм/дд/file.csv), можно перейти к этому [учебнику](tutorial-incremental-copy-partitioned-file-name-copy-data-tool.md) , чтобы получить более производительный подход к добавочной загрузке новых файлов. В этом шаблоне предполагается, что вы написали список секций во внешней таблице элементов управления в базе данных SQL Azure. Поэтому он будет использовать действие *уточняющего запроса* для получения списка секций из внешней таблицы элементов управления, итерации по каждой секции и создания каждой копии задания копирования ADF по одной секции за раз. Когда каждое задание копирования начинает копирование файлов из AWS S3, оно использует свойство время для обнаружения и копирования только новых или обновленных файлов. После завершения задания копирования оно использует действие *хранимой процедуры* для обновления состояния копирования каждой секции в таблице управления.

Шаблон содержит семь действий:
- **Уточняющий запрос** извлекает секции из внешней таблицы управления. Имя таблицы *s3_partition_delta_control_table* , а запрос для загрузки данных из таблицы — *"Select distinct партитионпрефикс from s3_partition_delta_control_table"*.
- **Foreach** получает список секций из действия *уточняющего запроса* и перебирает каждую секцию в действие *тригжерделтакопи* . Можно настроить *батчкаунт* для одновременного запуска нескольких заданий копирования ADF. Мы установили 2 в этом шаблоне.
- **ExecutePipeline** выполняет конвейер *DeltaCopyFolderPartitionFromS3* . Причина, по которой мы создаем другой конвейер для копирования каждого задания копирования в секцию, заключается в том, что это облегчит повторное выполнение неудачного задания копирования для повторной загрузки этой секции из AWS S3. Остальные задания копирования, которые загружают другие секции, не будут затронуты.
- **Уточняющий запрос** получает Последнее время выполнения задания копирования из внешней таблицы управления, чтобы новые или обновленные файлы могли быть идентифицированы через время. Имя таблицы *s3_partition_delta_control_table* , а запрос для загрузки данных из таблицы — *"SELECT MAX (Жобрунтиме) as время from s3_partition_delta_control_table WHERE партитионпрефикс =" @ {конвейер (). parameters. префиксстр} "и сукцессорфаилуре = 1"*.
- **Copy** копирует новые или измененные файлы только для каждого раздела из AWS S3 в Azure Data Lake Storage 2-го поколения. Свойству *модифиеддатетиместарт* задано Последнее время выполнения задания копирования. Свойству *модифиеддатетиминд* задано текущее время выполнения задания копирования. Имейте в виду, что время применяется к часовому поясу UTC.
- **SqlServerStoredProcedure** обновляет состояние копирования каждого раздела и время выполнения копирования в таблице управления, когда она выполняется. Столбцу Сукцессорфаилуре присваивается значение 1.
- **SqlServerStoredProcedure** обновляет состояние копирования каждого раздела и время выполнения копирования в таблице управления в случае сбоя. Столбцу Сукцессорфаилуре присваивается значение 0.

Шаблон содержит два параметра:
- **AWS_S3_bucketName** — это имя сегмента в AWS S3, в котором нужно перенести данные. Если вы хотите перенести данные из нескольких контейнеров в AWS S3, можно добавить еще один столбец во внешнюю таблицу управления для хранения имени контейнера для каждой секции, а также обновить конвейер для получения данных из этого столбца соответствующим образом.
- **Azure_Storage_fileSystem** — это имя файловой системы на Azure Data Lake Storage 2-го поколения, куда нужно перенести данные.

## <a name="how-to-use-these-two-solution-templates"></a>Использование этих двух шаблонов решений

### <a name="for-the-template-to-migrate-historical-data-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Для шаблона, чтобы перенести исторические данные из Amazon S3 в Azure Data Lake Storage 2-го поколения 

1. Создайте таблицу элементов управления в базе данных SQL Azure, чтобы сохранить список разделов AWS S3. 

    > [!NOTE]
    > Имя таблицы — s3_partition_control_table.
    > Схема управляющей таблицы — Партитионпрефикс и Сукцессорфаилуре, где Партитионпрефикс — это параметр префикса в S3 для фильтрации папок и файлов в Amazon S3 по имени, а Сукцессорфаилуре — состояние копирования каждого раздела: 0 означает, что этот раздел не был скопирован в Azure, а 1 означает, что этот раздел был успешно скопирован в Azure.
    > В таблице элементов управления определено 5 секций, и состояние по умолчанию для копирования каждой секции — 0.

    ```sql
    CREATE TABLE [dbo].[s3_partition_control_table](
        [PartitionPrefix] [varchar](255) NULL,
        [SuccessOrFailure] [bit] NULL
    )

    INSERT INTO s3_partition_control_table (PartitionPrefix, SuccessOrFailure)
    VALUES
    ('a', 0),
    ('b', 0),
    ('c', 0),
    ('d', 0),
    ('e', 0);
    ```

2. Создайте хранимую процедуру в той же базе данных SQL Azure для управления таблицей. 

    > [!NOTE]
    > Имя хранимой процедуры — sp_update_partition_success. Он будет вызываться действием SqlServerStoredProcedure в конвейере ADF.

    ```sql
    CREATE PROCEDURE [dbo].[sp_update_partition_success] @PartPrefix varchar(255)
    AS
    BEGIN
    
        UPDATE s3_partition_control_table
        SET [SuccessOrFailure] = 1 WHERE [PartitionPrefix] = @PartPrefix
    END
    GO
    ```

3. Перейдите к разделу **Миграция исторических данных из AWS S3 в шаблон Azure Data Lake Storage 2-го поколения** . Введите подключения к внешней таблице управления, AWS S3 в качестве хранилища источников данных и Azure Data Lake Storage 2-го поколения в качестве целевого хранилища. Имейте в виду, что внешняя таблица элемента управления и хранимая процедура ссылаются на одно и то же соединение.

    ![Снимок экрана, на котором показана миграция исторических данных из AWS S3 в шаблон Azure Data Lake Storage 2-го поколения.](media/solution-template-migration-s3-azure/historical-migration-s3-azure1.png)

4. Щелкните **Использовать этот шаблон**.

    ![Снимок экрана, на котором выделена кнопка "использовать этот шаблон".](media/solution-template-migration-s3-azure/historical-migration-s3-azure2.png)
    
5. Вы увидите два конвейера и 3 набора данных, как показано в следующем примере:

    ![Снимок экрана, на котором показаны два конвейера и три набора данных, созданные с помощью шаблона.](media/solution-template-migration-s3-azure/historical-migration-s3-azure3.png)

6. Перейдите к конвейеру "BulkCopyFromS3" и выберите **Отладка**, а затем введите **Параметры**. Нажмите кнопку **Готово**.

    ![Снимок экрана, на котором показано, где выбрать отладку, и введите параметры перед нажатием кнопки "Готово".](media/solution-template-migration-s3-azure/historical-migration-s3-azure4.png)

7. Вы увидите результат, аналогичный приведенному ниже:

    ![Снимок экрана, на котором показаны возвращенные результаты.](media/solution-template-migration-s3-azure/historical-migration-s3-azure5.png)


### <a name="for-the-template-to-copy-changed-files-only-from-amazon-s3-to-azure-data-lake-storage-gen2"></a>Чтобы шаблон скопировал измененные файлы только из Amazon S3 в Azure Data Lake Storage 2-го поколения

1. Создайте таблицу элементов управления в базе данных SQL Azure, чтобы сохранить список разделов AWS S3. 

    > [!NOTE]
    > Имя таблицы — s3_partition_delta_control_table.
    > Схема таблицы управления — Партитионпрефикс, Жобрунтиме и Сукцессорфаилуре, где Партитионпрефикс — это параметр префикса в S3 для фильтрации папок и файлов в Amazon S3 по имени, Жобрунтиме — это значение даты и времени при выполнении заданий копирования, а Сукцессорфаилуре — состояние копирования каждого раздела: 0 означает, что этот раздел не был скопирован в Azure и 1 означает, что этот раздел был успешно скопирован в Azure.
    > В таблице элементов управления определено 5 секций. Значением по умолчанию для Жобрунтиме может быть время, когда начинается однократная миграция данных журнала. Действие копирования ADF скопирует файлы на AWS S3, которые были в последний раз изменены. Состояние по умолчанию для копирования каждой секции — 1.

    ```sql
    CREATE TABLE [dbo].[s3_partition_delta_control_table](
        [PartitionPrefix] [varchar](255) NULL,
        [JobRunTime] [datetime] NULL,
        [SuccessOrFailure] [bit] NULL
        )

    INSERT INTO s3_partition_delta_control_table (PartitionPrefix, JobRunTime, SuccessOrFailure)
    VALUES
    ('a','1/1/2019 12:00:00 AM',1),
    ('b','1/1/2019 12:00:00 AM',1),
    ('c','1/1/2019 12:00:00 AM',1),
    ('d','1/1/2019 12:00:00 AM',1),
    ('e','1/1/2019 12:00:00 AM',1);
    ```

2. Создайте хранимую процедуру в той же базе данных SQL Azure для управления таблицей. 

    > [!NOTE]
    > Имя хранимой процедуры — sp_insert_partition_JobRunTime_success. Он будет вызываться действием SqlServerStoredProcedure в конвейере ADF.

    ```sql
        CREATE PROCEDURE [dbo].[sp_insert_partition_JobRunTime_success] @PartPrefix varchar(255), @JobRunTime datetime, @SuccessOrFailure bit
        AS
        BEGIN
            INSERT INTO s3_partition_delta_control_table (PartitionPrefix, JobRunTime, SuccessOrFailure)
            VALUES
            (@PartPrefix,@JobRunTime,@SuccessOrFailure)
        END
        GO
    ```


3. Перейдите к разделу **копирование разностных данных из AWS S3 в шаблон Azure Data Lake Storage 2-го поколения** . Введите подключения к внешней таблице управления, AWS S3 в качестве хранилища источников данных и Azure Data Lake Storage 2-го поколения в качестве целевого хранилища. Имейте в виду, что внешняя таблица элемента управления и хранимая процедура ссылаются на одно и то же соединение.

    ![Создание подключения](media/solution-template-migration-s3-azure/delta-migration-s3-azure1.png)

4. Щелкните **Использовать этот шаблон**.

    ![Использовать этот шаблон](media/solution-template-migration-s3-azure/delta-migration-s3-azure2.png)
    
5. Вы увидите два конвейера и 3 набора данных, как показано в следующем примере:

    ![Проверка конвейера](media/solution-template-migration-s3-azure/delta-migration-s3-azure3.png)

6.  Перейдите в конвейер "DeltaCopyFromS3" и выберите **Debug (Отладка**) и введите **Параметры**. Нажмите кнопку **Готово**.

    ![Щелкните **Отладка**](media/solution-template-migration-s3-azure/delta-migration-s3-azure4.png)

7. Вы увидите результат, аналогичный приведенному ниже:

    ![Просмотр результатов](media/solution-template-migration-s3-azure/delta-migration-s3-azure5.png)

8. Вы также можете проверить результаты из управляющей таблицы с помощью запроса *"Select * from s3_partition_delta_control_table"*, вы увидите результат, аналогичный приведенному ниже.

    ![Снимок экрана, показывающий результаты из таблицы элементов управления после выполнения запроса.](media/solution-template-migration-s3-azure/delta-migration-s3-azure6.png)
    
## <a name="next-steps"></a>Дальнейшие действия

- [Копирование файлов из нескольких контейнеров](solution-template-copy-files-multiple-containers.md)
- [Перемещение файлов](solution-template-move-files.md)
