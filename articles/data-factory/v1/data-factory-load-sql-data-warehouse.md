---
title: Загрузка терабайтов данных в Azure синапсе Analytics
description: Демонстрируется, как в Azure синапсе Analytics можно загрузить 1 ТБ данных в течение 15 минут с помощью фабрики данных Azure.
author: linda33wj
ms.service: data-factory
ms.topic: conceptual
ms.date: 01/10/2018
ms.author: jingwang
robots: noindex
ms.openlocfilehash: 5acae7c90efbf178fad199177fa6e0886e497fdf
ms.sourcegitcommit: 867cb1b7a1f3a1f0b427282c648d411d0ca4f81f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "100371215"
---
# <a name="load-1-tb-into-azure-synapse-analytics-under-15-minutes-with-data-factory"></a>Загрузка 1 ТБ в Azure синапсе Analytics в течение 15 минут с помощью фабрики данных
> [!NOTE]
> В этой статье рассматривается служба "Фабрика данных Azure" версии 1. Если вы используете текущую версию службы фабрики данных, см. статью [копирование данных в Azure синапсе Analytics или из нее с помощью фабрики данных](../connector-azure-sql-data-warehouse.md).


[Azure синапсе Analytics](../../synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is.md) — это облачная база данных с горизонтальным масштабированием, которая может обрабатывать большие объемы данных, как реляционные, так и нереляционные.  На основе архитектуры массовой параллельной обработки (MPP) Azure синапсе Analytics оптимизирована для рабочих нагрузок хранилища корпоративных данных.  Оно предоставляет эластичность облака и гибкие возможности масштабирования хранилища и вычислительной мощности независимо друг от друга.

Начало работы с Azure синапсе Analytics стало проще, чем когда-либо с помощью **фабрики данных Azure**.  Фабрика данных Azure — это полностью управляемая облачная служба интеграции данных, которую можно использовать для заполнения Azure синапсе Analytics данными из имеющейся системы и сохранения ценного времени при оценке Azure синапсе Analytics и создании решений аналитики. Ниже приведены основные преимущества загрузки данных в Azure синапсе Analytics с помощью фабрики данных Azure.

* **Простота настройки**: вам доступен 5-этапный интуитивно понятный мастер без необходимости создавать сценарии.
* **Поддержка расширенного хранилища данных**: встроенная поддержка широкого набора локальных и облачных хранилищ данных.
* **Безопасность и совместимость**: данные передаются по протоколу HTTPS или ExpressRoute, а наличие глобальной службы гарантирует, что ваши данные никогда не покинут заданных географических границ.
* **Непревзойденная производительность с помощью polybase** — с помощью polybase наиболее эффективный способ перемещения данных в Azure синапсе Analytics. Используя функцию промежуточных больших двоичных объектов, можно достичь высокой скорости загрузки данных из хранилищ всех типов, а не только из хранилища BLOB-объектов Azure, которое по умолчанию поддерживается Polybase.

В этой статье показано, как использовать мастер копирования фабрики данных для загрузки данных объемом 1 ТБ из хранилища BLOB-объектов Azure в Azure синапсе Analytics в течение 15 минут с пропускной способностью более 1,2 Гбит/с.

В этой статье содержатся пошаговые инструкции по перемещению данных в Azure синапсе Analytics с помощью мастера копирования.

> [!NOTE]
>  Общие сведения о возможностях фабрики данных по перемещению данных в Azure синапсе Analytics и обратно см. в статье [Перемещение данных в Azure синапсе Analytics и из нее с помощью фабрики данных Azure](data-factory-azure-sql-data-warehouse-connector.md) .
>
> Вы также можете создавать конвейеры с помощью Visual Studio, PowerShell и т. д. Краткое пошаговое руководство по использованию действия копирования в фабрике данных Azure см. в разделе [учебник. копирование данных из большого двоичного объекта Azure в базу данных SQL Azure](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) .  
>
>

## <a name="prerequisites"></a>Предварительные условия
* Хранилище BLOB-объектов Azure: в этом эксперименте хранилище BLOB-объектов Azure (GRS) используется для хранения тестового набора данных TPC-H.  Если у вас нет учетной записи хранения Azure, узнайте, как [создать учетную запись хранения](../../storage/common/storage-account-create.md).
* Данные [TPC-H](http://www.tpc.org/tpch/): в качестве тестового набора данных мы будем использовать TPC-H.  Для этого необходимо использовать `dbgen` из набора средств TPC-H. Это поможет создать набор данных.  Можно скачать исходный код `dbgen` из [инструментов TPC](http://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp) и скомпилировать его или скачать скомпилированный двоичный файл с сайта [GitHub](https://github.com/Azure/Azure-DataFactory/tree/master/SamplesV1/TPCHTools).  Выполните dbgen.exe с приведенными ниже командами, чтобы создать неструктурированный файл размером в 1 ТБ для таблицы `lineitem`, распределенной на 10 файлов.

  * `Dbgen -s 1000 -S **1** -C 10 -T L -v`
  * `Dbgen -s 1000 -S **2** -C 10 -T L -v`
  * …
  * `Dbgen -s 1000 -S **10** -C 10 -T L -v`

    Теперь скопируйте созданные файлы в большой двоичный объект Azure.  Ознакомьтесь с разделом [Перемещение данных в локальную файловую систему или из нее с помощью фабрики данных Azure](data-factory-onprem-file-system-connector.md), чтобы узнать, как это можно сделать с помощью фабрики данных Azure.    
* Azure синапсе Analytics. Этот эксперимент загружает данные в Azure синапсе Analytics, созданную с помощью 6 000 DWU

    Подробные инструкции по созданию базы данных Azure синапсе Analytics см. в статье [Создание Azure синапсе Analytics](../../synapse-analytics/sql-data-warehouse/create-data-warehouse-portal.md) .  Чтобы добиться максимальной производительности нагрузки в Azure синапсе Analytics с помощью polybase, мы выбираем максимальное количество единиц использования хранилища данных (Dwu), допустимых в параметре производительности, 6 000 DWU.

  > [!NOTE]
  > При загрузке из большого двоичного объекта Azure производительность загрузки данных прямо пропорциональна количеству DWU, настроенных для Azure синапсе Analytics:
  >
  > Загрузка 1 ТБ в 1 000 DWU Azure синапсе Analytics занимает 87 минут (~ 200 Мбит/с) Загрузка 1 ТБ в 2 000 DWU Azure синапсе Analytics занимает 46 минут (пропускная способность ~ 380 Мбит/с). Загрузка 1 ТБ в 6 000 DWU Azure синапсе Analytics занимает 14 минут (~ 1,2 Гбит/с)
  >
  >

    Чтобы создать выделенный пул SQL с 6 000 DWU, переместите ползунок производительности в нужное положение.

    ![Ползунок "Производительность"](media/data-factory-load-sql-data-warehouse/performance-slider.png)

    В случае, если для существующей базы данных не настроено 6000 DWU, ее можно масштабировать с помощью портала Azure.  Перейдите к базе данных на портале Azure. На панели **Обзор** имеется кнопка **Масштаб**, как показано на следующем рисунке.

    ![Кнопка "Масштаб"](media/data-factory-load-sql-data-warehouse/scale-button.png)    

    Нажмите кнопку **Масштаб**, чтобы открыть приведенную ниже панель, передвиньте ползунок до максимального значения и нажмите кнопку **Сохранить**.

    ![Диалоговое окно "Масштаб"](media/data-factory-load-sql-data-warehouse/scale-dialog.png)

    Этот эксперимент загружает данные в Azure синапсе Analytics с помощью `xlargerc` класса ресурсов.

    Чтобы добиться максимальной пропускной способности, необходимо выполнить копирование с помощью пользователя Azure синапсе Analytics, принадлежащего `xlargerc` классу ресурсов.  Узнайте, как это сделать, ознакомившись с [примером изменения класса ресурсов пользователя](../../synapse-analytics/sql-data-warehouse/resource-classes-for-workload-management.md).  
* Создайте схему целевой таблицы в базе данных Azure синапсе Analytics, выполнив следующую инструкцию DDL:

    ```SQL  
    CREATE TABLE [dbo].[lineitem]
    (
        [L_ORDERKEY] [bigint] NOT NULL,
        [L_PARTKEY] [bigint] NOT NULL,
        [L_SUPPKEY] [bigint] NOT NULL,
        [L_LINENUMBER] [int] NOT NULL,
        [L_QUANTITY] [decimal](15, 2) NULL,
        [L_EXTENDEDPRICE] [decimal](15, 2) NULL,
        [L_DISCOUNT] [decimal](15, 2) NULL,
        [L_TAX] [decimal](15, 2) NULL,
        [L_RETURNFLAG] [char](1) NULL,
        [L_LINESTATUS] [char](1) NULL,
        [L_SHIPDATE] [date] NULL,
        [L_COMMITDATE] [date] NULL,
        [L_RECEIPTDATE] [date] NULL,
        [L_SHIPINSTRUCT] [char](25) NULL,
        [L_SHIPMODE] [char](10) NULL,
        [L_COMMENT] [varchar](44) NULL
    )
    WITH
    (
        DISTRIBUTION = ROUND_ROBIN,
        CLUSTERED COLUMNSTORE INDEX
    )
    ```
  Мы выполнили необходимые предварительные действия и готовы к настройке действия копирования с помощью мастера копирования.

## <a name="launch-copy-wizard"></a>Запуск мастера копирования
1. Войдите на [портал Azure](https://portal.azure.com).
2. В верхнем левом углу щелкните **Создать ресурс**, выберите **Data + Analytics** (Данные и аналитика), а затем — **Фабрика данных**.
3. На панели **Новая фабрика данных** сделайте следующее:

   1. В поле **Имя** введите **LoadIntoSQLDWDataFactory**.
       Имя фабрики данных Azure должно быть глобально уникальным. Если возникает ошибка: **имя фабрики данных "LoadIntoSQLDWDataFactory" недоступно**, измените имя фабрики данных (например, йоурнамелоадинтосклдвдатафактори) и повторите попытку создания. Ознакомьтесь с разделом [Фабрика данных — правила именования](data-factory-naming-rules.md) , чтобы узнать о правилах именования артефактов фабрики данных.  
   2. Выберите свою **подписку Azure**.
   3. Для группы ресурсов выполните одно из следующих действий.
      1. а) выберите **Использовать существующую** и укажите имеющуюся группу ресурсов;
      2. выберите **Создать** и введите имя для группы ресурсов.
   4. Укажите **расположение** фабрики данных.
   5. Установите флажок **Закрепить на панели мониторинга** в нижней части колонки.  
   6. Нажмите кнопку **Создать**.
4. После создания вы увидите колонку **Фабрика данных**, как показано на рисунке ниже.

   ![Домашняя страница фабрики данных](media/data-factory-load-sql-data-warehouse/data-factory-home-page-copy-data.png)
5. Чтобы запустить **мастер копирования**, на домашней странице фабрики данных щелкните **Копирование данных**.

   > [!NOTE]
   > Если веб-браузер завис на действии "Авторизация...", отключите параметр или снимите флажок **Block third party cookies and site data** (Блокировать сторонние файлы cookie и данные сайта). Либо оставьте флажок и создайте исключение для адреса **login.microsoftonline.com**, а затем попробуйте запустить мастер еще раз.
   >
   >

## <a name="step-1-configure-data-loading-schedule"></a>Шаг 1. Настройка расписания загрузки данных
Первым шагом является настройка расписания загрузки данных.  

Вот что нужно сделать на странице **Свойства** :

1. В качестве **имени задачи** введите **CopyFromBlobToAzureSqlDataWarehouse**.
2. Выберите параметр **Run once now** (Запустить сейчас один раз).   
3. Щелкните **Далее**.  

    ![Мастер копирования — страница "Свойства"](media/data-factory-load-sql-data-warehouse/copy-wizard-properties-page.png)

## <a name="step-2-configure-source"></a>Шаг 2. Настройка источника
В этом разделе описываются шаги для настройки источника: большого двоичного объекта Azure, содержащего файлы размером в 1 ТБ с элементами строк TPC-H.

1. Выберите **хранилище BLOB-объектов Azure** в качестве хранилища и нажмите кнопку **Далее**.

    ![Мастер копирования — страница "Выбрать источник"](media/data-factory-load-sql-data-warehouse/select-source-connection.png)

2. Укажите сведения о подключении для учетной записи хранения BLOB-объектов Azure и нажмите кнопку **Далее**.

    ![Мастер копирования — сведения о подключении к источнику](media/data-factory-load-sql-data-warehouse/source-connection-info.png)

3. Выберите **папку**, содержащую файлы с элементами строк TPC-H, и нажмите кнопку **Далее**.

    ![Мастер копирования — выбор папки входных данных](media/data-factory-load-sql-data-warehouse/select-input-folder.png)

4. После нажатия кнопки **Далее** параметры формата файлов определяются автоматически.  Убедитесь, что разделителем столбцов является "|", а не запятая "," по умолчанию.  Просмотрев данные, нажмите кнопку **Далее**.

    ![Мастер копирования — параметры формата файлов](media/data-factory-load-sql-data-warehouse/file-format-settings.png)

## <a name="step-3-configure-destination"></a>Шаг 3. Настройка назначения
В этом разделе показано, как настроить назначение: `lineitem` Table в базе данных Azure синапсе Analytics.

1. Выберите **Azure синапсе Analytics** в качестве целевого хранилища и нажмите кнопку **Далее**.

    ![Мастер копирования — выбор целевого хранилища данных](media/data-factory-load-sql-data-warehouse/select-destination-data-store.png)

2. Введите сведения о подключении для Azure синапсе Analytics.  Обязательно укажите пользователя, который является участником роли `xlargerc` (подробные инструкции приведены в разделе **предварительных требований**), и нажмите кнопку **Далее**.

    ![Мастер копирования — сведения о подключении к целевому хранилищу](media/data-factory-load-sql-data-warehouse/destination-connection-info.png)

3. Выберите целевую таблицу и нажмите кнопку **Далее**.

    ![Мастер копирования — страница сопоставления таблиц](media/data-factory-load-sql-data-warehouse/table-mapping-page.png)

4. На странице сопоставления схемы оставьте флажок "Apply column mapping" (Применить сопоставление столбцов) снятым и нажмите кнопку **Далее**.

## <a name="step-4-performance-settings"></a>Шаг 4. Настройки производительности

Флажок **Allow polybase** (Разрешить использование PolyBase) установлен по умолчанию.  Щелкните **Далее**.

![Мастер копирования — страница сопоставления столбцов](media/data-factory-load-sql-data-warehouse/performance-settings-page.png)

## <a name="step-5-deploy-and-monitor-load-results"></a>Шаг 5. Развертывание и мониторинг результатов нагрузки
1. Нажмите кнопку **Готово**, чтобы осуществить развертывание.

    ![Мастер копирования — страница сводки 1](media/data-factory-load-sql-data-warehouse/summary-page.png)

2. После завершения развертывания щелкните `Click here to monitor copy pipeline`, чтобы отслеживать ход выполнения копирования. Выберите конвейер копирования, созданный при работе со списком **Окна действий**.

    ![Мастер копирования-страница "Сводка" 2](media/data-factory-load-sql-data-warehouse/select-pipeline-monitor-manage-app.png)

    Можно просмотреть сведения о выполнении копирования в **Activity Window Explorer** в правой панели, в том числе объем данных, считанных из источника и записанных в назначение, продолжительность и среднюю пропускную способность копирования.

    Как видно на следующем снимке экрана, копирование 1 ТБ из хранилища BLOB-объектов Azure в Azure синапсе Analytics заняло 14 минут, что эффективно достиг пропускной способности 1,22 Гбит/с.

    ![Мастер копирования — диалоговое окно успешного выполнения](media/data-factory-load-sql-data-warehouse/succeeded-info.png)

## <a name="best-practices"></a>Рекомендации
Ниже приведены некоторые рекомендации по запуску базы данных Azure синапсе Analytics.

* При загрузке в кластеризованный индекс columnstore используйте класс ресурсов большего размера.
* Чтобы повысить эффективность соединений, рассмотрите возможность использования хэш-распределения по выбранному столбцу вместо используемого по умолчанию циклического распределения.
* Чтобы ускорить загрузку, рекомендуется использовать кучу для хранения временных данных.
* После завершения загрузки в Azure синапсе Analytics создайте статистику.

Дополнительные сведения см. в статье рекомендации [по Azure синапсе Analytics](../../synapse-analytics/sql-data-warehouse/sql-data-warehouse-best-practices.md) .

## <a name="next-steps"></a>Дальнейшие действия
* [Мастер копирования фабрики данных](data-factory-copy-wizard.md). В этой статье приведены сведения о мастере копирования.
* [Руководство по настройке производительности действия копирования](data-factory-copy-activity-performance.md). Эта статья содержит эталонные измерения производительности и руководство по настройке.