---
title: Создание первой фабрики данных (портал Azure)
description: В этом руководстве вы создадите образец конвейера фабрики данных Azure с помощью редактора фабрик данных на портале Azure.
author: dcstwh
ms.author: weetok
ms.reviewer: jburchel
ms.service: data-factory
ms.topic: tutorial
ms.date: 01/22/2018
ms.openlocfilehash: 9794aa0750a886803aac3fec7622f6b3770acf9d
ms.sourcegitcommit: f611b3f57027a21f7b229edf8a5b4f4c75f76331
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/22/2021
ms.locfileid: "104785586"
---
# <a name="tutorial-build-your-first-data-factory-by-using-the-azure-portal"></a>Руководство по Создание первой фабрики данных с помощью портала Azure
> [!div class="op_single_selector"]
> * [Обзор и предварительные требования](data-factory-build-your-first-pipeline.md)
> * [Visual Studio](data-factory-build-your-first-pipeline-using-vs.md)
> * [PowerShell](data-factory-build-your-first-pipeline-using-powershell.md)
> * [Шаблон Azure Resource Manager](data-factory-build-your-first-pipeline-using-arm.md)
> * [REST API](data-factory-build-your-first-pipeline-using-rest-api.md)


> [!NOTE]
> Статья относится к версии 1 фабрики данных Azure, которая является общедоступной. Если вы используете текущую версию службы "Фабрика данных", ознакомьтесь с [кратким руководством по созданию фабрики данных с помощью службы "Фабрика данных"](../quickstart-create-data-factory-dot-net.md).

> [!WARNING]
> Доступный на портале Azure редактор JSON для создания и развертывания конвейеров ADF версии 1 будет отключен 31 июля 2019 г. После 31 июля 2019 г. для создания и развертывания конвейеров ADF версии 1 можно и дальше использовать [командлеты PowerShell для ADF версии 1](/powershell/module/az.datafactory/), [пакет SDK .NET для ADF версии 1](/dotnet/api/microsoft.azure.management.datafactories.models) и [REST API для ADF версии 1](/rest/api/datafactory/).

Из этой статьи вы узнаете, как создать свою первую фабрику данных с помощью [портала Azure](https://portal.azure.com/). Чтобы выполнить приведенные здесь инструкции с помощью других средств или пакетов SDK, выберите в раскрывающемся списке один из доступных вариантов. 

В этом руководстве конвейеру доступно одно действие — действие Hive HDInsight Azure. Это действие запускает сценарий Hive в кластере HDInsight, который преобразует входные данные в выходные. Конвейер запускается раз в месяц по расписанию. Время начала и окончания запуска также указаны. 

> [!NOTE]
> Описанный в этом руководстве конвейер данных преобразовывает входные данные в выходные. Инструкции по копированию данных с помощью службы "Фабрика данных" см. в статье [Копирование данных из хранилища BLOB-объектов Azure в базу данных SQL Azure с помощью Фабрики данных Azure](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).
> 
> Конвейер может содержать сразу несколько действий. Два действия можно объединить в цепочку (выполнить одно действие вслед за другим), настроив выходной набор данных одного действия как входной набор данных другого действия. Дополнительные сведения см. в разделе [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md#multiple-activities-in-a-pipeline).

## <a name="prerequisites"></a>Предварительные требования
Ознакомьтесь с [обзором руководства](data-factory-build-your-first-pipeline.md) и выполните действия, описанные в разделе "Предварительные требования".

Здесь не приводятся общие сведения о службе фабрики данных. Дополнительные сведения о службе см. в статье [Введение в фабрику данных Azure](data-factory-introduction.md).  

## <a name="create-a-data-factory"></a>Создание фабрики данных
Фабрика данных может иметь один или несколько конвейеров. Конвейер может содержать одно или несколько действий. Например, действие копирования, копирующее данные из исходного хранилища данных в целевое. Еще один пример — действие HDInsight Hive, запускающее сценарий Hive, преобразующий входные данные в выходные. 

Чтобы создать фабрику данных, сделайте следующее:

1. Войдите на [портал Azure](https://portal.azure.com/).

1. Выберите **Создать** > **Данные и аналитика** > **Фабрика данных**.

   ![Колонка "Создание"](./media/data-factory-build-your-first-pipeline-using-editor/create-blade.png)

1. В колонке **Новая фабрика данных** в поле **Имя** введите **GetStartedDF**.

   ![Создать колонку "Фабрика данных"](./media/data-factory-build-your-first-pipeline-using-editor/new-data-factory-blade.png)

   > [!IMPORTANT]
   > Имя фабрики данных должно быть глобально уникальным. Если появится сообщение об ошибке "Data factory name GetStartedDF is not available" (Имя GetStartedDF фабрики данных недоступно), измените имя фабрики данных. Например, используйте "ваше_имя_GetStartedDF" и снова создайте фабрику данных. Дополнительные сведения о правилах именования см. в статье [Фабрика данных Azure — правила именования](data-factory-naming-rules.md).
   >
   > В будущем имя фабрики данных может быть зарегистрировано в качестве DNS-имени и может стать отображаемым.
   >
   >
1. В разделе **Подписка** выберите подписку Azure, в рамках которой необходимо создать фабрику данных.

1. Выберите имеющуюся группу ресурсов или создайте новую. Для примера в этом руководстве создайте группу ресурсов с именем **ADFGetStartedRG**.

1. В поле **Расположение** выберите расположение фабрики данных. В раскрывающемся списке отображаются только те регионы, которые поддерживаются службой фабрики данных.

1. Установите флажок **Закрепить на панели мониторинга**.

1. Нажмите кнопку **создания**.

   > [!IMPORTANT]
   > Создавать экземпляры фабрики данных может пользователь с ролью [Участник Data Factory](../../role-based-access-control/built-in-roles.md#data-factory-contributor) на уровне подписки или группы ресурсов.
   >
   >
1. На панели мониторинга вы увидите приведенный ниже элемент с состоянием **Deploying data factory** (Развертывание фабрики данных).    

   ![Состояние Deploying data factory (Развертывание фабрики данных)](./media/data-factory-build-your-first-pipeline-using-editor/creating-data-factory-image.png)

1. После успешного создания фабрики данных ее содержимое отобразится на странице **Фабрика данных**.     

    ![Колонка "Фабрика данных"](./media/data-factory-build-your-first-pipeline-using-editor/data-factory-blade.png)

Прежде чем создавать конвейер в фабрике данных, необходимо создать несколько сущностей этой службы. Чтобы подключить хранилища данных и вычислительные ресурсы к своему хранилищу данных, сначала вам нужно создать связанные службы. Затем следует определить входные и выходные наборы данных для представления входных и выходных данных в связанных хранилищах. После этого создается конвейер с действием, которое использует эти наборы данных.

## <a name="create-linked-services"></a>Создание связанных служб
На этом шаге вы свяжете учетную запись службы хранилища Azure и используемый по запросу кластер HDInsight с фабрикой данных. В этом примере учетная запись хранения содержит входные и выходные данные для конвейера. Для выполнения скрипта Hive, указанного в действии конвейера, в этом примере используется связанная служба HDInsight. Определите, какое [хранилище данных](data-factory-data-movement-activities.md)/или какие [службы вычислений](data-factory-compute-linked-services.md) используются в вашем сценарии. Свяжите эти службы с фабрикой данных, создав связанные службы.  

### <a name="create-a-storage-linked-service"></a>Создание связанной службы хранилища
На этом шаге вы свяжете учетную запись хранения с фабрикой данных. В рамках этого руководства будет использоваться одна и та же учетная запись хранения для хранения входных и выходных данных и файла сценария HQL.

1. В колонке **Фабрика данных** для **GetStartedDF** выберите **Создать и развернуть**. Отобразится редактор фабрики данных.

   ![Плитка "Создание и развертывание"](./media/data-factory-build-your-first-pipeline-using-editor/data-factory-author-deploy.png)

1. Выберите **Новое хранилище данных** и **Служба хранилища Azure**.

   ![Колонка "Новое хранилище данных"](./media/data-factory-build-your-first-pipeline-using-editor/new-data-store-azure-storage-menu.png)

1. В редакторе отобразится сценарий JSON для создания связанной службы хранилища.

   ![Связанная служба хранилища](./media/data-factory-build-your-first-pipeline-using-editor/azure-storage-linked-service.png)

1. Замените **account name** именем своей учетной записи хранения. Замените **ключ учетной записи** ключом вашей учетной записи хранения. Сведения о том, как получить ключ доступа к хранилищу, см. в статье [Управление ключами доступа к хранилищу](../../storage/common/storage-account-keys-manage.md).

1. Чтобы развернуть связанную службу, выберите **Развернуть** на панели команд.

    ![Кнопка «Развернуть»](./media/data-factory-build-your-first-pipeline-using-editor/deploy-button.png)

   После успешного развертывания связанной службы окно Draft-1 исчезнет. Экземпляр **AzureStorageLinkedService** отобразится в представлении в виде дерева слева.

    ![AzureStorageLinkedService](./media/data-factory-build-your-first-pipeline-using-editor/StorageLinkedServiceInTree.png)    

### <a name="create-an-hdinsight-linked-service"></a>Создание связанной службы HDInsight
На этом шаге вы свяжете используемый по запросу кластер HDInsight с фабрикой данных. Кластер HDInsight создается автоматически во время выполнения. После завершения обработки и простоя в течение указанного времени кластер удаляется.

1. В редакторе фабрики данных выберите **More** (Дополнительно)  > **Новое вычисление** > **On-demand HDInsight cluster** (Кластер HDInsight по запросу).

    ![Новая служба вычислений](./media/data-factory-build-your-first-pipeline-using-editor/new-compute-menu.png)

1. Вставьте следующий фрагмент в окно Draft-1. Фрагмент кода JSON описывает свойства, которые будут использоваться для создания кластера HDInsight по запросу.

    ```JSON
    {
        "name": "HDInsightOnDemandLinkedService",
        "properties": {
            "type": "HDInsightOnDemand",
            "typeProperties": {
                "version": "3.5",
                "clusterSize": 1,
                "timeToLive": "00:05:00",
                "osType": "Linux",
                "linkedServiceName": "AzureStorageLinkedService"
            }
        }
    }
    ```

    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

   | Свойство | Описание |
   |:--- |:--- |
   | clusterSize |Указывает размер кластера HDInsight. |
   | timeToLive | Указывает, сколько времени может простаивать кластер HDInsight, прежде чем он будет удален. |
   | linkedServiceName | Указывает имя учетной записи хранения, в которой будут храниться журналы, создаваемые HDInsight. |

    Обратите внимание на следующие моменты.

     а. С помощью свойств JSON фабрика данных создает кластер HDInsight под управлением Linux. Дополнительные сведения см. в разделе [Связанная служба Azure HDInsight по запросу](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service).

     b. Вместо кластера HDInsight по запросу можно использовать собственный кластер HDInsight. Дополнительные сведения см. в разделе [Связанная служба Azure HDInsight](data-factory-compute-linked-services.md#azure-hdinsight-linked-service).

     c. Кластер HDInsight создает контейнер по умолчанию в хранилище BLOB-объектов, указанном в свойстве JSON (**linkedServiceName**). При удалении кластера HDInsight этот контейнер не удаляется. В этом весь замысел. Если используется связанная служба HDInsight по запросу, кластер HDInsight создается при каждой обработке среза данных (если не используется динамический кластер **timeToLive**). После завершения обработки кластер автоматически удаляется.

     По мере обработки новых срезов количество контейнеров в хранилище BLOB-объектов будет увеличиваться. Если эти контейнеры не используются для устранения неполадок с заданиями, удалите их — это позволит сократить расходы на хранение. Имена контейнеров указаны в формате "adf **имя_фабрики_данных**-**имя_связанной_службы**-метка_даты_и_времени". Для удаления контейнеров в хранилище BLOB-объектов используйте такие инструменты, как [Обозреватель службы хранилища Azure](https://storageexplorer.com/).

     Дополнительные сведения см. в разделе [Связанная служба Azure HDInsight по запросу](data-factory-compute-linked-services.md#azure-hdinsight-on-demand-linked-service).

1. Чтобы развернуть связанную службу, выберите **Развернуть** на панели команд.

    ![Параметр развертывания](./media/data-factory-build-your-first-pipeline-using-editor/ondemand-hdinsight-deploy.png)

1. Убедитесь, что обе службы (**AzureStorageLinkedService** и **HDInsightOnDemandLinkedService**) отображаются в представлении в виде дерева слева.

    ![Снимок экрана: связанные службы AzureStorageLinkedService и HDInsightOnDemandLinkedService](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-linked-services.png)

## <a name="create-datasets"></a>Создание наборов данных
На этом шаге вы создадите наборы данных, которые представляют входные и выходные данные для обработки Hive. Эти наборы данных ссылаются на службу AzureStorageLinkedService, созданную ранее в ходе работы с этим руководством. Связанная служба указывает на учетную запись хранения. Наборы данных указывают контейнер, папку и имя файла в хранилище, которое содержит входные и выходные данные.   

### <a name="create-the-input-dataset"></a>Создание входного набора данных
1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новый набор данных** > **Хранилище BLOB-объектов Azure**.

    ![Новый набор данных](./media/data-factory-build-your-first-pipeline-using-editor/new-data-set.png)

1. Вставьте следующий фрагмент в окно Draft-1. Во фрагменте JSON создается набор данных с именем **AzureBlobInput**, представляющий входные данные для действия в конвейере. Кроме того, нужно указать, что входные данные размещаются в контейнере BLOB-объектов **adfgetstarted** и в папке **inputdata**.

    ```JSON
    {
        "name": "AzureBlobInput",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "fileName": "input.log",
                "folderPath": "adfgetstarted/inputdata",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Month",
                "interval": 1
            },
            "external": true,
            "policy": {}
        }
    }
    ```
    В следующей таблице приведены описания свойств JSON, используемых в этом фрагменте кода.

   | Свойство | Где находится | Описание |
   |:--- |:--- |:--- |
   | type | properties |Для свойства типа задано значение **AzureBlob**, так как данные хранятся в хранилище BLOB-объектов. |
   | linkedServiceName | format |Ссылается на созданную ранее службу AzureStorageLinkedService. |
   | folderPath | typeProperties | Определяет контейнер больших двоичных объектов и папку, которая содержит входные большие двоичные объекты. | 
   | fileName | typeProperties |Это необязательное свойство. Если это свойство не указано, выбираются все файлы из папки folderPath. В этом руководстве обрабатывается только файл input.log. |
   | type | format |Файлы журнала представлены в текстовом формате, поэтому используйте значение **TextFormat**. |
   | columnDelimiter | format |Столбцы в файлах журнала разделяются запятыми (`,`). |
   | frequency/interval | availability |Для свойства frequency задано значение **Month**, а для свойства interval — значение **1**. Это означает, что срезы входных данных доступны ежемесячно. |
   | external | properties | Это свойство имеет значение **true**, если этот конвейер не создает входные данные. В этом руководстве файл input.log не создается этим конвейером, поэтому мы присвоим этому свойству значение **true**. |

    Дополнительные сведения об этих свойствах JSON см. в [этом разделе](data-factory-azure-blob-connector.md#dataset-properties).

1. На панели команд выберите **Развернуть**, чтобы развернуть только что созданный набор данных. Вы увидите набор данных в представлении в виде дерева слева.

### <a name="create-the-output-dataset"></a>Создание выходного набора данных
Теперь создайте выходной набор данных, представляющий выходные данные, которые хранятся в хранилище BLOB-объектов.

1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новый набор данных** > **Хранилище BLOB-объектов Azure**.

1. Вставьте следующий фрагмент в окно Draft-1. Этот фрагмент кода JSON создает набор данных с именем **AzureBlobOutput** и определяет структуру данных, получаемых с помощью сценария Hive. Кроме того, нужно указать, что результаты будут храниться в контейнере больших двоичных объектов с именем **adfgetstarted** и в папке с именем **partitioneddata**. В разделе **availability** указывается частота, с которой ежемесячно будет создаваться выходной набор данных.

    ```JSON
    {
      "name": "AzureBlobOutput",
      "properties": {
        "type": "AzureBlob",
        "linkedServiceName": "AzureStorageLinkedService",
        "typeProperties": {
          "folderPath": "adfgetstarted/partitioneddata",
          "format": {
            "type": "TextFormat",
            "columnDelimiter": ","
          }
        },
        "availability": {
          "frequency": "Month",
          "interval": 1
        }
      }
    }
    ```
    Описание этих свойств можно найти в разделе "Создание входного набора данных". Значение свойства external для выходного набора данных не указывается, так как набор данных создается службой фабрики данных.

1. На панели команд выберите **Развернуть**, чтобы развернуть только что созданный набор данных.

1. Убедитесь, что набор данных успешно создан.

    ![Иерархическое представление со связанными службами](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-data-set.png)

## <a name="create-a-pipeline"></a>Создание конвейера
На этом шаге вы создадите свой первый конвейер с действием HDInsight Hive. Входной срез данных доступен ежемесячно (для частоты задано значение Month, а для интервала — 1). Выходной срез данных создается ежемесячно. Свойство scheduler для действия указывается ежемесячно. Параметры выходного набора данных (outputs) и планировщика действия (scheduler) должны совпадать. Сейчас расписание активируется с помощью выходного набора данных, поэтому его необходимо создать, даже если действие не создает никаких выходных данных. Если действие не принимает никаких входных данных, входной набор данных можно не создавать. Свойства, используемые в следующем фрагменте кода JSON, описаны в конце этого раздела.

1. В редакторе фабрики данных выберите **More** (Дополнительно) > **Новый конвейер**.

    ![Параметр "Новый конвейер"](./media/data-factory-build-your-first-pipeline-using-editor/new-pipeline-button.png)

1. Вставьте следующий фрагмент в окно Draft-1.

   > [!IMPORTANT]
   > Во фрагменте кода JSON замените свойство **storageaccountname** именем своей учетной записи хранения.
   >
   >

    ```JSON
    {
        "name": "MyFirstPipeline",
        "properties": {
            "description": "My first Azure Data Factory pipeline",
            "activities": [
                {
                    "type": "HDInsightHive",
                    "typeProperties": {
                        "scriptPath": "adfgetstarted/script/partitionweblogs.hql",
                        "scriptLinkedService": "AzureStorageLinkedService",
                        "defines": {
                            "inputtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/inputdata",
                            "partitionedtable": "wasb://adfgetstarted@<storageaccountname>.blob.core.windows.net/partitioneddata"
                        }
                    },
                    "inputs": [
                        {
                            "name": "AzureBlobInput"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "AzureBlobOutput"
                        }
                    ],
                    "policy": {
                        "concurrency": 1,
                        "retry": 3
                    },
                    "scheduler": {
                        "frequency": "Month",
                        "interval": 1
                    },
                    "name": "RunSampleHiveActivity",
                    "linkedServiceName": "HDInsightOnDemandLinkedService"
                }
            ],
            "start": "2017-07-01T00:00:00Z",
            "end": "2017-07-02T00:00:00Z",
            "isPaused": false
        }
    }
    ```

    Этот фрагмент создает конвейер из одного действия, использующего Hive для обработки данных в кластере HDInsight.

    Файл сценария Hive **partitionweblogs.hql** хранится в учетной записи хранения, которая задается с помощью свойства scriptLinkedService с именем **AzureStorageLinkedService**. Он находится в папке **script** в контейнере **adfgetstarted**.

    Раздел **defines** используется, чтобы указать параметры среды выполнения, передаваемые сценарию Hive в качестве значений конфигурации Hive. Примеры: ${hiveconf:inputtable} и ${hiveconf:partitionedtable}.

    Активный период конвейера задается с помощью свойств **start** и **end**.

    В действии JSON укажите, что сценарий Hive будет выполняться в вычислительной среде, указанной в свойстве **linkedServiceName**: **HDInsightOnDemandLinkedService**.

   > [!NOTE]
   > Сведения о свойствах JSON, используемых в этом примере, см. в разделе "Конвейер JSON" статьи [Конвейеры и действия в фабрике данных Azure](data-factory-create-pipelines.md).
   >
   >
1. Проверьте следующее:

   а. В папке **inputdata** контейнера **adfgetstarted** в хранилище BLOB-объектов есть файл **input.log**.

   b. В папке **script** контейнера **adfgetstarted** в хранилище BLOB-объектов есть файл **partitionweblogs.hql**. Выполните необходимые действия, описанные в разделе "Предварительные требования" [обзора руководства](data-factory-build-your-first-pipeline.md), если этих файлов нет в указанных папках.

   c. В JSON-файле конвейера замените свойство **storageaccountname** именем своей учетной записи хранения.

1. Чтобы развернуть конвейер, нажмите кнопку **Развернуть** на панели команд. Так как время в свойствах **start** и **end** задано в прошлом, а для свойства **isPaused** задано значение **false**, конвейер (действие в конвейере) запускается сразу после развертывания.

1. Убедитесь, что конвейер отображается в иерархической структуре.

    ![Иерархическое представление с конвейером](./media/data-factory-build-your-first-pipeline-using-editor/tree-view-pipeline.png)



## <a name="monitor-a-pipeline"></a>Мониторинг конвейера
### <a name="monitor-a-pipeline-by-using-the-diagram-view"></a>Мониторинг конвейера с помощью представления схемы
1. В колонке **Фабрика данных** щелкните **Схема**.

    ![Плитка "Схема"](./media/data-factory-build-your-first-pipeline-using-editor/diagram-tile.png)

1. В представлении **схемы** вы увидите все конвейеры и наборы данных, используемые в этом руководстве.

    ![Представление схемы](./media/data-factory-build-your-first-pipeline-using-editor/diagram-view-2.png)

1. Чтобы просмотреть все действия в конвейере, щелкните конвейер в схеме правой кнопкой мыши и выберите пункт **Открыть конвейер**.

    ![Откройте меню конвейера](./media/data-factory-build-your-first-pipeline-using-editor/open-pipeline-menu.png)

1. Убедитесь, что **действие Hive** отображается в конвейере.

    ![Откройте представление конвейера](./media/data-factory-build-your-first-pipeline-using-editor/open-pipeline-view.png)

    Чтобы перейти к предыдущему представлению, щелкните **Фабрика данных** в меню навигации вверху.

1. В представлении **схемы** дважды щелкните набор данных **AzureBlobInput**. Убедитесь, что срез находится в состоянии **Готово**. Для отображения состояния **Готово** может потребоваться несколько минут. Если это не произойдет через некоторое время, убедитесь, что входной файл (**input.log**) расположен в правильном контейнере (**adfgetstarted**) и папке (**inputdata**).

   ![Срез входных данных в состоянии "Готово"](./media/data-factory-build-your-first-pipeline-using-editor/input-slice-ready.png)

1. Закройте колонку **AzureBlobInput**.

1. В представлении **схемы** дважды щелкните набор данных **AzureBlobOutput**. Вы увидите срез, который сейчас обрабатывается.

   ![Выполнение обработки набора данных](./media/data-factory-build-your-first-pipeline-using-editor/dataset-blade.png)

1. После завершения обработки вы увидите, что срез находится в состоянии **Готово**.

   ![Набор данных в состоянии "Готово"](./media/data-factory-build-your-first-pipeline-using-editor/dataset-slice-ready.png)  

   > [!IMPORTANT]
   > Создание используемого по требованию кластера HDInsight обычно занимает около 20 минут. Конвейер обработает срез примерно через 30 минут.
   >
   >

1. Когда срез перейдет в состояние **Готово**, проверьте выходные данные в папке **partitioneddata** контейнера **adfgetstarted** в хранилище BLOB-объектов.  

   ![Выходные данные](./media/data-factory-build-your-first-pipeline-using-editor/three-ouptut-files.png)

1. Щелкните срез, чтобы просмотреть сведения о нем в колонке **Срез данных**.

    ![Сведения о срезе данных](./media/data-factory-build-your-first-pipeline-using-editor/data-slice-details.png)

1. В списке **Выполнения действий** выберите выполнение действия, чтобы просмотреть дополнительные сведения о нем. (В этом сценарии это действие Hive.) Эта информация отображается в колонке **Подробности о выполнении операции**.   

    ![Окно "Подробности о выполнении операции"](./media/data-factory-build-your-first-pipeline-using-editor/activity-window-blade.png)    

   В файлах журналов содержатся сведения о выполненном запросе Hive и его состоянии. Эти журналы полезны при устранении неполадок.
   Дополнительные сведения см. в статье [Мониторинг конвейеров фабрики данных Azure и управление ими](data-factory-monitor-manage-pipelines.md).

> [!IMPORTANT]
> В случае успешной обработки среза входной файл удаляется. Если вы хотите повторно обработать срез или еще раз выполнить инструкции из руководства, передайте входной файл (**input.log**) в папку **inputdata** в контейнере **adfgetstarted**.
>
>

### <a name="monitor-a-pipeline-by-using-the-monitor--manage-app"></a>Мониторинг конвейера с помощью приложения по мониторингу и управлению
Для мониторинга конвейеров также можно использовать приложение по мониторингу и управлению. Чтобы узнать больше об использовании этого приложения, ознакомьтесь со статьей [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью приложения для мониторинга и управления](data-factory-monitor-manage-app.md).

1. Щелкните плитку **Monitor & Manage** (Мониторинг и управление) на домашней странице фабрики данных.

    ![Плитка Monitor & Manage (Мониторинг и управление)](./media/data-factory-build-your-first-pipeline-using-editor/monitor-and-manage-tile.png)

1. В приложении по мониторингу и управлению приведите значения параметров **времени начала** и **времени окончания** для конвейера. Нажмите кнопку **Применить**.

    ![Приложение по мониторингу и управлению](./media/data-factory-build-your-first-pipeline-using-editor/monitor-and-manage-app.png)

1. Выберите окно действия в списке **Activity Windows** (Окна действий), чтобы просмотреть сведения о нем.

    ![Список "Activity Windows" (Окна действий)](./media/data-factory-build-your-first-pipeline-using-editor/activity-window-details.png)

## <a name="summary"></a>Сводка
Следуя инструкциям из этого руководства, вы создали фабрику данных для обработки данных путем выполнения сценария Hive в кластере Hadoop HDInsight. Вы использовали редактор фабрики данных на портале Azure для выполнения следующих действий:  

* Создали фабрику данных.
* Создание двух связанных служб:
   * Служба хранилища — связанная служба для связывания хранилища BLOB-объектов, которое содержит входные и выходные файлы, с фабрикой данных.
   * HDInsight — связанная служба по запросу для связывания кластера HDInsight Hadoop с фабрикой данных. Фабрика данных своевременно создает кластер HDInsight Hadoop для обработки входных данных и генерирования выходных данных.
* Создание двух наборов данных, которые описывают входные и выходные данные для действия HDInsight Hive в конвейере.
* Создание конвейера с действием HDInsight Hive.

## <a name="next-steps"></a>Дальнейшие действия
В этой статье вы создали конвейер с действием преобразования (действие HDInsight), которое выполняет сценарий Hive в кластере HDInsight по требованию. Сведения о копировании данных из хранилища BLOB-объектов в Базу данных SQL Azure с помощью действия копирования см. в [этом руководстве](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md).

## <a name="see-also"></a>См. также раздел
| Раздел | Описание |
|:--- |:--- |
| [Конвейеры](data-factory-create-pipelines.md) |Эта статья поможет вам понять сущность конвейеров и действий в фабрике данных, а также научиться с их помощью создавать комплексные рабочие процессы, управляемые данными, для конкретных бизнес-сценариев. |
| [Наборы данных](data-factory-create-datasets.md) |Эта статья поможет вам понять, что такое наборы данных в фабрике данных. |
| [Планирование и исполнение с использованием фабрики данных](data-factory-scheduling-and-execution.md) |Здесь объясняются аспекты планирования и исполнения в модели приложений фабрики данных. |
| [Мониторинг конвейеров фабрики данных Azure и управление ими с помощью приложения для мониторинга и управления](data-factory-monitor-manage-app.md) |В этой статье описывается мониторинг и отладка конвейеров, а также управление ими с помощью приложения мониторинга и управления. |