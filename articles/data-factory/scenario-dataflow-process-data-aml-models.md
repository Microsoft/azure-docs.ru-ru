---
title: Использование потоков данных для обработки данных из моделей автоматизированного машинного обучения (Аутомл)
description: Узнайте, как использовать потоки данных фабрики данных Azure для обработки данных из моделей автоматизированного машинного обучения (Аутомл).
services: data-factory
author: amberz
co-author: ATLArcht
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 1/31/2021
ms.author: amberz
ms.co-author: Donnana
ms.openlocfilehash: 45cd44cc0678b7f3a006a88bf66be2bca091af76
ms.sourcegitcommit: 772eb9c6684dd4864e0ba507945a83e48b8c16f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "104595385"
---
# <a name="process-data-from-automated-machine-learning-models-by-using-data-flows"></a>Обработка данных из автоматических моделей машинного обучения с помощью потоков данных

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Автоматизированное машинное обучение (Аутомл) используется проектами машинного обучения для обучения, настройки и получения лучших моделей автоматически с помощью целевых метрик, указанных для классификации, регрессии и прогнозирования временных рядов.

Одной из трудностей для Аутомл является то, что необработанные данные из хранилища данных или транзакционной базы данных будут представлять собой огромный набор данных, возможно 10 ГБ. Большой набор данных требует больше времени для обучения моделей, поэтому рекомендуется оптимизировать обработку данных перед обучением Машинное обучение Azure моделей. В этом руководстве описано, как с помощью фабрики данных Azure секционировать набор данных в Аутомл-файлы для набора данных Машинное обучение.

Проект Аутомл включает следующие три сценария обработки данных:

* Разбейте большие данные на Аутомл в файлах до начала обучения моделей.

     [Кадр данных Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html) обычно используется для обработки данных перед обучением моделей. Кадр данных Pandas хорошо работает для размеров данных менее 1 ГБ, но если объем данных превышает 1 ГБ, кадр данных Pandas замедляется для обработки данных. Иногда может даже возникнуть сообщение об ошибке нехватки памяти. Мы рекомендуем использовать формат [файла Parquet](https://parquet.apache.org/) для машинного обучения, так как это формат с двоичными столбцами.
    
     Потоки данных сопоставления фабрики данных являются визуально спроектированными преобразованиями данных, которые позволяют специалистам по работе с данными освобождать написание кода. Сопоставление потоков данных — это мощный способ обработки больших данных, так как в конвейере используются масштабируемые кластеры Spark.

* Разделите обучающий набор данных и тестовый набор данных.
    
    Набор данных для обучения будет использоваться для обучающей модели. Тестовый набор данных будет использоваться для вычисления моделей в проекте машинного обучения. Действие условного разбиения для сопоставления потоков данных разбивает обучающие данные и проверочные данные.

* Удалите неполные данные.

    Может потребоваться удалить неполные данные, например файл Parquet с нулевым числом строк. В этом руководстве мы будем использовать статистическое действие для получения числа строк. Число строк будет являться условием удаления неквалифицированных данных.

## <a name="preparation"></a>Подготовка

Используйте следующую таблицу базы данных SQL Azure.

```
CREATE TABLE [dbo].[MyProducts](
    [ID] [int] NULL,
    [Col1] [char](124) NULL,
    [Col2] [char](124) NULL,
    [Col3] datetime NULL,
    [Col4] int NULL

) 

```

## <a name="convert-data-format-to-parquet"></a>Преобразование формата данных в Parquet

Следующий поток данных преобразует таблицу базы данных SQL в формат файла Parquet:

- **Исходный набор данных**: таблица транзакций базы данных SQL.
- **Набор данных приемника**: хранилище BLOB-объектов с форматом Parquet.

## <a name="remove-unqualified-data-based-on-row-count"></a>Удалить неполные данные на основе числа строк

Предположим, нам нужно удалить число строк, которое меньше двух.

1. Чтобы получить количество строк, используйте статистическое действие. Используйте **группирование** в зависимости от Col2 и **статистических выражений** с `count(1)` для счетчика строк.

    ![Снимок экрана, показывающий настройку агрегатного действия для получения числа строк.](./media/scenario-dataflow-process-data-aml-models/aggregate-activity-addrowcount.png)

1. С помощью действия приемника выберите **тип приемника** в качестве **кэша** на вкладке **приемник** . Затем выберите нужный столбец из раскрывающегося списка **Ключевые столбцы** на вкладке **Параметры** .

    ![Снимок экрана, показывающий настройку действия Качесинк для получения количества строк в кэшированном приемнике.](./media/scenario-dataflow-process-data-aml-models/cachesink-activity-addrowcount.png)

1. Используйте действие производный столбец для добавления столбца Count строк в исходный поток. На вкладке **Параметры производного столбца** используйте `CacheSink#lookup` выражение для получения числа строк из качесинк.

    ![Снимок экрана, показывающий настройку действия "производный столбец" для добавления количества строк в Source1.](./media/scenario-dataflow-process-data-aml-models/derived-column-activity-rowcount-source-1.png)

1. Используйте действие Условное разбиение, чтобы удалить неполные данные. В этом примере количество строк основано на столбце Col2. Условие заключается в том, чтобы удалить число строк меньше двух, поэтому будут удалены две строки (ID = 2 и ID = 7). Неполные данные можно сохранить в хранилище BLOB-объектов для управления данными.

    ![Снимок экрана, показывающий настройку действия "Условное разбиение" для получения данных, которые больше или равны двум.](./media/scenario-dataflow-process-data-aml-models/conditionalsplit-greater-or-equal-than-2.png)

> [!NOTE]
>    * Создайте новый источник для получения количества строк, которые будут использоваться в исходном источнике в последующих шагах.
>    * Используйте Качесинк с точки зрения производительности.

## <a name="split-training-data-and-test-data"></a>Разделение обучающих данных и тестовых данных

Мы хотим разделить обучающие данные и проверочные данные для каждой секции. В этом примере для того же значения Col2 получите первые две строки в качестве тестовых данных, а остальные строки — как обучающие данные.

1. Используйте действие окна, чтобы добавить один номер строки столбца для каждой секции. На вкладке **over** выберите столбец для раздела. В этом руководстве мы создадим секционирование для Col2. Задайте порядок на вкладке **Сортировка** , который в этом руководстве будет основан на идентификаторе. Задайте порядок на вкладке **столбцы окна** , чтобы добавить один столбец в качестве номера строки для каждой секции.

    ![Снимок экрана, показывающий настройку действия окна для добавления одного нового столбца в номер строки.](./media/scenario-dataflow-process-data-aml-models/window-activity-add-row-number.png)

1. Используйте действие Условное разбиение, чтобы разделить две верхние строки секций на набор данных теста и остальные строки в набор данных для обучения. На вкладке **Параметры условного разбиения** используйте выражение в `lesserOrEqual(RowNum,2)` качестве условия.

    ![Снимок экрана, показывающий настройку действия условного разбиения для разбиения текущего набора данных на обучающий набор данных и проверочный набор данных.](./media/scenario-dataflow-process-data-aml-models/split-training-dataset-test-dataset.png)

## <a name="partition-the-training-and-test-datasets-with-parquet-format"></a>Разделение обучающих и тестовых наборов данных с помощью формата Parquet

С помощью действия приемника на вкладке **Оптимизация** используйте **уникальное значение для каждой секции** , чтобы задать столбец в качестве ключа столбца для секции.

![Снимок экрана, показывающий настройку действия приемника для задания Секции набора данных для обучения.](./media/scenario-dataflow-process-data-aml-models/partition-training-dataset-sink.png)

Давайте взглянем на всю логику конвейера.

![Снимок экрана, показывающий логику всего конвейера.](./media/scenario-dataflow-process-data-aml-models/entire-pipeline.png)

## <a name="next-steps"></a>Дальнейшие действия

Создайте оставшуюся часть логики потока данных с помощью [сопоставления преобразования потока данных.](concepts-data-flow-overview.md)
