---
title: Использование DataFlow для обработки данных из моделей автоматизированного машинного обучения (Аутомл)
description: Узнайте, как использовать потоки данных фабрики данных Azure для обработки данных из моделей автоматизированного машинного обучения (Аутомл).
services: data-factory
author: amberz
co-author: ATLArcht
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.date: 1/31/2021
ms.author: amberz
ms.co-author: Donnana
ms.openlocfilehash: 494fc2c227b6fe855e96a780d43cc6702722fa94
ms.sourcegitcommit: e972837797dbad9dbaa01df93abd745cb357cde1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/14/2021
ms.locfileid: "100521127"
---
# <a name="process-data-from-automated-machine-learningautoml-models-using-data-flow"></a>Обработка данных из моделей автоматизированного машинного обучения (Аутомл) с помощью потока данных

Автоматизированное машинное обучение (Аутомл) принято в проектах машинного обучения для обучения, настройки и получения лучшей модели с помощью целевой метрики, указанной для классификации, регрессии и прогнозирования временных рядов. 

Одна из проблем — необработанные данные из хранилища данных или транзакционная база данных — огромный набор данных, например: 10 ГБ, большой набор данных требует больше времени для обучения моделей, поэтому рекомендуется оптимизировать обработку данных перед обучением Машинное обучение Azure моделями. Из этого руководства вы узнаете, как использовать ADF для разделения набора данных на Parquet-файлы для Машинное обучение Azure набора данных. 

В проекте автоматизированного машинного обучения (Аутомл) применяются следующие три сценария обработки данных:

* Секционирование больших данных в файлы Parquet перед обучением моделей. 

     [Кадр данных Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html) обычно используется для обработки данных перед моделями обучения. Кадр данных Pandas хорошо работает для размеров данных менее 1 ГБ, но если объем данных превышает 1 ГБ, Pandas кадров данных замедляет обработку данных, что иногда даже повлечет за собой сообщение об ошибке нехватки памяти. Форматы [файлов Parquet](https://parquet.apache.org/) рекомендуются для машинного обучения, так как это формат двоичных столбцов.
    
    Фабрики данных Azure для сопоставления потоков данных — это визуально спроектированные преобразования данных с несамостоятельным кодом для специалистов по работе с данными. Обработка больших данных эффективна, так как конвейер использует масштабируемые кластеры Spark.

* Разделите обучающий набор данных и тестовый набор данных.
    
    Обучающий набор данных будет использоваться для обучения модели, тестовый набор данных будет использоваться для вычисления моделей в проекте машинного обучения. Действие "Условное разбиение потоков данных сопоставления" разбивает обучающие данные и проверочные данные. 

* Удалите неполные данные.

    Может потребоваться удалить неполные данные, например: Parquet file с нулевой строкой. В этом руководстве мы будем использовать статистическое действие для получения количества строк, а число строк будет условием удаления неквалифицированных данных. 


## <a name="preparation"></a>Подготовка
Используйте следующую таблицу базы данных SQL Azure. 
```
CREATE TABLE [dbo].[MyProducts](
    [ID] [int] NULL,
    [Col1] [char](124) NULL,
    [Col2] [char](124) NULL,
    [Col3] datetime NULL,
    [Col4] int NULL

) 

```

## <a name="convert-data-format-to-parquet"></a>Преобразование формата данных в Parquet

Поток данных преобразует таблицу базы данных SQL Azure в формат файла Parquet. 

**Исходный набор данных**: таблица транзакций базы данных SQL Azure

**Набор данных приемника**: хранилище BLOB-объектов с форматом Parquet


## <a name="remove-unqualified-data-based-on-row-count"></a>Удалить неполные данные на основе числа строк

Предположим, нужно удалить число строк меньше 2. 

1. Использовать статистическое действие для получения количества строк: **Группировать по** на основе столбца Col2 и **статистических выражений** с Count (1) для количества строк. 

    ![Настройка агрегатного действия для получения количества строк](./media/scenario-dataflow-process-data-aml-models/aggregate-activity-addrowcount.png)

1. Используйте действие приемника, выберите **тип приемника** в качестве кэша на вкладке **приемник** , а затем выберите нужный столбец из раскрывающегося списка **Ключевые столбцы** на вкладке **Параметры** . 

    ![Настройка действия Качесинк для получения количества строк в кэшированном приемнике](./media/scenario-dataflow-process-data-aml-models/cachesink-activity-addrowcount.png)

1. Действие "производный столбец" используется для добавления столбца счетчика строк в в исходном потоке. На вкладке **Параметры производного столбца** используйте качесинк # Уточняющий запрос, чтобы получить количество строк из синккаче.
    ![Настройка действия "производный столбец" для добавления количества строк в источнике 1](./media/scenario-dataflow-process-data-aml-models/derived-column-activity-rowcount-source-1.png)

1. Используйте действие Условное разбиение для удаления неквалифицированных данных. В этом примере количество строк на основе столбца Col2, а условие состоит в том, чтобы удалить число строк меньше 2, поэтому будут удалены две строки (ID = 2 и ID = 7). Неполные данные можно сохранить в хранилище BLOB-объектов для управления данными. 

    ![Настройка действия "Условное разбиение" для получения данных, которые больше или равны 2](./media/scenario-dataflow-process-data-aml-models/conditionalsplit-greater-or-equal-than-2.png)

> [!NOTE]
>    *    Создайте новый источник для получения количества строк, которые будут использоваться в исходном источнике в последующих шагах. 
>    *    Используйте Качесинк из точки зрения производительности. 

## <a name="split-training-data-and-test-data"></a>Разделение обучающих данных и тестовых данных 

1. Мы хотим разделить обучающие данные и проверочные данные для каждой секции. В этом примере для того же значения Col2 получите первые 2 строки как проверочные данные, а остальные строки — как обучающие данные. 

    Используйте действие окна, чтобы добавить один номер строки столбца для каждой секции. На **вкладке "вверх" выберите** столбец для раздела (в этом руководстве будет секционирование для столбца Col2), а затем на вкладке **Сортировка** (в этом руководстве он будет основываться на идентификаторе для заказа) и на вкладке **столбцы окна** добавьте один столбец в качестве номера строки для каждой секции. 
    ![Настройка действия окна для добавления одного нового столбца в номер строки](./media/scenario-dataflow-process-data-aml-models/window-activity-add-row-number.png)

1. Используйте действие Условное разбиение, чтобы разделить все первые 2 строки секций на проверочный набор данных, а остальные строки — на набор данных для обучения. На вкладке **Параметры условного разбиения** в качестве условия используйте выражение Лессерорекуал (RowNum, 2). 

    ![Настройка действия условного разбиения для разделения текущего набора данных на обучающий набор данных и проверочный набор данных](./media/scenario-dataflow-process-data-aml-models/split-training-dataset-test-dataset.png)

## <a name="partition-training-dataset-and-test-dataset-with-parquet-format"></a>Набор данных для обучения секций и тестовый набор данных в формате Parquet

1. Используйте действие приемника на вкладке " **Оптимизация** ", используя **уникальное значение для каждой секции** , чтобы задать столбец в качестве ключа столбца для секции. 
    ![Настройка действия приемника для задания Секции набора данных для обучения](./media/scenario-dataflow-process-data-aml-models/partition-training-dataset-sink.png)

    Давайте взглянем на всю логику конвейера.
    ![Логика всего конвейера](./media/scenario-dataflow-process-data-aml-models/entire-pipeline.png)


## <a name="next-steps"></a>Дальнейшие шаги

* Создайте оставшуюся часть логики потока данных с помощью [преобразования потоков данных](concepts-data-flow-overview.md)сопоставления.
