---
title: Действие потока данных
description: Выполнение потоков данных внутри конвейера фабрики данных.
services: data-factory
documentationcenter: ''
author: kromerm
ms.service: data-factory
ms.workload: data-services
ms.topic: conceptual
ms.author: makromer
ms.date: 01/03/2021
ms.openlocfilehash: 3eff23a42a6ac5f5360bdebfcc692e13acb3e8b0
ms.sourcegitcommit: 89c0482c16bfec316a79caa3667c256ee40b163f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/04/2021
ms.locfileid: "97858791"
---
# <a name="data-flow-activity-in-azure-data-factory"></a>Действие потока данных в фабрике данных Azure

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Используйте действие потока данных для преобразования и перемещения данных посредством сопоставления потоков данных. Если вы не знакомы с потоками данных, см. раздел [Общие сведения о сопоставлении потока данных](concepts-data-flow-overview.md)

## <a name="syntax"></a>Синтаксис

```json
{
    "name": "MyDataFlowActivity",
    "type": "ExecuteDataFlow",
    "typeProperties": {
      "dataflow": {
         "referenceName": "MyDataFlow",
         "type": "DataFlowReference"
      },
      "compute": {
         "coreCount": 8,
         "computeType": "General"
      },
      "traceLevel": "Fine",
      "runConcurrently": true,
      "continueOnError": true,      
      "staging": {
          "linkedService": {
              "referenceName": "MyStagingLinkedService",
              "type": "LinkedServiceReference"
          },
          "folderPath": "my-container/my-folder"
      },
      "integrationRuntime": {
          "referenceName": "MyDataFlowIntegrationRuntime",
          "type": "IntegrationRuntimeReference"
      }
}

```

## <a name="type-properties"></a>Свойства типа

Свойство | Описание | Допустимые значения | Обязательно
-------- | ----------- | -------------- | --------
DataFlow | Ссылка на выполняемый поток данных | датафловреференце | Да
интегратионрунтиме | Среда вычислений, в которой выполняется поток данных. Если этот параметр не указан, будет использоваться автоматическая разрешающая среда выполнения интеграции Azure. | интегратионрунтимереференце | Нет
COMPUTE. Корекаунт | Число ядер, используемых в кластере Spark. Можно указать, только если используется среда выполнения интеграции Azure с авторазрешением | 8, 16, 32, 48, 80, 144, 272 | Нет
COMPUTE. Компутетипе | Тип вычислений, используемых в кластере Spark. Можно указать, только если используется среда выполнения интеграции Azure с авторазрешением | "General", "Компутеоптимизед", "MemoryOptimized" | Нет
промежуточное хранение. linkedService | Если вы используете источник или приемник Azure синапсе Analytics, укажите учетную запись хранения, используемую для промежуточной базы данных Polybase.<br/><br/>Если в службе хранилища Azure настроена конечная точка службы виртуальной сети, необходимо использовать управляемую проверку подлинности с включенным параметром "разрешить использование доверенной службы Майкрософт" в учетной записи хранения. см. статью [влияние использования конечных точек службы виртуальной сети в службе хранилища Azure](../azure-sql/database/vnet-service-endpoint-rule-overview.md#impact-of-using-virtual-network-service-endpoints-with-azure-storage) Также ознакомьтесь с необходимыми конфигурациями для [больших двоичных объектов Azure](connector-azure-blob-storage.md#managed-identity) и [Azure Data Lake Storage 2-го поколения](connector-azure-data-lake-storage.md#managed-identity) соответственно.<br/> | LinkedServiceReference | Только если поток данных считывает или записывает данные в Azure синапсе Analytics
промежуточное хранение. folderPath | Если вы используете источник или приемник аналитики Azure синапсе, путь к папке в учетной записи хранения BLOB-объектов, используемый для промежуточного хранения Polybase. | Строка | Только при чтении или записи потока данных в Azure синапсе Analytics
traceLevel | Задание уровня ведения журнала для выполнения действия потока данных | Прекрасно, грубая, нет | Нет

![Выполнение потока данных](media/data-flow/activity-data-flow.png "Выполнение потока данных")

### <a name="dynamically-size-data-flow-compute-at-runtime"></a>Динамический размер потока данных, вычисленный во время выполнения

Свойства "ядро" и "тип вычисления" можно динамически задать для изменения размера входящих исходных данных во время выполнения. Используйте такие действия конвейера, как Уточняющий запрос или получение метаданных, чтобы определить размер данных исходного набора данных. Затем используйте команду Добавить динамическое содержимое в свойствах действия потока данных.

![Поток платформа динамических данных](media/data-flow/dyna1.png "Динамический поток данных")

[Ниже приведен краткий видеоролик, объясняющий этот прием.](https://www.youtube.com/watch?v=jWSkJdtiJNM)

### <a name="data-flow-integration-runtime"></a>Среда выполнения интеграции потока данных

Выберите Integration Runtime, который будет использоваться для выполнения действия потока данных. По умолчанию в фабрике данных используется автоматическая разрешающая среда выполнения интеграции Azure с четырьмя рабочими ядрами и без срока жизни (TTL). Этот IR имеет тип вычислений общего назначения и работает в том же регионе, что и фабрика. Вы можете создавать собственные среды выполнения интеграции Azure, определяющие конкретные регионы, тип вычислений, количество ядер и TTL для выполнения действий потока данных.

Для выполнения конвейера кластером является кластер заданий, для запуска которого требуется несколько минут перед началом выполнения. Если TTL не указан, это время запуска необходимо для каждого выполнения конвейера. Если указать срок жизни, горячий пул кластера останется активным в течение времени, указанного после последнего выполнения, что приведет к сокращению времени запуска. Например, если срок жизни составляет 60 минут и поток данных будет выполняться один раз в час, пул кластера останется активным. Дополнительные сведения см. в разделе [Среда выполнения интеграции Azure](concepts-integration-runtime.md).

![Azure Integration Runtime](media/data-flow/ir-new.png "Azure Integration Runtime")

> [!IMPORTANT]
> Выбор Integration Runtime в действии потока данных применяется только к *запущенным выполнениям* конвейера. Отладка конвейера с потоками данных выполняется в кластере, указанном в сеансе отладки.

### <a name="polybase"></a>PolyBase

Если вы используете Azure синапсе Analytics в качестве приемника или источника, необходимо выбрать промежуточное расположение для пакетной загрузки Polybase. Polybase обеспечивает неполное пакетную загрузку, а не загружает данные построчно. Polybase радикально сокращает время загрузки в Azure синапсе Analytics.

## <a name="logging-level"></a>Уровень ведения журнала

Если не требуется, чтобы каждое выполнение конвейера действий потока данных полностью запускало все журналы телеметрии, при необходимости можно задать уровень ведения журнала "базовый" или "нет". При выполнении потоков данных в режиме "подробного" (по умолчанию) вы запрашиваете ADF на полную регистрацию на каждом отдельном уровне секций во время преобразования данных. Это может быть дорогостоящей операцией, поэтому только включение подробных сведений при устранении неполадок может повысить общую производительность потока данных и конвейера. В режиме "базовый" будут регистрироваться только длительности преобразования, а "нет" — только сводка длительностей.

![Уровень ведения журнала](media/data-flow/logging.png "Задать уровень ведения журнала")

## <a name="sink-properties"></a>Свойства приемника

Функция группирования в потоках данных позволяет задавать порядок выполнения приемников, а также группировать приемники, используя один и тот же номер группы. Чтобы упростить управление группами, можно попросить ADF запустить приемники в одной группе параллельно. Можно также задать продолжение работы группы приемников даже после того, как один из приемников обнаружит ошибку.

Поведением приемников потока данных по умолчанию является выполнение каждого приемника последовательно, в последовательном режиме и для сбоя потока данных при обнаружении ошибки в приемнике. Кроме того, все приемники по умолчанию относятся к одной группе, если вы не перейдете в свойства потока данных и не установите различные приоритеты для приемников.

![Свойства приемника](media/data-flow/sink-properties.png "Задание свойств приемника")

## <a name="parameterizing-data-flows"></a>Параметризация потоков данных

### <a name="parameterized-datasets"></a>Параметризованные наборы данных

Если в потоке данных используются параметризованные DataSet, задайте значения параметров на вкладке **Параметры** .

![Выполнение параметров потока данных](media/data-flow/params.png "Параметры")

### <a name="parameterized-data-flows"></a>Параметризованные потоки данных

Если поток данных является параметризованным, задайте динамические значения параметров потока данных на вкладке **Параметры** . Для назначения динамических или литеральных значений параметров можно использовать язык выражений конвейера ADF или язык выражений потока данных. Дополнительные сведения см. в разделе [Параметры потока данных](parameters-data-flow.md).

### <a name="parameterized-compute-properties"></a>Параметризованные свойства вычислений.

Вы можете параметризовать количество ядер или тип вычислений, если вы используете автоматическую разрешающую среду выполнения интеграции Azure и указали значения для COMPUTE. Корекаунт и COMPUTE. Компутетипе.

![Пример выполнения параметра потока данных](media/data-flow/parameterize-compute.png "Пример параметра")

## <a name="pipeline-debug-of-data-flow-activity"></a>Отладка конвейера действия потока данных

Для выполнения конвейера отладки, выполняемого с действием потока данных, необходимо переключиться в режим отладки потока данных с помощью ползунка **отладки потока данных** на верхней панели. Режим отладки позволяет запускать поток данных в активном кластере Spark. Дополнительные сведения см. в статье [Режим отладки](concepts-data-flow-debug-mode.md).

![Кнопка отладки](media/data-flow/debugbutton.png "Кнопка отладки")

Конвейер отладки выполняется для активного кластера отладки, а не для среды выполнения интеграции, указанной в параметрах действия потока данных. При запуске режима отладки можно выбрать среду вычислений для отладки.

## <a name="monitoring-the-data-flow-activity"></a>Наблюдение за действием потока данных

В действии потока данных предусмотрена специальная процедура мониторинга, позволяющая просматривать сведения о секционировании, времени этапа и преобразовании данных. Откройте панель "Мониторинг" с помощью значка очков в разделе " **действия**". Дополнительные сведения см. в разделе [наблюдение за потоками данных](concepts-data-flow-monitoring.md).

### <a name="use-data-flow-activity-results-in-a-subsequent-activity"></a>Использование действия потока данных приводит к последующему действию

Действие потока данных выводит метрики, касающиеся количества строк, записанных в каждый приемник, и строк, считываемых из каждого источника. Эти результаты возвращаются в `output` разделе результата выполнения действия. Возвращаемые метрики имеют формат ниже JSON.

``` json
{
    "runStatus": {
        "metrics": {
            "<your sink name1>": {
                "rowsWritten": <number of rows written>,
                "sinkProcessingTime": <sink processing time in ms>,
                "sources": {
                    "<your source name1>": {
                        "rowsRead": <number of rows read>
                    },
                    "<your source name2>": {
                        "rowsRead": <number of rows read>
                    },
                    ...
                }
            },
            "<your sink name2>": {
                ...
            },
            ...
        }
    }
}
```

Например, чтобы получить число строк, записанных в приемник с именем "sink1" в действии с именем "Датафловактивити", используйте `@activity('dataflowActivity').output.runStatus.metrics.sink1.rowsWritten` .

Чтобы получить число строк, считанных из источника с именем source1, который использовался в этом приемнике, используйте `@activity('dataflowActivity').output.runStatus.metrics.sink1.sources.source1.rowsRead` .

> [!NOTE]
> Если в приемнике записано ноль строк, он не будет отображаться в метриках. Существование можно проверить с помощью `contains` функции. Например, `contains(activity('dataflowActivity').output.runStatus.metrics, 'sink1')` проверит, были ли строки записаны в sink1.

## <a name="next-steps"></a>Дальнейшие действия

См. раздел действия потока управления, поддерживаемые фабрикой данных. 

- [Действие условия If](control-flow-if-condition-activity.md)
- [Действие выполнения конвейера](control-flow-execute-pipeline-activity.md)
- [Действие For Each](control-flow-for-each-activity.md)
- [Действие получения метаданных](control-flow-get-metadata-activity.md)
- [Действие поиска](control-flow-lookup-activity.md)
- [Веб-действие](control-flow-web-activity.md)
- [Действие Until](control-flow-until-activity.md)
