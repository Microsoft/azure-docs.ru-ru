---
title: Получение сведений о ценах на фабрику данных Azure с помощью примеров
description: В этой статье описывается и демонстрируется модель ценообразования Фабрики данных Azure с подробными примерами
author: dcstwh
ms.author: weetok
ms.reviewer: maghan
ms.service: data-factory
ms.topic: conceptual
ms.date: 09/14/2020
ms.openlocfilehash: b9f163a7632ca59d4f97aef21d8d62157610ba73
ms.sourcegitcommit: 867cb1b7a1f3a1f0b427282c648d411d0ca4f81f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "100372813"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>Изучение ценообразования Фабрики данных Azure посредством примеров

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

В этой статье описывается и демонстрируется модель ценообразования Фабрики данных Azure с подробными примерами.

> [!NOTE]
> Цены, используемые в приведенных ниже примерах, являются гипотетическими и не отражают фактическую цену.

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>Copy data from AWS S3 to Azure Blob storage hourly (Ежечасное копирование данных из AWS S3 в хранилище BLOB-объектов Azure)

В этом сценарии необходимо скопировать данные из AWS S3 в хранилище BLOB-объектов Azure по графику ежечасно.

Для выполнения сценария необходимо создать конвейер со следующими элементами.

1. Операция копирования с входным набором данных для копирования из AWS S3.

2. Выходной набор для данных на Azure Storage.

3. Триггер расписания для выполнения конвейера каждый час.

   ![На схеме показан конвейер с триггером расписания. В конвейере действие копирования помещается во входной набор данных, который передается в связанную службу W S3, а действие копирования — в выходной набор данных, который передается в связанную службу хранилища Azure.](media/pricing-concepts/scenario1.png)

| **Операции** | **Типы и единицы измерения** |
| --- | --- |
| Создание связанной службы | 2 Чтение и запись сущности  |
| Создание наборов данных | 4 Чтение и запись сущностей (2 для создания набора данных, 2 — для ссылок на связную службу) |
| Создание конвейера | 3 Чтение и запись сущностей (1 для создания конвейера, 2 — для ссылок на набор данных) |
| Получение конвейера | 1 Чтение и запись сущности |
| Выполнение конвейера | 2 Выполнение действий (1 для запуска триггера, 1 — для выполнения действий) |
| Допущение копирования данных: время выполнения = 10 мин. | 10 \* 4 Azure Integration Runtime (настройка DIU по умолчанию = 4). Дополнительные сведения о единицах интеграции данных и оптимизации выполнения копирования см. в [этой статье](copy-activity-performance.md). |
| Допущение отслеживания конвейера: произошло только 1 выполнение | 2 контрольные записи запуска, полученные (1 для выполнения конвейера, 1 для выполнения действия) |

**Общая стоимость сценариев: 0,16811 долл. США**

- Операции Фабрики данных = **0,0001 долл. США**
  - Чтение и запись = 10\*00001 = 0,0001 долл. США [1 R/W = 0,50/50 000 долл. США = 0.00001]
  - Отслеживание  = 2\*000005 = 0,00001 долл. США [1 Отслеживание = 0,25 долл. США/50 000 = 0,000005]
- Оркестрация и &amp; выполнение конвейера = **0,168 долл. США**
  - Выполнение действий = 001\*2 = 0,002 [1 выполнение = 1 долл. США/1000 = 0,001]
  - Действия по перемещению данных = 0,166 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,25 долл. США/час на Azure Integration Runtime)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>Ежечасное копирование и преобразование данных с помощью Azure Databricks

В этом сценарии необходимо скопировать данные из AWS S3 в хранилище BLOB-объектов Azure и преобразовать данные с помощью Azure Databricks по графику ежечасно.

Для выполнения сценария необходимо создать конвейер со следующими элементами.

1. Одно действие копирования с входным набором данных для копирования данных из AWS S3 и набор выходных данных для данных хранилища Azure.
2. Одно действие преобразования данных с помощью Azure Databricks.
3. Один триггер расписания для выполнения конвейера каждый час.

![На схеме показан конвейер с триггером расписания. В конвейере действие копирования помещается во входной набор данных, выходной набор данных и действие "данные модуля", выполняемые в Azure Databricks. Входной набор данных передается в связанную службу W-S3. Выходной набор данных направляется в связанную службу хранилища Azure.](media/pricing-concepts/scenario2.png)

| **Операции** | **Типы и единицы измерения** |
| --- | --- |
| Создание связанной службы | 3 Чтение и запись сущности  |
| Создание наборов данных | 4 Чтение и запись сущностей (2 для создания набора данных, 2 — для ссылок на связную службу) |
| Создание конвейера | 3 Чтение и запись сущностей (1 для создания конвейера, 2 — для ссылок на набор данных) |
| Получение конвейера | 1 Чтение и запись сущности |
| Выполнение конвейера | 3 Выполнение действий (1 для запуска триггера, 1 — для выполнения действий) |
| Допущение копирования данных: время выполнения = 10 мин. | 10 \* 4 Azure Integration Runtime (настройка DIU по умолчанию = 4). Дополнительные сведения о единицах интеграции данных и оптимизации выполнения копирования см. в [этой статье](copy-activity-performance.md). |
| Допущение отслеживания конвейера: произошло только 1 выполнение | 3 контрольные записи запуска, полученные (1 для выполнения конвейера, 2 для выполнения действия) |
| Допущение выполнения копирования данных: время выполнения = 10 мин | 10-минутное выполнение внешнего действия конвейера |

**Общая стоимость сценариев: 0,16916 долл. США**

- Операции Фабрики данных = **0,00012 долл. США**
  - Чтение и запись = 11\*00001 = 0,00011 долл. США [1 R/W = 0,50 долл. США/50000 = 0,00001]
  - Отслеживание = 3\*000005 = 0,00001 долл. США [1 Отслеживание = 0,25 долл. США/50000 = 0,000005]
- Оркестрация и &amp; выполнение конвейера = **0,16904 долл. США**
  - Выполнение действий = 001\*3 = 0,003 [1 выполнение = 1 долл. США/1000 = 0,001]
  - Действия по перемещению данных = 0,166 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,25 долл. США/час на Azure Integration Runtime)
  - Внешнее действие конвейера = 0,000041 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,00025 долл. США/час на Azure Integration Runtime)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>Ежечасное копирование и преобразование данных с помощью динамических параметров

В этом сценарии необходимо скопировать данные из AWS S3 в хранилище BLOB-объектов Azure и преобразовать данные с помощью Azure Databricks (динамичных параметров сценария) по графику ежечасно.

Для выполнения сценария необходимо создать конвейер со следующими элементами.

1. Одно действие копирования с набором входных данных для копирования из AWS S3 и набор выходных данных для данных хранилища Azure.
2. Один поиск действия для динамического преобразования параметров в сценарий.
3. Одно действие преобразования данных с помощью Azure Databricks.
4. Один триггер расписания для выполнения конвейера каждый час.

![На схеме показан конвейер с триггером расписания. В конвейере действие копирования помещается во входной набор данных, выходной набор данных и действие уточняющего запроса, которые передаются в действие "кирпичи данных", которое выполняется в Azure Databricks. Входной набор данных передается в связанную службу W-S3. Выходной набор данных направляется в связанную службу хранилища Azure.](media/pricing-concepts/scenario3.png)

| **Операции** | **Типы и единицы измерения** |
| --- | --- |
| Создание связанной службы | 3 Чтение и запись сущности  |
| Создание наборов данных | 4 Чтение и запись сущностей (2 для создания набора данных, 2 — для ссылок на связную службу) |
| Создание конвейера | 3 Чтение и запись сущностей (1 для создания конвейера, 2 — для ссылок на набор данных) |
| Получение конвейера | 1 Чтение и запись сущности |
| Выполнение конвейера | 4 Выполнения действий (1 для запуска триггера, 3 — для выполнения действий) |
| Допущение копирования данных: время выполнения = 10 мин. | 10 \* 4 Azure Integration Runtime (настройка DIU по умолчанию = 4). Дополнительные сведения о единицах интеграции данных и оптимизации выполнения копирования см. в [этой статье](copy-activity-performance.md). |
| Допущение отслеживания конвейера: произошло только 1 выполнение | 4 контрольные записи запуска, полученные (1 для выполнения конвейера, 3 для выполнения действия) |
| Допущение выполнения поиска: время выполнения = 1 мин | 1-минутное действие выполнения конвейера |
| Допущение выполнения копирования данных: время выполнения = 10 мин | 10-минутное выполнение внешнего действия конвейера |

**Общая стоимость сценариев: 0,17020 долл. США**

- Операции Фабрики данных = **0,00013 долл. США**
  - Чтение и запись = 11\*00001 = 0,00011 долл. США [1 R/W = 0,50 долл. США/50000 = 0,00001]
  - Отслеживание  = 4\*000005 = 0,00002 долл. США [1 Отслеживание = 0,25 долл. США/50000 = 0,000005]
- Оркестрация и &amp;выполнение конвейера = **0,17007 долл. США**
  - Выполнение действий = 001\*4 = 0,004 [1 выполнение = 1 долл. США/1000 = 0,001]
  - Действия по перемещению данных = 0,166 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,25 долл. США/час на Azure Integration Runtime)
  - Действие конвейера = 0,00003 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,002 долл. США/час на Azure Integration Runtime)
  - Внешнее действие конвейера = 0,000041 долл. США (распределено пропорционально в течение 10 минут времени выполнения. 0,00025 долл. США/час на Azure Integration Runtime)

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>Использование отладки потока данных сопоставления для обычного рабочего дня

Как специалист по обработке данных, Сэм отвечает за проектирование, построение и тестирование потоков данных сопоставления каждый день. Сэм регистрирует в пользовательском интерфейсе ADF и включает режим отладки для потоков данных. Срок жизни по умолчанию для сеансов отладки составляет 60 минут. Сэм работает в течение 8 часов, поэтому сеанс отладки никогда не истечет. Поэтому плата за день будет взиматься по следующим параметрам:

**8 (ч) x 8 (оптимизированные для вычислений ядра) x $0,193 = $12,35**

В то же время, Крис, другой разработчик данных, также входит в пользовательский интерфейс браузера ADF для профилирования данных и проектирования ETL. Крис не работает в ADF, например SAM. Крис должен использовать только отладчик потока данных в течение 1 часа в течение одного и того же дня, что и Сэм выше. К этим расходам относятся расходы на использование отладки:

**1 (час) x 8 (ядер общего назначения) x $0,274 = $2,19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>Преобразование данных в хранилище BLOB-объектов с помощью потоков данных сопоставления

В этом сценарии вы хотите, чтобы данные в хранилище BLOB-объектов были визуально преобразованы в потоки данных сопоставления, почасовое расписание.

Для выполнения сценария необходимо создать конвейер со следующими элементами.

1. Действие потока данных с логикой преобразования.

2. Входной набор данных для данных в службе хранилища Azure.

3. Выходной набор для данных на Azure Storage.

4. Триггер расписания для выполнения конвейера каждый час.

| **Операции** | **Типы и единицы измерения** |
| --- | --- |
| Создание связанной службы | 2 Чтение и запись сущности  |
| Создание наборов данных | 4 Чтение и запись сущностей (2 для создания набора данных, 2 — для ссылок на связную службу) |
| Создание конвейера | 3 Чтение и запись сущностей (1 для создания конвейера, 2 — для ссылок на набор данных) |
| Получение конвейера | 1 Чтение и запись сущности |
| Выполнение конвейера | 2 Выполнение действий (1 для запуска триггера, 1 — для выполнения действий) |
| Предположения потока данных: время выполнения = 10 мин + 10 мин TTL | 10 \* 16 ядер общего вычислений с TTL 10 |
| Допущение отслеживания конвейера: произошло только 1 выполнение | 2 контрольные записи запуска, полученные (1 для выполнения конвейера, 1 для выполнения действия) |

**Общая стоимость сценария: $1,4631**

- Операции Фабрики данных = **0,0001 долл. США**
  - Чтение и запись = 10\*00001 = 0,0001 долл. США [1 R/W = 0,50/50 000 долл. США = 0.00001]
  - Отслеживание  = 2\*000005 = 0,00001 долл. США [1 Отслеживание = 0,25 долл. США/50 000 = 0,000005]
- Выполнение оркестрации конвейера &amp; = **$1,463**
  - Выполнение действий = 001\*2 = 0,002 [1 выполнение = 1 долл. США/1000 = 0,001]
  - Действия потока данных = $1,461 пропорционально в течение 20 минут (время выполнения 10 мин + 10 минут TTL). $0.274/час на Azure Integration Runtime с 16 ядрами общее вычисление

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Интеграция данных в управляемой виртуальной сети фабрики данных Azure
В этом сценарии необходимо удалить исходные файлы в хранилище BLOB-объектов Azure и скопировать данные из базы данных SQL Azure в хранилище BLOB-объектов Azure. Это выполнение будет выполняться дважды в разных конвейерах. Время выполнения этих двух конвейеров пересекается.
![Scenario4 ](media/pricing-concepts/scenario-4.png) для выполнения сценария, необходимо создать два конвейера со следующими элементами:
  - Действие конвейера — удаление.
  - Действие копирования с входным набором данных, копируемым из хранилища BLOB-объектов Azure.
  - Набор выходных данных для данных в базе данных SQL Azure.
  - Триггер расписания для выполнения конвейера.


| **Операции** | **Типы и единицы измерения** |
| --- | --- |
| Создание связанной службы | 4 сущность для чтения и записи |
| Создание наборов данных | 8 сущностей для чтения и записи (4 для создания набора данных, 4 для ссылок на связанные службы) |
| Создание конвейера | 6 сущностей для чтения и записи (2 для создания конвейера, 4 для ссылок на наборы данных) |
| Получение конвейера | 2 Чтение и запись сущности |
| Выполнение конвейера | 6 запусков действий (2 для запуска триггера, 4 для выполнения действий). |
| Действие "выполнить удаление": каждое время выполнения = 5 минут. Выполнение действия удаления в первом конвейере составляет от 10:00 часов UTC до 10:05 по ГРИНВИЧу. Время выполнения действия удаления во втором конвейере — от 10:02 часов UTC до 10:07 по ГРИНВИЧу.|Всего 7 минут выполнения действия конвейера в управляемой виртуальной сети. Действие конвейера поддерживает до 50 параллельной обработки в управляемой виртуальной сети. |
| Допущение Копирование данных: каждое время выполнения = 10 мин. Выполнение копирования в первом конвейере — от 10:06 по ГРИНВИЧу до 10:15 по Гринвичу (UTC). Время выполнения действия удаления во втором конвейере — от 10:08 часов UTC до 10:17 по ГРИНВИЧу. | 10 * 4 Azure Integration Runtime (параметр Диу по умолчанию = 4) Дополнительные сведения о единицах интеграции данных и оптимизации производительности копирования см. в [этой статье](copy-activity-performance.md) . |
| Предположение конвейера мониторинга: возникло только 2 выполнения | 6 контрольных записей запуска мониторинга (2 для выполнения конвейера, 4 для выполнения действия) |


**Общая стоимость сценария: $0,45523**

- Операции фабрики данных = $0,00023
  - Чтение/запись = 20 * 00001 = $0,0002 [1 R/W = $0,50/50000 = 0,00001]
  - Мониторинг = 6 * 000005 = $0,00003 [1 Мониторинг = $0,25/50000 = 0,000005]
- Выполнение & оркестрации конвейера = $0,455
  - Запуски действия = 0,001 * 6 = 0,006 [1 Run = $1/1000 = 0,001]
  - Действия перемещения данных = $0,333 (пропорционально 10 минут времени выполнения. 0,25 долл. США/час на Azure Integration Runtime)
  - Действие конвейера = $0,116 (распределяется пропорционально за 7 минут времени выполнения. 1 в час на Azure Integration Runtime)

> [!NOTE]
> Эти цены указаны только в качестве примера.

**FAQ**

Вопрос. Если я хочу выполнить более 50 действий конвейера, могут ли эти действия выполняться одновременно?

Ответ. будет разрешено не более 50 параллельных действий конвейера.  Действие конвейера 51th будет поставлено в очередь, пока не будет открыта Свободная ячейка. То же самое для внешних действий. Допускается не более 800 одновременных внешних действий.

## <a name="next-steps"></a>Дальнейшие действия

Теперь, когда вы ознакомились с ценами на Фабрику данных Azure, можно начинать работу.

- [Создание фабрики данных с помощью пользовательского интерфейса фабрики данных Azure](quickstart-create-data-factory-portal.md)

- [Общие сведения о службе фабрики данных Azure, службе интеграции данных в облаке](introduction.md)

- [Визуальная разработка в фабрике данных Azure](author-visually.md)
