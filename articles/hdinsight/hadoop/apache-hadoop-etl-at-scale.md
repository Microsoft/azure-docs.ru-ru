---
title: Извлечение, преобразование и загрузка в большом масштабе (Azure HDInsight)
description: Сведения об операциях извлечения, преобразования и загрузки в HDInsight с использованием Apache Hadoop.
author: hrasheed-msft
ms.author: hrasheed
ms.reviewer: jasonh
ms.service: hdinsight
ms.topic: how-to
ms.custom: hdinsightactive,seoapr2020
ms.date: 04/28/2020
ms.openlocfilehash: e7a8a72d0669f39cc27c997d83af1e6272d045a6
ms.sourcegitcommit: 75041f1bce98b1d20cd93945a7b3bd875e6999d0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/22/2021
ms.locfileid: "98704158"
---
# <a name="extract-transform-and-load-etl-at-scale"></a>Извлечение, преобразование и загрузка (ETL) в масштабе

Процесс извлечения, преобразования и загрузки используется для получения данных из разных источников. Он позволяет собрать данные в стандартном расположении, очистить их и обработать. В результате готовые данные помещаются в хранилище данных и становятся пригодны для выполнения запросов по ним. Прежние ETL-процессы дают возможность импортировать данные, очищать их на месте, а затем сохранять в реляционный обработчик данных. Широкий набор компонентов среды Apache Hadoop для Azure HDInsight поддерживает выполнение извлечения, преобразования и загрузки в большом масштабе.

Использование HDInsight в процессах ETL можно кратко описать таким конвейером:

![Обзор процесса ETL в HDInsight](./media/apache-hadoop-etl-at-scale/hdinsight-etl-at-scale-overview.png)

В разделах ниже рассматриваются все этапы ETL и их связанные компоненты.

## <a name="orchestration"></a>Оркестрация

Оркестрация охватывает все этапы конвейера ETL. Задания ETL в HDInsight часто включают в себя несколько совместно функционирующих различных продуктов. Пример:

- Так, можно применить Apache Hive, чтобы очистить одну часть данных, и Apache Pig для очистки другой части.
- Фабрику данных Azure можно использовать для загрузки данных в базу данных SQL Azure из Azure Data Lake Store.

Оркестрация необходима для запуска соответствующего задания в соответствующее время.

### <a name="apache-oozie"></a>Apache Oozie

Apache Oozie — это система координации рабочих процессов, которая управляет заданиями Hadoop. Oozie работает в кластере HDInsight и интегрирована со стеком Hadoop. Oozie поддерживает задания Apache Hadoop MapReduce, Pig, Hive и Sqoop. Вы можете использовать Oozie для планирования заданий для конкретной системы, например программ Java или сценариев оболочки.

Дополнительные сведения см. в статье [Использование Apache Oozie с Apache Hadoop для определения и запуска рабочих процессов в HDInsight под управлением Linux](../hdinsight-use-oozie-linux-mac.md). Также изучите статью [Ввод в эксплуатацию конвейера аналитики данных](../hdinsight-operationalize-data-pipeline.md).

### <a name="azure-data-factory"></a>Фабрика данных Azure

Фабрика данных Azure предоставляет возможности оркестрации в формате PaaS (платформа как услуга). Фабрика данных Azure — это облачная служба интеграции данных. Она позволяет создавать управляемые данными рабочие процессы для оркестрации и автоматизации перемещения и преобразования данных.

Применение Фабрики данных Azure:

1. Создание и планирование рабочих процессов на основе данных. Эти конвейеры могут принимать данные из разрозненных хранилищ данных.
1. Обработка и преобразование данных с помощью вычислительных служб, как например HDInsight и Hadoop. Для этого шага можно также использовать Spark, Azure Data Lake Analytics, пакетную службу Azure или Машинное обучение Azure.
1. Опубликуйте выходные данные в хранилищах данных, например Azure синапсе Analytics, для использования приложениями бизнес-аналитики.

Дополнительные сведения о фабрике данных Azure см. в [этой статье](../../data-factory/introduction.md).

## <a name="ingest-file-storage-and-result-storage"></a>Хранилище файлов приема и хранилище результатов

Исходные файлы данных обычно загружаются в расположение в службе хранилища Azure или Azure Data Lake Storage. Обычно это файлы в неструктурированном формате, например CSV. Но они могут быть в любом формате.

### <a name="azure-storage"></a>Хранилище Azure

Служба хранилища Azure имеет определенные целевые показатели адаптируемости. Дополнительные сведения см. в статье [Целевые показатели масштабируемости и производительности для хранилища BLOB-объектов](../../storage/blobs/scalability-targets.md). Для большинства аналитических узлов эта служба лучше всего масштабируется при использовании множества небольших файлов. Служба хранилища Azure гарантирует одинаковую производительность независимо от количества файлов и их размера, пока соблюдаются ограничения для учетной записи. Вы можете хранить терабайты данных и получать все ту же стабильную производительность. Это справедливо как при работе с некоторым подмножеством, так и для всего объема данных.

Служба хранилища Azure использует несколько типов больших двоичных объектов. *Добавочный большой двоичный объект* — оптимальный вариант для хранения веб-журналов или данных датчиков.

Несколько больших двоичных объектов можно распределить по множеству серверов, чтобы обеспечить горизонтальное масштабирование доступа к ним. Но каждый большой двоичный объект обслуживается только одним сервером. Большие двоичные объекты могут быть логически сгруппированы в контейнеры, но такая группировка никак не влияет на их распределение по разделам.

Служба хранилища Azure также включает слой API WebHDFS для хранения больших двоичных объектов. Все службы HDInsight могут обращаться к файлам в хранилище BLOB-объектов Azure для очистки и обработки данных. Это похоже на то, как эти службы используют распределенную файловую систему Hadoop (HDFS).

Обычно данные помещаются в службу хранилища Azure с помощью PowerShell, SDK службы хранилища Azure или AZCopy.

### <a name="azure-data-lake-storage"></a>Azure Data Lake Storage

Azure Data Lake Storage — это управляемый и гипермасштабируемый репозиторий для данных аналитики. Он совместим с HDFS и использует сходную концепцию архитектуры. Data Lake Storage предоставляет неограниченную адаптируемость емкости и поддержку любого размера отдельных файлов. Это хороший выбор для работы с большими файлами, так как это хранилище позволяет сохранить такие файлы на нескольких узлах. Секционирование данных в Data Lake Storage выполняется в фоновом режиме. Это хранилище обеспечивает колоссальную пропускную способность: более тысячи исполнителей могут одновременно запускать аналитические задания, которые эффективно считывают и записывают тысячи терабайтов данных.

Данные обычно передаются в Data Lake Storage через Фабрику данных Azure. Вы также можете использовать пакеты SDK для Data Lake Storage, службу AdlCopy, Apache DistCp или Apache Sqoop. Выбор службы зависит от того, где находятся данные. Если это существующий кластер Hadoop, можно использовать Apache DistCp, службу AdlCopy или Фабрику данных Azure. Для данных в хранилище больших двоичных объектов Azure можно применить пакет средств разработки .NET для Azure Data Lake Storage, Azure PowerShell или Фабрику данных Azure.

Хранилище Data Lake Storage оптимизировано для приема событий через Центры событий Azure или Apache Storm.

### <a name="considerations-for-both-storage-options"></a>Рекомендации для двух вариантов хранения

Для отправки наборов данных, размер которых измеряется терабайтами, задержки сети могут стать серьезной проблемой. Это особенно важно, если данные поступают из локальной среды. В таких ситуациях будут уместны следующие варианты.

- **Azure ExpressRoute.** Позволяет создавать частные подключения между центрами обработки данных Azure и локальной инфраструктурой. Такое подключение обеспечивает надежный вариант передачи больших объемов данных. Дополнительные сведения см. в [техническом обзоре ExpressRoute](../../expressroute/expressroute-introduction.md).

- **Оправка файлов с жестких дисков.** Вы можете использовать [службу импорта и экспорта Azure](../../import-export/storage-import-export-service.md) для доставки жестких дисков с данными в центр обработки данных Azure. Данные сначала будут отправлены в хранилище BLOB-объектов Azure. Затем с помощью Фабрики данных Azure или инструмента AdlCopy их можно скопировать из хранилища BLOB-объектов Azure в Data Lake Storage.

### <a name="azure-synapse-analytics"></a>Azure Synapse Analytics

Azure синапсе Analytics — это подходящий вариант для хранения подготовленных результатов. Azure HDInsight можно использовать для выполнения этих служб в Azure синапсе Analytics.

Azure синапсе Analytics — это хранилище реляционных баз данных, оптимизированное для аналитических рабочих нагрузок. Масштабирование этого хранилища выполняется на основе секционированных таблиц. Таблицы можно секционировать на нескольких узлах. Узлы выбираются во время создания. Затем их можно масштабировать, но для такого активного процесса может потребоваться перемещение данных. Дополнительные сведения см. [в статье управление расчетами в Azure синапсе Analytics](../../synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md).

### <a name="apache-hbase"></a>Apache HBase

Apache HBase представляет собой хранилище данных типа "ключ — значение", доступное в Azure HDInsight. Это база данных NoSQL с открытым кодом, созданная на основе Hadoop по типу Google BigTable. HBase обеспечивает производительный быстрый доступ и строгую согласованность для больших объемов неструктурированных и полуструктурированных данных.

Поскольку HBase является бессхемной базой данных, вам не придется определять столбцы и типы данных для ее использования. Данные хранятся в строках таблицы и группируются по семействам столбцов.

Открытый код линейно масштабируется, чтобы обрабатывать петабайты данных на тысячах узлов. HBase полагается на избыточность данных, пакетную обработку и другие возможности, предоставляемые распределенными приложениями в среде Hadoop.

Это хорошее место назначения для данных датчиков и журналов, которые потребуются для анализа.

Адаптируемость HBase определяется количеством узлов в кластере HDInsight.

### <a name="azure-sql-databases"></a>Базы данных SQL Azure

В Azure предлагаются три реляционных базы данных в формате PaaS:

* [База данных SQL Azure](../../azure-sql/database/sql-database-paas-overview.md) представляет собой реализацию Microsoft SQL Server. Дополнительные сведения о производительности см. в статье [Настройка производительности в Базе данных SQL Azure](../../azure-sql/database/performance-guidance.md).
* [База данных Azure для MySQL](../../mysql/overview.md) представляет собой реализацию Oracle MySQL.
* [База данных Azure для PostgreSQL](../../postgresql/quickstart-create-server-database-portal.md) представляет собой реализацию PostgreSQL.

Добавьте больше ресурсов ЦП и памяти, чтобы увеличить масштаб этих продуктов.  Для них также можно использовать диски категории "Премиум", чтобы повысить производительность ввода-вывода.

## <a name="azure-analysis-services"></a>Службы Azure Analysis Services

Azure Analysis Services — это подсистема аналитических данных, которая используется для поддержки принятия решений и бизнес-аналитики. Она предоставляет аналитические данные для бизнес-отчетов и клиентских приложений, таких как Power BI. Аналитические данные также используются с Excel, отчетами SQL Server Reporting Services и другими средствами визуализации данных.

Масштабируйте кубы анализа, изменяя уровни для каждого отдельного куба. Дополнительные сведения см. на странице [цен на службы Azure Analysis Services](https://azure.microsoft.com/pricing/details/analysis-services/).

## <a name="extract-and-load"></a>Извлечение и загрузка

Поместив данные в Azure, вы сможете использовать несколько служб для их извлечения и загрузки в другие продукты. HDInsight поддерживает средства Sqoop и Flume.

### <a name="apache-sqoop"></a>Apache Sqoop

Apache Sqoop — это средство для эффективной передачи данных между структурированными, полуструктурированными и неструктурированными источниками данных.

Sqoop использует MapReduce для импорта и экспорта данных, чтобы обеспечить параллельное выполнение операций и отказоустойчивость.

### <a name="apache-flume"></a>Apache Flume

Apache Flume — это распределенная, надежная и доступная служба для эффективного сбора, статистической обработки и перемещения больших объемов данных журнала. Ее гибкая архитектура основана на передаче потоков данных. Это надежная отказоустойчивая служба с настраиваемыми механизмами для обеспечения надежности. Она имеет много возможностей для отработки отказа и восстановления. Flume использует простую модель данных с возможностью расширения, позволяющую применять интерактивное приложение аналитики.

Apache Flume нельзя использовать с Azure HDInsight. Но в локальной версии Hadoop может применять Flume для отправки данных в хранилище BLOB-объектов Azure или Azure Data Lake Storage. Дополнительные сведения см. в записи блога об [использовании Apache Flume с HDInsight](https://web.archive.org/web/20190217104751/https://blogs.msdn.microsoft.com/bigdatasupport/2014/03/18/using-apache-flume-with-hdinsight/).

## <a name="transform"></a>Преобразование

Поместив данные в выбранное расположение, вы должны их очистить, объединить или подготовить для определенного шаблона использования. Hive, Pig и Spark SQL — оптимальные варианты для такой работы. В HDInsight они все поддерживаются.

## <a name="next-steps"></a>Дальнейшие действия

- [Использование Apache Hive как средства для извлечения, преобразования и загрузки](apache-hadoop-using-apache-hive-as-an-etl-tool.md)
- [Использование Azure Data Lake Storage Gen2 с кластерами Azure HDInsight](../hdinsight-hadoop-use-data-lake-storage-gen2.md)
- [Перемещение данных из Базы данных SQL Azure в таблицу Apache Hive](./apache-hadoop-use-sqoop-mac-linux.md)
