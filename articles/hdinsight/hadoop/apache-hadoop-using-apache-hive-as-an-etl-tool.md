---
title: Использование Apache Hive как средства для извлечения, преобразования и загрузки в Azure HDInsight
description: Apache Hive можно использовать в Azure HDInsight для извлечения, преобразования и загрузки данных.
ms.service: hdinsight
ms.topic: how-to
ms.custom: hdinsightactive,seoapr2020
ms.date: 04/28/2020
ms.openlocfilehash: bcf2f39423f033ccd5bfdb6bf51ebc89e254f802
ms.sourcegitcommit: 42e4f986ccd4090581a059969b74c461b70bcac0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/23/2021
ms.locfileid: "104867820"
---
# <a name="use-apache-hive-as-an-extract-transform-and-load-etl-tool"></a>Использование Apache Hive как средства для извлечения, преобразования и загрузки

Обычно перед загрузкой входных данных в целевое назначение для аналитики вам нужно очистить и преобразовать эти данные. Операции извлечения, преобразования и загрузки используются для подготовки данных и загрузки в целевое назначение.  Apache Hive в HDInsight умеет принимать неструктурированные данные, обрабатывать их по определенным правилам и передавать в реляционное хранилище данных, чтобы их могли использовать системы поддержки принятия решений. При таком подходе данные извлекаются из источника. Затем хранится в адаптируемом хранилище, таком как большие двоичные объекты службы хранилища Azure или Azure Data Lake Storage. Затем данные преобразуются с помощью последовательности запросов Hive. Затем размещается в Hive при подготовке к массовым загрузкам в целевое хранилище данных.

## <a name="use-case-and-model-overview"></a>Обзор модели и примера использования

На следующем рисунке представлена схема примера использования и модель для автоматизации процессов извлечения, преобразования и загрузки. Входные данные преобразуются в выходные данные определенного формата.  Во время этого преобразования данные изменяются, тип данных и даже язык.  Процессы извлечения, преобразования и загрузки могут переводить имперские единицы измерения в метрические, изменять часовые пояса и повышать точность данных, чтобы новые данные в точности соответствовали тем, которые уже существуют в целевом хранилище. Процессы ETL могут также объединять новые данные с существующими данными, чтобы получать отчеты в актуальном состоянии или предоставлять дополнительные сведения о существующих данных. Затем приложения, такие как средства создания отчетов и службы, могут использовать эти данные в нужном формате.

:::image type="content" source="./media/apache-hadoop-using-apache-hive-as-an-etl-tool/hdinsight-etl-architecture.png" alt-text="Apache Hive архитектура ETL" border="false":::

Hadoop обычно используется в процессах ETL, которые импортируют большое количество текстовых файлов (например, CSV). Или меньшее, но часто изменяющееся количество текстовых файлов или и то, и другое.  Hive — это прекрасный инструмент для подготовки данных перед отправкой в целевое назначение.  Hive позволяет создать схему для CSV и применять язык запросов, близкий к SQL, чтобы создавать программы MapReduce для взаимодействия с данными.

Ниже приведены стандартные действия по использованию Hive для извлечения, преобразования и загрузки.

1. Передайте данные в Azure Data Lake Storage или в хранилище BLOB-объектов Azure.
2. Создайте базу данных для хранения метаданных (на основе Базы данных SQL Azure), в которой Hive будет хранить схемы данных.
3. Создайте кластер HDInsight и подключите хранилище данных.
4. Определите схему, которая будет применяться в хранилище данных при считывании данных.

    ```hql
    DROP TABLE IF EXISTS hvac;

    --create the hvac table on comma-separated sensor data stored in Azure Storage blobs

    CREATE EXTERNAL TABLE hvac(`date` STRING, time STRING, targettemp BIGINT,
        actualtemp BIGINT,
        system BIGINT,
        systemage BIGINT,
        buildingid BIGINT)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    STORED AS TEXTFILE LOCATION 'wasbs://{container}@{storageaccount}.blob.core.windows.net/HdiSamples/SensorSampleData/hvac/';
    ```

5. Преобразуйте данные и передайте их в целевое расположение.  Есть несколько способов применить Hive для преобразования и загрузки.

    * Выполните с помощью Hive запросы и подготовку данных, затем сохраните данные в формате CSV в Azure Data Lake Storage или в хранилище BLOB-объектов Azure.  После этого вы сможете применить внешнее средство, например SQL Server Integration Services (SSIS), для получения данных в формате CSV и их передачи в реляционную базу данных, такую как SQL Server.
    * Выполняйте запросы непосредственно из Excel или C# с помощью драйвера Hive ODBC.
    * Используйте [Apache Sqoop](apache-hadoop-use-sqoop-mac-linux.md) для чтения подготовленных неструктурированных CSV-файлов и передачи данных в целевую реляционную базу данных.

## <a name="data-sources"></a>Источники данных

Источниками данных обычно являются внешние данные, которые можно сопоставить с существующими данными в хранилище данных, например:

* социальные сети, файлы журналов, данные от датчиков и приложений, которые создают файлы данных;
* наборы данных от поставщиков данных, например статистика погоды или продаж;
* данные потоковой передачи, собранные, отфильтрованные и обработанные с помощью соответствующих средств или платформ.

<!-- TODO: (see Collecting and loading data into HDInsight). -->

## <a name="output-targets"></a>Целевые назначения

С помощью Hive можно выводить данные в различные типы целевых объектов, в том числе:

* реляционные базы данных, например SQL Server или базу данных SQL Azure;
* Хранилище данных, например Azure синапсе Analytics.
* Excel;
* хранилища Azure для таблиц и больших двоичных объектов;
* приложения или службы, которым нужны данные в определенных форматах или в виде файлов с определенным типом структуры информации;
* Хранилище документов JSON, например Azure Cosmos DB.

## <a name="considerations"></a>Рекомендации

Модель извлечения, преобразования и загрузки обычно используется в следующих ситуациях.

`*` Загрузка потоковых данных или больших объемов частично структурированных или неструктурированных данных из внешних источников в существующую базу данных или информационную систему.
`*` Очистите, преобразуйте и проверьте данные перед загрузкой, возможно, с помощью нескольких преобразований, прошедших через кластер.
`*` Создание отчетов и визуализаций, которые регулярно обновляются. Например, если создание отчета занимает слишком много времени и не может выполняться в течение дня, вы можете генерировать его по расписанию в ночное время. Для автоматического выполнения запроса Hive можно использовать [Azure Logic Apps](../../logic-apps/logic-apps-overview.md) и PowerShell.

Если целевой объект для данных не является базой данных, можно создать файл в соответствующем формате в запросе, например в CSV-файле. Затем этот файл можно затем в Microsoft Excel или Power BI.

Если в процессе извлечения, преобразования и загрузки вам нужно выполнять несколько операций с данными, уделите внимание координации этих действий. Если операции управляются внешней программой, а не рабочим процессом, определите, могут ли некоторые операции выполняться параллельно. И для обнаружения завершения каждого задания. Зачастую проще применить механизм управления рабочим процессом, например Oozie в среде Hadoop, чем самостоятельно распределять последовательность операций внешними скриптами или программами.

## <a name="next-steps"></a>Дальнейшие действия

* [ETL в масштабе](apache-hadoop-etl-at-scale.md)
* [`Operationalize a data pipeline`](../hdinsight-operationalize-data-pipeline.md)
