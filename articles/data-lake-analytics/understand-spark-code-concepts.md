---
title: Ознакомьтесь с Apache Spark концепциями кода для разработчиков Azure Data Lake Analytics U-SQL.
description: В этой статье описываются Apache Spark концепции, которые помогут разработчикам U-SQL понять концепции кода Spark.
ms.reviewer: jasonh
ms.service: data-lake-analytics
ms.topic: how-to
ms.custom: Understand-apache-spark-code-concepts
ms.date: 10/15/2019
ms.openlocfilehash: 2abd5882e310b17c633a82009f44624fad156f14
ms.sourcegitcommit: 772eb9c6684dd4864e0ba507945a83e48b8c16f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "92221134"
---
# <a name="understand-apache-spark-code-for-u-sql-developers"></a>Знакомство с Apache Spark кодом для разработчиков U-SQL

В этом разделе содержатся рекомендации по преобразованию скриптов U-SQL в Apache Spark.

- Он начинается с [сравнения парадигм обработки двух языков](#understand-the-u-sql-and-spark-language-and-processing-paradigms)
- Содержит советы по выполнению следующих действий:
   - [Скрипты преобразования](#transform-u-sql-scripts) , включая [выражения набора строк](#transform-u-sql-rowset-expressions-and-sql-based-scalar-expressions) U-SQL
   - [Код .NET](#transform-net-code)
   - [Типы данных](#transform-typed-values)
   - [Объекты каталога](#transform-u-sql-catalog-objects).

## <a name="understand-the-u-sql-and-spark-language-and-processing-paradigms"></a>Общие сведения о языке U-SQL и Spark и парадигмах обработки

Прежде чем приступить к переносу сценариев U-SQL Azure Data Lake Analytics в Spark, полезно разобраться в общем языке и обосновании концепций обработки двух систем.

U-SQL — это похожий на SQL язык декларативных запросов, использующий парадигму потока данных и позволяющий легко внедрять и масштабировать пользовательский код, написанный на .NET (например, C#), Python и R. Пользовательские расширения могут реализовывать простые выражения или определяемые пользователем функции, но также могут предоставить пользователю возможность реализовывать такие, называемые пользовательскими операторами, которые реализуют пользовательские операторы для выполнения преобразований, извлечений и записи выходных данных на уровне наборов строк.

Spark — это платформа с горизонтальным масштабированием, предлагающая несколько языковых привязок в Scala, Java, Python, .NET и т. д. в первую очередь написание кода на одном из этих языков, создание абстракций данных, которые называются отказоустойчивыми распределенными наборами (RDD), кадрами и DataSets, а затем использование LINQ-ориентированного языка DSL для их преобразования. Он также предоставляет SparkSQL в качестве декларативного подязыка для абстракций кадров данных и DataSet. DSL предоставляет две категории операций, преобразований и действий. Применение преобразований к абстракциям данных не приводит к выполнению преобразования, а создает план выполнения, который будет отправлен для оценки с действием (например, запись результата во временную таблицу или файл или печать результата).

Таким образом, при преобразовании скрипта U-SQL в программу Spark необходимо решить, какой язык вы хотите использовать по крайней мере для создания абстракции кадра данных (в настоящее время это наиболее часто используемая Абстракция данных) и нужно ли писать декларативные преобразования потоковой передачи с помощью DSL или SparkSQL. В некоторых более сложных случаях может потребоваться разделить скрипт U-SQL на последовательность Spark и другие действия, реализованные с помощью пакетной службы Azure или функции Azure.

Более того, Azure Data Lake Analytics предлагает U-SQL в среде службы бессерверных заданий, в то время как Azure Databricks и Azure HDInsight предлагают Spark в форме службы кластеров. При преобразовании приложения необходимо принять во внимание последствия создания, изменения размера, масштабирования и списания кластеров.

## <a name="transform-u-sql-scripts"></a>Преобразование скриптов U-SQL

Скрипты U-SQL соответствуют следующему шаблону обработки:

1. Данные считываются из неструктурированных файлов с помощью `EXTRACT` инструкции, расположения или спецификации набора файлов, встроенного или определяемого пользователем средства извлечения и требуемой схемы или из таблиц U-SQL (управляемых или внешних таблиц). Он представляется в виде набора строк.
2. Наборы строк преобразуются в несколько инструкций U-SQL, которые применяют выражения U-SQL к наборам строк и создают новые наборы строк.
3. Наконец, результирующие наборы строк выводятся в любые файлы с помощью `OUTPUT` инструкции, задающего расположение и встроенное или определяемое пользователем средство вывода, или в таблицу U-SQL.

Сценарий оценивается отложенным, то есть каждый шаг извлечения и преобразования состоит из дерева выражений и глобально оценивается (поток данных).

Программы Spark аналогичны тем, что используют соединители Spark для чтения и создания кадров данных, а затем применяются преобразования для кадров данных с помощью DSL или SparkSQL, а затем записывают результаты в файлы, временные таблицы Spark, некоторые типы языков программирования или консоль.

## <a name="transform-net-code"></a>Преобразование кода .NET

Языком выражений U-SQL является C#, и он предлагает разнообразные способы масштабирования пользовательского кода .NET.

Поскольку в настоящее время Spark не поддерживает исполнение кода .NET, необходимо либо переписать выражения в эквивалентное выражение Spark, Scala, Java или Python, либо найти способ вызова кода .NET. Если в скрипте используются библиотеки .NET, доступны следующие варианты.

- Преобразовывать код .NET в Scala или Python.
- Разделите скрипт U-SQL на несколько шагов, в которых используются процессы пакетной службы Azure для применения преобразований .NET (если можно получить приемлемую шкалу).
- Используйте языковую привязку .NET, доступную в открытом источнике с именем Моебиус. Этот проект находится в неподдерживаемом состоянии.

В любом случае, если в скриптах U-SQL используется большой объем логики .NET, свяжитесь с нами через представителя учетной записи Майкрософт, чтобы получить дополнительные указания.

Следующие сведения относятся к различным случаям использования .NET и C# в скриптах U-SQL.

### <a name="transform-scalar-inline-u-sql-c-expressions"></a>Преобразование скалярных выражений в языке U-SQL для преобразования

Язык выражений U-SQL — C#. Многие из скалярных выражений U-SQL реализуются изначально для повышения производительности, в то время как более сложные выражения могут выполняться путем вызова в .NET Framework.

Spark имеет собственный язык скалярных выражений (как часть DSL или в SparkSQL) и позволяет вызывать определяемые пользователем функции, написанные на его языке размещения.

Если у вас есть скалярные выражения в U-SQL, сначала следует найти наиболее подходящее скалярное выражение Spark, чтобы получить максимальную производительность, а затем сопоставлять другие выражения в определяемую пользователем функцию языка размещения Spark по своему усмотрению.

Имейте в виду, что .NET и C# имеют разную семантику типов, чем языки размещения Spark и язык DSL для Spark. Дополнительные сведения о различиях в системе типов см. [ниже](#transform-typed-values) .

### <a name="transform-user-defined-scalar-net-functions-and-user-defined-aggregators"></a>Преобразование определяемых пользователем скалярных функций .NET и определяемых пользователем агрегатов

U-SQL предоставляет способы вызова произвольных скалярных функций .NET и вызова определяемых пользователем агрегатов, написанных на платформе .NET.

Spark также предлагает поддержку определяемых пользователем функций и пользовательских агрегатов, написанных на большинстве языков размещения, которые можно вызывать из DSL и SparkSQL.

### <a name="transform-user-defined-operators-udos"></a>Преобразование определяемых пользователем операторов (Udo)

U-SQL предоставляет несколько категорий определяемых пользователем операторов (Udo), таких как средства извлечения, средства вывода, модулей сжатия, процессоры, средств применения и средства объединения, которые могут быть написаны на .NET (и в некоторой степени в Python и R).

Spark не предоставляет такую же модель расширяемости для операторов, но имеет аналогичные возможности для некоторых.

Аналогом для средств извлечения и вывода Spark является соединители Spark. Для многих средств извлечения U-SQL вы можете найти эквивалентный соединитель в сообществе Spark. Для других пользователей вам потребуется написать пользовательский соединитель. Если средство извлечения U-SQL является сложным и использует несколько библиотек .NET, может быть предпочтительнее создать соединитель в Scala, который использует взаимодействие для вызова библиотеки .NET, которая выполняет фактическую обработку данных. В этом случае необходимо развернуть среду выполнения .NET Core в кластере Spark и убедиться, что ссылки на библиотеки .NET .NET Standard соответствуют требованиям 2,0.

Другие типы Udo U-SQL потребуется переписывать с помощью определяемых пользователем функций и агрегатов, а также семантически подходящих выражений для рассылки Spark или SparkSQL. Например, процессор может быть сопоставлен с ВЫБОРкой различных вызовов UDF, упакованных в виде функции, принимающей в качестве аргумента кадр данных и возвращающей кадр данных.

### <a name="transform-u-sqls-optional-libraries"></a>Преобразование необязательных библиотек U-SQL

U-SQL предоставляет набор необязательных и демонстрационных библиотек, которые предлагают поддержку [Python](data-lake-analytics-u-sql-python-extensions.md), [R](data-lake-analytics-u-sql-r-extensions.md), [JSON, XML, Avro](https://github.com/Azure/usql/tree/master/Examples/DataFormats)и некоторых возможностей работы со [средствами](data-lake-analytics-u-sql-cognitive.md).

Spark предлагает собственные возможности интеграции Python и R, pySpark и Spark, а также предоставляет соединители для чтения и записи JSON, XML и AVRO.

Если необходимо преобразовать скрипт, ссылающийся на библиотеки наличных служб, мы рекомендуем связаться с нами через представителя учетных записей Майкрософт.

## <a name="transform-typed-values"></a>Преобразование типизированных значений

Поскольку система типов U-SQL основана на системе типов .NET, а Spark имеет собственную систему типов, на которую влияет привязка языка узла, необходимо убедиться, что типы, на которых вы используете, близки, а для определенных типов диапазоны типов, точность и масштаб могут немного отличаться. Более того, U-SQL и Spark обрабатывают значения по- `null` разному.

### <a name="data-types"></a>Типы данных

В следующей таблице приведены эквивалентные типы в Spark, Scala и PySpark для заданных типов U-SQL.

| U-SQL | Spark |  Scala | PySpark |
| ------ | ------ | ------ | ------ |
|`byte`       ||||
|`sbyte`      |`ByteType` |`Byte` | `ByteType`|
|`int`        |`IntegerType` |`Int` | `IntegerType`|
|`uint`       ||||
|`long`       |`LongType` |`Long` | `LongType`|
|`ulong`      ||||
|`float`      |`FloatType` |`Float` | `FloatType`|
|`double`     |`DoubleType` |`Double` | `DoubleType`|
|`decimal`    |`DecimalType` |`java.math.BigDecimal` | `DecimalType`|
|`short`      |`ShortType` |`Short` | `ShortType`|
|`ushort`     ||||
|`char`   | |`Char`||
|`string` |`StringType` |`String` |`StringType` |
|`DateTime`   |`DateType`, `TimestampType` |`java.sql.Date`, `java.sql.Timestamp` | `DateType`, `TimestampType`|
|`bool`   |`BooleanType` |`Boolean` | `BooleanType`|
|`Guid`   ||||
|`byte[]` |`BinaryType` |`Array[Byte]` | `BinaryType`|
|`SQL.MAP<K,V>`   |`MapType(keyType, valueType, valueContainsNull)` |`scala.collection.Map` | `MapType(keyType, valueType, valueContainsNull=True)`|
|`SQL.ARRAY<T>`   |`ArrayType(elementType, containsNull)` |`scala.collection.Seq` | `ArrayType(elementType, containsNull=True)`|

Дополнительные сведения см. в разделе:

- [org. Apache. Spark. SQL. types](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.package)
- [Spark SQL и типы кадров данных](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)
- [Типы значений Scala](https://www.scala-lang.org/api/current/scala/AnyVal.html)
- [pyspark. SQL. types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)

### <a name="treatment-of-null"></a>Обработка значения NULL

В Spark типы по умолчанию допускают значения NULL в U-SQL. вы явно помечаете скалярный, а не объект как допускающий значение null. Хотя Spark позволяет определить столбец как не допускающий значения NULL, он не будет применять ограничение и [может привести к неправильному результату](https://medium.com/@weshoffman/apache-spark-parquet-and-troublesome-nulls-28712b06f836).

В Spark значение NULL указывает на неизвестное значение. Значение Spark NULL отличается от любого значения, включая само себя. Сравнение двух значений NULL в Spark или между значением NULL и любым другим значением возвращает Unknown, так как значение каждого значения NULL неизвестно.  

Это поведение отличается от U-SQL, что соответствует семантике C#, где `null` отличается от любого значения, но равно самому себе.  

Таким образом, `SELECT` инструкция SparkSQL, которая использует, `WHERE column_name = NULL` возвращает нулевые строки, даже если в `column_name` U-SQL есть значения NULL `column_name` `null` . Аналогичным образом `SELECT` инструкция Spark, использующая, `WHERE column_name != NULL` возвращает нулевые строки, даже если в `column_name` U-SQL значения, отличные от NULL, возвращают строки со значением, отличным от NULL. Таким же, если требуется семантика проверки значений NULL в U-SQL, следует использовать [IsNull](https://spark.apache.org/docs/2.3.0/api/sql/index.html#isnull) и [isnotnull](https://spark.apache.org/docs/2.3.0/api/sql/index.html#isnotnull) соответственно (или их эквивалент DSL).

## <a name="transform-u-sql-catalog-objects"></a>Преобразование объектов каталога U-SQL

Одно из основных различий заключается в том, что скрипты U-SQL могут использовать свои объекты каталога, многие из которых не имеют прямого эквивалента Spark.

Spark обеспечивает поддержку концепций мета хранилища Hive, главным образом базы данных и таблицы, позволяющие сопоставлять базы данных и схемы U-SQL с базами данных Hive, а также таблицы U-SQL в таблицы Spark (см. раздел [Перемещение данных, хранящихся в таблицах u-SQL](understand-spark-data-formats.md#move-data-stored-in-u-sql-tables)), но не поддерживают представления, функции с табличным значением (возвращающие табличное), хранимые процедуры, сборки U-SQL, внешние источники данных и т. д.

Объекты кода U-SQL, такие как представления, возвращающие табличное, хранимые процедуры и сборки, можно моделировать с помощью функций кода и библиотек в Spark и ссылаться на них с помощью функции языка узла и механизмов абстракции процедур (например, путем импорта модулей Python или ссылок на функции Scala).

Если каталог U-SQL используется для совместного использования данных и объектов кода в проектах и командах, необходимо использовать эквивалентные механизмы для совместного использования (например, Maven для совместного использования объектов кода).

## <a name="transform-u-sql-rowset-expressions-and-sql-based-scalar-expressions"></a>Преобразование выражений набора строк U-SQL и скалярных выражений на основе SQL

Базовый язык U-SQL преобразует наборы строк и основан на SQL. Ниже приведен неисчерпывающий список наиболее распространенных выражений набора строк, предлагаемых в U-SQL:

- `SELECT`/`FROM`/`WHERE`/`GROUP BY`+ Агрегаты +`HAVING`/`ORDER BY`+`FETCH`
- `INNER`/`OUTER`/`CROSS`/`SEMI``JOIN`выражения
- `CROSS`/`OUTER``APPLY`выражения
- `PIVOT`/`UNPIVOT` выражения
- `VALUES` Конструктор наборов строк

- Выражения наборов `UNION`/`OUTER UNION`/`INTERSECT`/`EXCEPT`

Кроме того, U-SQL предоставляет разнообразные скалярные выражения на основе SQL, такие как

- `OVER` оконные выражения
- разнообразные встроенные функции агрегации и ранжирования (и `SUM` `FIRST` т. д.).
- Некоторые из наиболее известных скалярных выражений SQL: `CASE` , `LIKE` , ( `NOT` ) `IN` и `AND` `OR` т. д.

Для большинства из этих выражений Spark предлагает эквивалентные выражения в обеих формах DSL и SparkSQL. Некоторые выражения, не поддерживаемые изначально в Spark, придется переписывать, используя сочетание собственных выражений Spark и семантически эквивалентных шаблонов. Например, необходимо `OUTER UNION` перевести в эквивалентное сочетание проекций и объединений.

Из-за различной обработки значений NULL соединение U-SQL всегда будет соответствовать строке, если оба сравниваемых столбца содержат значение NULL, тогда как соединение в Spark не будет соответствовать таким столбцам, если не будут добавлены явные проверки NULL.

## <a name="transform-other-u-sql-concepts"></a>Преобразование других концепций U-SQL

U-SQL также предлагает множество других функций и концепций, таких как Федеративные запросы к SQL Server базам данных, параметрам, скалярным значениям и переменным лямбда-выражений, системным переменным, `OPTION` указаниям.

### <a name="federated-queries-against-sql-server-databasesexternal-tables"></a>Федеративные запросы к SQL Server базам данных и внешним таблицам

U-SQL предоставляет источник данных и внешние таблицы, а также прямые запросы к базе данных SQL Azure. Хотя Spark не предоставляет одинаковые абстракции объектов, он предоставляет [соединитель Spark для базы данных SQL Azure](../azure-sql/database/spark-connector.md) , который можно использовать для выполнения запросов к базам данных SQL.

### <a name="u-sql-parameters-and-variables"></a>Параметры и переменные U-SQL

Параметры и пользовательские переменные имеют эквивалентные понятия в Spark и их языках размещения.

Например, в Scala можно определить переменную с помощью `var` ключевого слова:

```
var x = 2 * 3;
println(x)
```

Системные переменные U-SQL (переменные, начинающиеся с `@@` ) можно разделить на две категории:

- Настраиваемые системные переменные, для которых можно задать конкретные значения, влияющие на поведение сценариев
- Информационные системные переменные, которые запрашивать сведения об уровне системы и заданий

Большинство настраиваемых системных переменных не имеют прямого эквивалента в Spark. Некоторые информационные системные переменные могут быть смоделированы путем передачи информации в качестве аргументов во время выполнения задания, другие могут иметь эквивалентную функцию на языке размещения Spark.

### <a name="u-sql-hints"></a>Указания U-SQL

U-SQL предлагает несколько синтаксических способов предоставления подсказок оптимизатору запросов и подсистеме выполнения.  

- Установка системной переменной U-SQL
- `OPTION`предложение, связанное с выражением набора строк для предоставления указания данных или плана
- указание о соединении в синтаксисе выражения JOIN (например, `BROADCASTLEFT` )

Оптимизатор запросов, основанный на стоимости Spark, обладает собственными возможностями для предоставления подсказок и настройки производительности запросов. См. соответствующую документацию.

## <a name="next-steps"></a>Дальнейшие действия

- [Общие сведения о форматах данных Spark для разработчиков U-SQL](understand-spark-data-formats.md)
- [.NET для Apache Spark](/dotnet/spark/what-is-apache-spark-dotnet)
- [Обновление решений для аналитики больших данных с Azure Data Lake Storage 1-го поколения до Azure Data Lake Storage 2-го поколения](../storage/blobs/data-lake-storage-migrate-gen1-to-gen2.md)
- [Преобразование данных с помощью действия Spark в фабрике данных Azure](../data-factory/transform-data-using-spark.md)
- [Преобразование данных с помощью действия Hadoop Hive в фабрике данных Azure](../data-factory/transform-data-using-hadoop-hive.md)
- [Apache Spark в Azure HDInsight](../hdinsight/spark/apache-spark-overview.md)