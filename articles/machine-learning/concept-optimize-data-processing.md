---
title: Оптимизация обработки данных
titleSuffix: Azure Machine Learning
description: Ознакомьтесь с рекомендациями по оптимизации скорости обработки данных и интеграции, которые Машинное обучение Azure поддерживаются для обработки данных в масштабе.
services: machine-learning
ms.service: machine-learning
ms.author: sgilley
author: sdgilley
ms.subservice: core
ms.reviewer: nibaccam
ms.topic: conceptual
ms.date: 06/26/2020
ms.custom: data4ml
ms.openlocfilehash: 5ab7bac635a0b670087800212727b0d2e2b96934
ms.sourcegitcommit: 66ce33826d77416dc2e4ba5447eeb387705a6ae5
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/15/2021
ms.locfileid: "103472210"
---
# <a name="optimize-data-processing-with-azure-machine-learning"></a>Оптимизация обработки данных с помощью Машинное обучение Azure

В этой статье вы узнаете о рекомендациях, которые помогут оптимизировать скорость обработки данных в локальной и масштабируемой среде.

Машинное обучение Azure интегрированы с пакетами и платформами с открытым исходным кодом для обработки данных. Используя эти интеграции и рекомендации в этой статье, можно улучшить скорость обработки данных как локально, так и в масштабе.

## <a name="parquet-and-csv-file-formats"></a>Форматы файлов Parquet и CSV

Файлы с разделителями-запятыми (CSV) являются обычными форматами для обработки данных. Однако для задач машинного обучения рекомендуется использовать форматы файлов Parquet.

[Файлы Parquet](https://parquet.apache.org/) хранят данные в формате с двоичными столбцами. Этот формат полезен, если требуется разделить данные на несколько файлов. Кроме того, этот формат позволяет ориентироваться на соответствующие поля для экспериментов машинного обучения. Вместо того чтобы читать файл данных размером 20 ГБ, можно уменьшить эту нагрузку, выбрав необходимые столбцы для обучения модели машинного обучения. Файлы Parquet также можно сжать, чтобы снизить вычислительную мощность и занимать меньше пространства.

CSV-файлы обычно используются для импорта и экспорта данных, так как их легко редактировать и читать в Excel. Данные в CSV хранятся в виде строк в формате, основанном на строках, а файлы можно сжимать, чтобы уменьшить нагрузку при обмене данными. Несжатые CSV могут увеличиться на 2-10, а сжатие CSV — еще дальше. Таким образом, размер CSV-файла размером 5 ГБ в памяти расширяется до 8 ГБ ОЗУ на компьютере. Такое сжатие может увеличить задержку передачи данных, что не идеально подходит для обработки больших объемов данных. 

## <a name="pandas-dataframe"></a>Кадр данных Pandas

[Кадры данных Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html) обычно используются для обработки и анализа. `Pandas` хорошо работает с размерами данных менее 1 ГБ, но время обработки для `pandas` кадров данные замедляется, когда размер файлов достигает 1 ГБ. Это замедление связано с тем, что размер данных в хранилище не совпадает с размером данных в кадре. Например, данные в CSV-файлах могут расширяться до 10 раз в кадре данных, поэтому CSV-файл размером 1 ГБ может стать 10 ГБ в кадре данных.

`Pandas` является однопотоковым, то есть операции выполняются по одному за раз на одном ЦП. Рабочие нагрузки можно легко распараллелить нескольким виртуальным ЦП в одном Машинное обучение Azure вычислительного экземпляра с такими пакетами, как [Модин](https://modin.readthedocs.io/en/latest/) , которые переносятся `Pandas` с помощью распределенной серверной части.

Для параллелизации задач с помощью `Modin` и [Даск](https://dask.org)просто измените эту строку кода `import pandas as pd` на `import modin.pandas as pd` .

## <a name="dataframe-out-of-memory-error"></a>Кадр данных: ошибка нехватки памяти 

Обычно ошибка *нехватки памяти* возникает, когда кадр данных расширяется выше доступной оперативной памяти на компьютере. Эта концепция также применима к распределенной платформе, такой как `Modin` или `Dask` .  Это значит, что Ваша операция пытается загрузить кадр данных в памяти на каждом узле в кластере, но для этого недостаточно ОЗУ.

Одним из решений является увеличение объема ОЗУ в соответствии с размерами кадров данных в памяти. Рекомендуемый размер вычислительных ресурсов и вычислительная мощность в два раза больше объема ОЗУ. Поэтому, если размер таблицы данных составляет 10 ГБ, используйте целевой объект вычислений с объемом ОЗУ не менее 20 ГБ, чтобы кадр данных можно было разместить в памяти и обработать. 

Для нескольких виртуальных процессоров виртуальных ЦП следует помнить о том, что один раздел можно легко разместить в ОЗУ, каждый виртуальных ЦП на компьютере. То есть при наличии 16 ГБ ОЗУ 4 виртуальных ЦП требуется около 2 ГБ кадров данных на каждый виртуальных ЦП.

### <a name="local-vs-remote"></a>Локальное и удаленное

Вы можете заметить, что некоторые команды кадров данных Pandas работают быстрее при работе на локальном компьютере, а также на удаленной виртуальной машине, подготовленной с помощью Машинное обучение Azure. На локальном компьютере, как правило, включен файл подкачки, который позволяет загружать больше, чем помещается в физической памяти, то есть жесткий диск используется как расширение ОЗУ. В настоящее время Машинное обучение Azure виртуальные машины выполняются без файла подкачки, поэтому может загружать только столько объемов данных, сколько доступно для физической памяти. 

Для заданий с большим количеством вычислений рекомендуется выбрать более крупную виртуальную машину, чтобы улучшить скорость обработки.

Дополнительные сведения о [доступных рядах и размерах виртуальных машин](concept-compute-target.md#supported-vm-series-and-sizes) для машинное обучение Azure. 

Спецификации ОЗУ см. на соответствующих страницах серии виртуальных машин, таких как, серии [Dv2-Dsv2](../virtual-machines/dv2-dsv2-series-memory.md) или [NC](../virtual-machines/nc-series.md).

### <a name="minimize-cpu-workloads"></a>Сокращение количества рабочих нагрузок ЦП

Если не удается добавить ОЗУ на компьютер, можно применить следующие методы для сокращения нагрузки ЦП и оптимизации времени обработки. Эти рекомендации относятся как к отдельным, так и к распределенным системам.

Метод | Описание
----|----
сжатие; | Используйте другое представление для данных, так как использует меньше памяти и не влияет на результаты вычисления.<br><br>*Пример:* Вместо того чтобы хранить записи в виде строки с приблизительно 10 байтами на запись, сохраните их как логическое значение, true или false, которое можно хранить в 1 байт.
Фрагментирующего | Загрузка данных в память в поднаборах (фрагментах), обработка данных по одному подмножеству во времени или несколько подмножеств в параллельном режиме. Этот метод лучше всего подходит, если необходимо обработать все данные, но не нужно загружать все данные в память одновременно. <br><br>*Пример:* Вместо того чтобы обрабатывать данные за весь год одновременно, загрузите и обработайте данные за один месяц за раз.
Индексация | Примените и используйте индекс — сводку, в которой вы узнаете, где найти нужные данные. Индексирование полезно, когда необходимо использовать только подмножество данных, а не полный набор<br><br>*Пример:* Если у вас есть годовые данные по продажам, отсортированные по месяцам, индекс поможет быстро найти нужный месяц, который необходимо обработать.

## <a name="scale-data-processing"></a>Масштабирование обработки данных

Если предыдущие рекомендации недостаточно, и вы не можете получить виртуальную машину, подходящую для ваших данных, 

* Используйте платформу, например `Spark` или, `Dask` для обработки данных "недостаточно памяти". В этом случае кадр данных загружается в раздел ОЗУ по секциям и обрабатывается с окончательным результатом сбора в конце.  

* Развертывание в кластере с помощью распределенной платформы. В этом случае нагрузка обработки данных разделяется и обрабатывается на нескольких процессорах, работающих параллельно, с окончательным результатом, собранными в конце.

### <a name="recommended-distributed-frameworks"></a>Рекомендуемые распределенные платформы

В следующей таблице приводятся рекомендации по распределенным платформам, интегрированным с Машинное обучение Azure в зависимости от настроек кода или размера данных.

Опыт или размер данных | Рекомендация
------|------
Если вы знакомы с `Pandas`| `Modin` или `Dask` кадр данных
Если вы предпочитаете `Spark` | `PySpark`
Для данных менее 1 ГБ | `Pandas` Локальное **или** удаленное машинное обучение Azure вычислительного экземпляра
Для данных, превышающих 10 ГБ| Перемещение в кластер с помощью `Ray` , `Dask` или `Spark`

## <a name="next-steps"></a>Дальнейшие действия

* [Параметры приема данных с машинное обучение Azure](concept-data-ingestion.md).
* [Создание и регистрация наборов данных](how-to-create-register-datasets.md).
