---
title: Платформы и средства для проектов обработки и анализа данных — командный процесс обработки и анализа данных
description: Список и описание ресурсов для получения и анализа данных, которые предприятия могут использовать для стандартизации командного процесса обработки и анализа данных.
author: marktab
manager: marktab
editor: marktab
ms.service: machine-learning
ms.subservice: team-data-science-process
ms.topic: article
ms.date: 01/10/2020
ms.author: tdsp
ms.custom: seodec18, previous-author=deguhath, previous-ms.author=deguhath
ms.openlocfilehash: 4ba7b8af9b50b9173f5e2040bb8b623eeafdd538
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "96453861"
---
# <a name="platforms-and-tools-for-data-science-projects"></a>Платформы и средства для проектов обработки и анализа данных

Корпорация Майкрософт предоставляет широкий спектр ресурсов для анализа данных как для облачных, так и для локальных платформ. Развертывание этих ресурсов повысит эффективность и масштабируемость выполнения проектов по обработке и анализу данных. Для команд, выполняющих проекты по обработке и анализу данных, предлагается [командный процесс обработки и анализа данных](overview.md) (TDSP), который обеспечивает отслеживаемость, контроль версий и совместную работу.  Роли и задачи сотрудников в области стандартизации обработки и анализа данных описаны в статье [Team Data Science Process roles and tasks](roles-tasks.md) (Роли и задачи в командном процессе обработки и анализа данных).

Командам обработки и анализа данных, использующим TDSP, доступны следующие ресурсы для анализа данных:

- виртуальные машины для обработки и анализа данных (Windows и Linux CentOS);
- кластеры Spark в HDInsight;
- Azure Synapse Analytics
- Azure Data Lake;
- кластеры Hive в HDInsight;
- Хранилище файлов Azure
- SQL Server 2019 R и службы Python
- Azure Databricks

В этом документе мы кратко опишем эти ресурсы и приведем ссылки на руководства и пошаговые инструкции, опубликованные командами TDSP. Они помогут вам постепенно изучить все ресурсы и успешно применить их для создания интеллектуальных приложений. Дополнительные сведения об этих ресурсах можно найти на страницах соответствующих продуктов. 

## <a name="data-science-virtual-machine-dsvm"></a>Виртуальная машина для обработки и анализа данных

Виртуальная машина для обработки и анализа данных, которую корпорация Майкрософт предоставляет для ОС Windows и Linux, содержит набор популярных средств моделирования и разработки для систем обработки и анализа данных. Среди прочего, она оснащена такими средствами:

- Microsoft R Server Developer Edition 
- дистрибутив Anaconda Python;
- записные книжки Jupyter для Python и R; 
- Visual Studio Community Edition в комплекте со средствами Python и R для Windows (или Eclipse для Linux);
- Power BI Desktop для Windows;
- SQL Server 2016 Developer Edition для Windows (или Postgres для Linux).

Он также включает **средства машинного обучения и искусственного интеллекта**, такие как xgboost, mxnet и Vowpal Wabbit.

Сейчас виртуальная машина для обработки и анализа данных доступна для операционных систем **Windows** и **Linux CentOS**. Выберите нужный размер виртуальной машины для обработки и анализа данных (число ядер и размер памяти), исходя из потребностей проектов обработки и анализа данных, которые вы будете на ней выполнять. 

Дополнительные сведения о выпуске виртуальной машины для обработки и анализа данных для Windows см. в [этом описании](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-dsvm.dsvm-win-2019) на Azure Marketplace. Выпуск виртуальной машины для обработки и анализа данных для Linux [описан здесь](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-dsvm.ubuntu-1804).

Чтобы научиться эффективно выполнять некоторые распространенные задачи обработки и анализа данных на виртуальной машине для обработки и анализа данных, см. статью [10 задач, которые можно выполнить на виртуальной машине для обработки и анализа данных](../data-science-virtual-machine/vm-do-ten-things.md).


## <a name="azure-hdinsight-spark-clusters"></a>Кластеры Spark в Azure HDInsight

Apache Spark — это платформа параллельной обработки с открытым кодом, которая поддерживает обработку в памяти, чтобы повысить производительность приложений для анализа больших данных. Подсистема обработки Spark призвана ускорить разработку, повысить удобство использования и реализовать сложную аналитику. Возможности вычисления в памяти позволяют Spark эффективно применять итеративные алгоритмы в машинном обучении и графовых вычислениях. Подсистема Spark также совместима с хранилищем BLOB-объектов Azure (WASB), поэтому сможет легко обрабатывать существующие данные, хранящиеся в Azure.

При создании кластера Spark в HDInsight создание вычислительных ресурсов Azure следует выполнять после установки и настройки Spark. Создание кластера Spark в HDInsight займет около 10 минут. Разместите данные для обработки в хранилище BLOB-объектов Azure. Сведения об использовании хранилища BLOB-объектов Azure совместно с кластером см. в разделе [Использование службы хранилища Azure с кластерами Azure HDInsight](../../hdinsight/hdinsight-hadoop-use-blob-storage.md).

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для Python, другое — для Scala) по использованию кластеров Spark в Azure HDInsight для создания решений по обработке и анализу данных. Дополнительные сведения о кластерах Azure HDInsight **Spark** см. в разделе [Обзор. Apache Spark в HDInsight Linux](../../hdinsight/spark/apache-spark-overview.md). Чтобы узнать, как с помощью **Python** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Общие сведения об обработке и анализе данных с помощью платформы Spark в Azure HDInsight](spark-overview.md). Чтобы узнать, как с помощью **Scala** создать решение по обработке и анализу данных в кластере Spark в Azure HDInsight, см. статью [Обработка и анализ данных с использованием Scala и Spark в Azure](scala-walkthrough.md). 


##  <a name="azure-synapse-analytics"></a>Azure Synapse Analytics

Azure синапсе Analytics позволяет легко масштабировать ресурсы вычислений в секундах без чрезмерной подготовки или чрезмерной оплаты. Также оно предоставляет уникальную возможность приостановить использование вычислительных ресурсов, обеспечивая более гибкое управление затратами на облачные решения. Возможность развертывания масштабируемых ресурсов вычислений позволяет перенести все данные в Azure синапсе Analytics. Затраты на хранение здесь минимальны, а вычисления можно выполнять только для тех сегментов наборов данных, которые нужны вам для анализа. 

Дополнительные сведения об Azure синапсе Analytics см. на веб-сайте [Azure синапсе Analytics](https://azure.microsoft.com/services/sql-data-warehouse) . Сведения о создании комплексных решений для расширенной аналитики с помощью Azure синапсе Analytics см. в статье [процесс обработки и анализа данных группы в действии: использование Azure синапсе Analytics](sqldw-walkthrough.md).


## <a name="azure-data-lake"></a>Azure Data Lake;

Azure Data Lake — это репозиторий корпоративного уровня, позволяющий собрать в едином расположении данные любого типа, прежде чем применять к ним формальные требования или строгие схемы. Такая гибкость позволяет хранить в репозитории данные любого типа, любого размера и структуры, и принимать их с любой скоростью. Организации могут применить Hadoop или углубленную аналитику для поиска закономерностей в репозиториях типа Data Lake. Репозитории типа Data Lake могут также служить бюджетными репозиториями для подготовки данных перед их очисткой и переносом в хранилище данных.

Дополнительные сведения об Azure Data Lake см. в записи блога [Introducing Azure Data Lake](https://azure.microsoft.com/blog/introducing-azure-data-lake/) (Знакомство с Azure Data Lake). Чтобы узнать, как создать комплексное решение для обработки и анализа данных на базе Azure Data Lake, изучите руководство [Масштабируемая обработка и анализ данных с помощью Azure Data Lake. Полное пошаговое руководство](data-lake-walkthrough.md)


## <a name="azure-hdinsight-hive-hadoop-clusters"></a>Кластеры Hive (Hadoop) в Azure HDInsight

Apache Hive — это система хранилища данных для Hadoop, которая позволяет обобщать и анализировать данные, а также обрабатывать запросы с использованием HiveQL (язык запросов, подобный SQL). Hive можно использовать для интерактивного исследования данных или создания многократно используемых заданий пакетного задания обработки.

Hive позволяет создавать структуру для преимущественно неструктурированных данных. Определив такую структуру, вы сможете использовать Hive для отправки запросов к данным в кластере Hadoop. Для этого не нужно изучать Java или MapReduce. HiveQL (язык запросов Hive) позволяет создавать запросы, используя операторы, подобные операторам T-SQL.

Hive позволяет включать пользовательские функции Python в запросы Hive для обработки и анализа данных в записях. Так вы сможете значительно расширить потенциал запросов Hive при анализе данных. В частности, специалисты по обработке и анализу данных смогут создавать масштабируемые функции на языках, с которыми они лучше всего знакомы: HiveQL, который похож на SQL, и Python. 

Дополнительные сведения о кластерах Hive в Azure HDInsight см. в статье [Использование Hive и HiveQL с Hadoop в HDInsight](../../hdinsight/hadoop/hdinsight-use-hive.md). Чтобы научиться создавать масштабируемые комплексные решения для обработки и анализа данных на базе кластеров Hive в Azure HDInsight, см. статью [Командный процесс обработки и анализа данных на практике: использование хранилища данных SQL](hive-walkthrough.md).


## <a name="azure-file-storage"></a>Хранилище файлов Azure 

Хранилище файлов Azure — это служба, которая предоставляет доступ к общим папкам в облаке с использованием стандартного протокола SMB. Поддерживаются версии SMB 2.1 и SMB 3.0. Хранилище файлов Azure позволяет быстро и без дорогостоящей перезаписи выполнить перенос приложений прежних версий, связанных с общими папками. Приложения, работающие на виртуальных машинах Azure, в облачных службах или на локальных клиентах, могут подключать общую папку в облаке так же, как настольное приложение подключает обычную общую папку SMB. Любое количество компонентов приложений может одновременно подключаться и получать доступ к ресурсам хранилища файлов.

Особенно полезной для проектов по обработке и анализу данных будет возможность создать хранилище файлов Azure для совместного использования всеми участниками команды проекта. Все специалисты будут обращаться к одной и той же копии данных, размещенной в хранилище файлов Azure. Также с помощью этого хранилища файлов они смогут совместно использовать наборы функций, созданные во время выполнения проекта. Если проект предусматривает взаимодействие с клиентом, клиент может создать хранилище файлов Azure в своей подписке Azure и разместить в нем данные и компоненты проекта, чтобы предоставить вам доступ к ним. Это позволит клиенту сохранить полный контроль над ресурсами данных, используемыми в проекте. Дополнительные сведения о хранилище файлов Azure см. в статьях [Разработка для службы файлов Azure с помощью .NET](../../storage/files/storage-dotnet-how-to-use-files.md) и [Использование файлов Azure в Linux](../../storage/files/storage-how-to-use-files-linux.md).


## <a name="sql-server-2019-r-and-python-services"></a>SQL Server 2019 R и службы Python

Службы R (в базе данных) предоставляют платформу для разработки и развертывания интеллектуальных приложений, которые позволяют получить новые ценные сведения. Вы можете использовать полнофункциональный мощный язык R и множество пакетов, созданных сообществом R, чтобы разрабатывать модели и формировать прогнозы на основе данных, хранящихся в SQL Server. Службы R (в базе данных) интегрируют язык R с SQL Server, что позволяет выполнять анализ там же, где расположены данные. Так вы сможете избежать лишних затрат и рисков для безопасности, связанных с перемещением данных.

Службы R (в базе данных) поддерживают язык R с открытым кодом, а также широкий набор средств и технологий SQL Server. Они обеспечивают высокую производительность, безопасность, надежность и управляемость. Для развертывания решений R вам доступны удобные и знакомые средства. Ваши рабочие приложения могут вызывать среду выполнения R, получать прогнозы и визуальные элементы с использованием Transact-SQL. Кроме того, вы можете использовать библиотеки ScaleR для увеличения масштаба и производительности решений R. Дополнительные сведения см. в разделе [SQL Server R Services](/sql/advanced-analytics/r/sql-server-r-services).

Команда TDSP корпорации Майкрософт опубликовала два полных пошаговых руководства (одно для R-программистов, другое — для разработчиков SQL) по созданию решений для обработки и анализа данных в службах R для SQL Server 2016. Для **R-программистов** предлагается [полное пошаговое руководство по обработке и анализу данных](/sql/advanced-analytics/tutorials/walkthrough-data-science-end-to-end-walkthrough). Для **разработчиков SQL** будет полезно руководство по [аналитике в базе данных R для разработчиков SQL](/sql/advanced-analytics/tutorials/sqldev-in-database-r-for-sql-developers).


## <a name="appendix-tools-to-set-up-data-science-projects"></a><a name="appendix"></a>Приложение. Средства для настройки проектов по обработке и анализу данных

### <a name="install-git-credential-manager-on-windows"></a>Установка диспетчера учетных данных Git в Windows

Если вы организуете процесс TDSP на **Windows**, вам потребуется **диспетчер учетных данных Git (GCM)** для обмена данными с репозиториями Git. Перед установкой GCM необходимо сначала установить **Chocolaty**. Чтобы установить Chocolaty и GCM, выполните следующие команды в Windows PowerShell **с правами администратора**.  

```powershell
iwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex
choco install git-credential-manager-for-windows -y
```  

### <a name="install-git-on-linux-centos-machines"></a>Установка Git на компьютерах Linux (CentOS)

Выполните следующую команду в оболочке bash, чтобы установить Git на компьютере Linux (CentOS):

```powershell
sudo yum install git
```

### <a name="generate-public-ssh-key-on-linux-centos-machines"></a>Создание открытого ключа SSH на компьютерах Linux (CentOS)

Если для выполнения команд Git вы используете компьютер Linux (CentOS), на нем необходимо установить открытый ключ SSH, чтобы службы Azure DevOps Services распознали ваш компьютер. Для этого сначала нужно создать этот открытый ключ SSH, а затем добавить его в список открытых ключей SSH на странице настроек безопасности для Azure DevOps Services. 

1. Чтобы создать ключ SSH, выполните следующие две команды. 

   ```
   ssh-keygen
   cat .ssh/id_rsa.pub
   ```
   
   ![Команды генерации ключа SSH](./media/platforms-and-tools/resources-1-generate_ssh.png)

1. Скопируйте полный текст ключа SSH, включая *ssh-rsa*. 
1. Войдите в Azure DevOps Services. 
1. В правом верхнем углу страницы щелкните **<свое имя\>** и выберите пункт **Безопасность**. 
    
   ![Щелкните свое имя и выберите команду "Безопасность"](./media/platforms-and-tools/resources-2-user-setting.png)

1. Щелкните **Открытые ключи SSH** и нажмите кнопку **+Добавить**. 

   ![Щелкните "Открытые ключи SSH" и нажмите кнопку "+Добавить"](./media/platforms-and-tools/resources-3-add-ssh.png)

1. Вставьте в текстовое поле скопированный ключ SSH и сохраните его.


## <a name="next-steps"></a>Дальнейшие действия

Также предоставляются полные пошаговые руководства, которые демонстрируют все этапы процесса для **конкретных сценариев** . Эти этапы с иллюстрациями и краткими описаниями перечислены в [примерах пошаговых руководств](walkthroughs.md). В них показано, как объединить облачные и локальные средства и службы в единый рабочий процесс или конвейер, чтобы создать интеллектуальное приложение. 

Примеры выполнения шагов командного процесса обработки и анализа данных, в которых задействуется студия машинного обучения Azure (классическая), см. в схеме обучения с использованием [службы "Машинное обучение Azure"](./index.yml).