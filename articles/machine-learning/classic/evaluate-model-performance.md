---
title: 'ML Studio (классическая модель): Оценка моделей & перекрестной проверки в Azure'
description: Сведения о метриках, которые можно использовать для мониторинга производительности модели в Машинное обучение Azure Studio (классическая модель).
services: machine-learning
ms.service: machine-learning
ms.subservice: studio-classic
ms.topic: how-to
author: likebupt
ms.author: keli19
ms.custom: seodec18, previous-author=heatherbshapiro, previous-ms.author=hshapiro
ms.date: 03/20/2017
ms.openlocfilehash: b2ca78d30659fce6e4246c81216cae94b404955e
ms.sourcegitcommit: e972837797dbad9dbaa01df93abd745cb357cde1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/14/2021
ms.locfileid: "100520023"
---
# <a name="evaluate-model-performance-in-azure-machine-learning-studio-classic"></a>Оценка производительности модели в Машинное обучение Azure Studio (классическая модель)

**ПРИМЕНИМО К:**  ![Применимо к.](../../../includes/media/aml-applies-to-skus/yes.png)Студия машинного обучения (классическая)   ![Неприменимо к.](../../../includes/media/aml-applies-to-skus/no.png)[Машинное обучение Azure](../overview-what-is-machine-learning-studio.md#ml-studio-classic-vs-azure-machine-learning-studio)


В этой статье можно узнать о метриках, которые можно использовать для мониторинга производительности модели в Машинное обучение Azure Studio (классическая модель).  Оценка эффективности модели является одним из основных этапов процесса обработки и анализа данных. Она показывает, насколько успешно обученная модель обрабатывает (прогнозирует) набор данных. Машинное обучение Azure Studio (классическая модель) поддерживает оценку модели с помощью двух основных модулей машинного обучения: 
+ [Анализ модели][evaluate-model] 
+ [Модель перекрестной проверки][cross-validate-model]

Эти модули позволяют видеть эффективность модели в пересчете на различные показатели, обычно используемые в машинном обучении и статистике.

Оценка моделей должна рассматриваться вместе с:
+ [Оптимизация параметров для алгоритмов](algorithm-parameters-optimize.md)
+ [Интерпретируемость модели](interpret-model-results.md)

Доступны три стандартных сценария управляемого обучения: 
* регрессия;
* двоичная классификация; 
* классификация по нескольким классам.


## <a name="evaluation-vs-cross-validation"></a>Сравнение оценки и перекрестной проверки
Оценка и перекрестная проверка — это стандартные способы для измерения эффективности модели. Оба модуля генерируют показатели оценки, которые вы можете проверить или сравнить с показателями других моделей.

Функция " [Вычисление модели][evaluate-model] " предполагает, что в качестве входных данных используется оцененный DataSet (или два, если вы хотите сравнить производительность двух разных моделей). Поэтому необходимо обучить модель с помощью модуля [обучение модели][train-model] и создать прогнозы для некоторого набора данных с помощью модуля [Оценка модели][score-model] , прежде чем можно будет оценить результаты. Оценка основывается на подсчитанных метках или вероятностях и на истинных метках. Все эти значения предоставляет модуль [Score Model][score-model] (Оценка модели).

Кроме того, вы можете использовать перекрестную проверку, чтобы автоматически выполнить ряд операций "обучить-подсчитать-оценить" (10 сборок) для различных подмножеств входных данных. Входные данные делятся на 10 частей: одна резервируется для тестирования, а остальные 9 — для обучения. Этот процесс повторяется 10 раз, затем из показателей оценки выводится средняя величина. Эта процедура позволяет определить, насколько хорошо модель будет обобщаться на новых наборах данных. Модуль [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) берет необученную модель и несколько группированных наборов данных, а затем в дополнение к усредненным результатам выводит результаты оценки каждой из 10 сборок.

В следующих разделах мы создадим простые модели регрессии и классификации и оценим их эффективность, используя модули [Evaluate Model][evaluate-model] (Анализ модели) и [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели).

## <a name="evaluating-a-regression-model"></a>Оценка модели регрессии
Предположим, что мы хотим спрогнозировать цену автомобиля с помощью таких функций, как измерения, лошадиные мощности, спецификации ядер и т. д. Это типичная задача регрессии, где целевой переменной *price* (Цена) присвоено непрерывное числовое значение. Мы можем разместить модель линейной регрессии, которая, учитывая значения признаков определенного автомобиля, может прогнозировать стоимость этого автомобиля. Эту модель регрессии можно использовать для подсчета того же набора данных, который использовался для обучения. После получения прогнозируемых цен на автомобиль можно оценить производительность модели, просмотрев количество отклонений прогнозов от фактических цен в среднем. Чтобы проиллюстрировать это, мы используем набор данных « *данные о ценах автомобилей (RAW)* », доступный в разделе « **сохраненные наборы** данных» в машинное обучение Studio (классическая модель).

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в Машинное обучение Azure Studio (классическая модель):

* данные о ценах на автомобили (необработанные);
* [Линейная регрессия][linear-regression]
* [Train Model][train-model] (Обучение модели);
* [Оценка модели][score-model]
* [Анализ модели][evaluate-model]

Соедините порты, как показано на рисунке 1 ниже, и установите для столбца "Метка" модуля [Обучение модели][train-model] значение *цена*.

![Оценка модели регрессии](./media/evaluate-model-performance/1.png)

Рис. 1. Оценка модели регрессии.

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
После проведения эксперимента щелкните порт вывода модуля [Evaluate Model][evaluate-model] (Анализ модели) и выберите *Визуализировать*, чтобы отобразить результаты оценки. Для моделей регрессии доступны такие метрики оценки: *Mean Absolute Error* (Средняя абсолютная погрешность), *Root Mean Absolute Error* (Среднеквадратическая абсолютная погрешность), *Relative Absolute Error* (Относительная абсолютная погрешность), *Relative Squared Error* (Относительная среднеквадратическая погрешность) и *Coefficient of Determination* (Коэффициент детерминации).

Термин "ошибка" здесь означает разницу между прогнозируемым значением и истинным значением. Абсолютное значение или квадрат этой разницы обычно вычисляется, чтобы зафиксировать абсолютную величину ошибки во всех экземплярах, так как разница между прогнозируемым и истинным значением иногда может быть отрицательным числом. Показатели ошибки измеряют прогнозируемую эффективность модели регрессии с точки зрения среднего отклонения ее прогнозов от истинных значений. Чем ниже значения ошибок, тем более точно модель прогнозирует. Общий показатель ошибок 0 означает, что модель идеально подбирает данные.

Для определения способности модели подбирать данные также часто используется коэффициент детерминации, который также известен как R-квадрат. Его можно интерпретировать как пропорцию отклонений, которые объясняются моделью. В этом случае чем выше пропорция, тем лучше. Значение 1 означает идеальное совпадение.

![Показатели оценки линейной регрессии](./media/evaluate-model-performance/2.png)

Рис. 2. Показатели оценки линейной регрессии.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как уже упоминалось ранее, с помощью модуля [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) можно автоматически выполнять повторное обучение, оценку и анализ. Для этого вам потребуются набор данных, необученная модель и модуль [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) (см. рисунок ниже). Необходимо задать для столбца метка значение *Цена* в свойствах модуля [Перекрестная проверка модели][cross-validate-model] .

![Перекрестная проверка модели регрессии](./media/evaluate-model-performance/3.png)

Рис. 3. Перекрестная проверка модели регрессии.

После проведения эксперимента вы можете проверить результаты оценки. Для этого щелкните правый порт вывода модуля [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели). Вы увидите подробное представление показателей для каждой итерации (сборки) и усредненные результаты каждого из показателей (рис. 4).

![Результаты перекрестной проверки модели регрессии](./media/evaluate-model-performance/4.png)

Рис. 4 Результаты перекрестной проверки модели регрессии.

## <a name="evaluating-a-binary-classification-model"></a>Оценка модели двоичной классификации
При использовании двоичной классификации целевая переменная имеет только два возможных результата (например, {0, 1} или {ложь, истина}, {отрицательный, положительный}). Предположим, что у вас есть набор данных сотрудников для взрослых с некоторыми демографическими и трудовыми переменными, и вам будет предложено предсказать уровень дохода, двоичную переменную со значениями {"<= 50 K", ">50 K"}. Иными словами, отрицательный класс представляет работников, которые зарабатывают меньше 50 000 в год, а положительный класс представляет всех остальных работников. Как и в сценарии с регрессией, мы должны обучить модель, посчитать некоторые данные и оценивать результаты. Основное отличие заключается в выборе метрик Машинное обучение Azure Studio (классическая модель) вычислений и выходов. Чтобы проиллюстрировать ситуацию прогноза на уровне дохода, мы будем использовать набор данных для [взрослых](https://archive.ics.uci.edu/ml/datasets/Adult) для создания эксперимента Studio (классический) и оценки производительности модели логистической регрессии из двух классов — широко используемого двоичного классификатора.

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в Машинное обучение Azure Studio (классическая модель):

* набор данных Adult Census Income Binary Classification;
* [Двухклассовая регрессионная логистическая модель][two-class-logistic-regression]
* [Train Model][train-model] (Обучение модели);
* [Оценка модели][score-model]
* [Анализ модели][evaluate-model]

Соедините порты, как показано на рисунке 5 ниже, и установите для столбца "Метка" модуля [Обучение модели][train-model] значение *доход*.

![Оценка модели двоичной классификации](./media/evaluate-model-performance/5.png)

Рис. 5. Оценка модели двоичной классификации.

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
После проведения эксперимента щелкните порт вывода модуля [Evaluate Model][evaluate-model] (Анализ модели) и выберите *Визуализировать*, чтобы увидеть результаты оценки (рис. 7). Для моделей двоичной классификации доступны такие метрики оценки: *Accuracy* (Правильность), *Precision* (Точность), *Recall* (Полнота), *F1 Score* (Оценка F1) и *AUC*. Кроме того, модуль выводит матрицы неточностей, которые отображают число истинно положительных, ложноотрицательных, ложноположительных и истинно отрицательных результатов, а также кривые *ROC*, *Precision/Recall* (Точность и полнота) и *Lift* (Точность прогноза).

Правильность выражается пропорцией правильно классифицированных экземпляров. Это, как правило, первый показатель, который вы видите во время оценки классификатора. Однако если тестовые данные не сбалансированы (где большинство экземпляров принадлежат одному из классов) или вы больше заинтересованы в производительности одного из классов, точность не захватывает эффективность классификатора. Предположим, вы тестируете в сценарии классификации уровня дохода, данные, в которых 99 % экземпляров представляют людей, которые зарабатывают меньше или ровно 50 000 $ в год. Можно добиться точности 0,99, спрогнозировать класс «<= 50 000» для всех экземпляров. Кажется, что классификатор в целом хорошо справляется с заданием, но в действительности он не смог правильно классифицировать ни одно из лиц с высоким уровнем дохода (1 %).

Поэтому будет целесообразно вычислить дополнительные показатели, которые фиксируют более конкретные аспекты оценки. Прежде чем углубляться в подробности таких показателей, важно понять матрицу неточностей оценки двоичной классификации. Метки классов в обучающем наборе могут принимать только два возможных значения, которые обычно называются положительными или отрицательными. Положительные и отрицательные экземпляры, которые классификатор прогнозирует правильно, называются истинно положительными (ИП) и истинно отрицательными (ИО) результатами соответственно. Точно так же неправильно классифицированные экземпляры называются ложно положительными (ЛП) и ложно отрицательными результатами (ЛО). Матрица путаницы — это просто таблица, показывающая количество экземпляров, которые попадают под каждую из этих четырех категорий. Машинное обучение Azure Studio (классическая модель) автоматически определяет, какой из двух классов в наборе данных является положительным классом. Если метки класса являются логическими или целыми числами, то экземплярам с метками "true" или "1" назначается положительный класс. Если метки являются строками, например с набором данных о доходах, то метки сортируются в алфавитном порядке, а первый уровень выбирается как отрицательный класс, а второй — как положительный.

![Матрица неточностей двоичной классификации](./media/evaluate-model-performance/6a.png)

Рис. 6. Матрица неточностей двоичной классификации.

Возвращаясь к проблеме классификации доходов, нужно задать несколько оценочных вопросов, которые помогут определить эффективность используемого классификатора. Естественным вопросом является следующее: "из тех, кто прогнозируется в модели >50 K (TP + FP), сколько было правильно классифицировано (TP)?" На этот вопрос можно ответить, взглянув на показатель **точности** модели, который представляет долю правильно классифицированных положительных результатов: ИП / (ИП + ЛП). Еще один распространенный вопрос — «из всех высоко ценных сотрудников с доходами >50 000 (TP + FN), сколько классификаторов правильно классифицируется (TP). Это значение представлено показателем **полноты** или процента истинно положительных результатов классификатора: ИП / (ИП + ЛП). Вы могли заметить, что существует очевидный компромисс между точностью и полнотой. Например, обрабатывая относительно сбалансированный набор данных, классификатор, который прогнозирует в основном положительные экземпляры, будет иметь высокий уровень полноты, но довольно низкий уровень точности, так как многие отрицательные экземпляры будут неправильно классифицированы из-за большого количества ложно позитивных результатов. Чтобы увидеть график изменения этих двух показателей, щелкните кривую **ТОЧНОСТЬ/ПОЛНОТА** на странице вывода результатов оценки (верхняя левая часть рисунка 7).

![Результаты оценки двоичной классификации](./media/evaluate-model-performance/7.png)

Рис. 7. Результаты оценки двоичной классификации.

Не менее часто используется показатель **оценки F1**, который учитывает и точность, и полноту. Это среднее гармоническое этих двух метрик и вычисляется следующим образом: F1 = 2 (точность x отозвать)/(точность и отзыв). Оценка F1 — хороший способ суммирования оценки по одному числу, но рекомендуется рассмотреть как точность, так и отзыв, чтобы лучше понять, как работает классификатор.

Кроме того, вы можете сравнить доли истинно положительных результатов и ложноположительных результатов, представленных кривой **рабочей характеристики приемника (ROC)** и соответствующим значением **площади под ROC-кривой (AUC)**. Чем ближе эта кривая к верхнему левому углу, тем выше производительность классификатора (то есть максимальная положительная скорость, а при минимизации ложных положительных ставок). Кривые, близкие к диагонали графика, получаются из классификаторов, которые, как правило, делают прогнозы, близкие к случайному угадыванию.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как и в примере регрессии, можно выполнить перекрестную проверку для многократного обучения, оценки и оценки разных подмножеств данных автоматически. Подобным образом мы можем использовать модуль [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели), необученную регрессионную логистическую модель и набор данных. Столбец меток должен иметь значение " *доход* " в свойствах модуля [перекрестной проверки модели][cross-validate-model] . Если по завершении эксперимента щелкнуть правый порт вывода в модуле [Cross-Validate Model][cross-validate-model] (Модель перекрестной проверки), отобразятся значения метрик двоичной классификации для каждой свертки, а также среднее значение и стандартное отклонение каждого из них. 

![Перекрестная проверка модели двоичной классификации](./media/evaluate-model-performance/8.png)

Рис. 8. Перекрестная проверка модели двоичной классификации.

![Результаты перекрестной проверки модели двоичной классификации](./media/evaluate-model-performance/9.png)

Рис. 9. Результаты перекрестной проверки модели двоичной классификации.

## <a name="evaluating-a-multiclass-classification-model"></a>Оценка модели классификации по нескольким классам
В этом эксперименте мы будем использовать популярный набор данных [IRI](https://archive.ics.uci.edu/ml/datasets/Iris "IRI") , который содержит экземпляры трех различных типов (классов) растения. Существует четыре значения компонентов (чашелистика длина, ширина и длина лепестка/ширина) для каждого экземпляра. В предыдущих экспериментах мы обучили и протестировали модели с помощью тех же наборов данных. Здесь мы будем использовать модуль [Split Data (разделение данных][split] ) для создания двух подмножеств данных, обучения по первому и оценке и оценки во втором. Набор данных Iris находится в открытом доступе в [репозитории машинного обучения UCI](https://archive.ics.uci.edu/ml/index.html). Его можно скачать с помощью модуля [импорта данных][import-data].

### <a name="creating-the-experiment"></a>Создание эксперимента
Добавьте следующие модули в рабочую область в Машинное обучение Azure Studio (классическая модель):

* [Импорт данных][import-data]
* [Мультиклассовый лес принятия решений][multiclass-decision-forest]
* [Split Data][split] (Разделение данных);
* [Train Model][train-model] (Обучение модели);
* [Оценка модели][score-model]
* [Анализ модели][evaluate-model]

Соедините порты, как показано на рисунке 10.

Установите значение 5 для индекса столбца "Метка" в модуле [Обучение модели][train-model]. У этого набора данных нет строки заголовка, но мы знаем, что этикетки находятся в пятом столбце.

Щелкните модуль [импорта данных][import-data] и задайте для свойства *Источник данных* значение *Web URL via HTTP* (URL-адрес с использованием протокола HTTP), а для свойства *URL-адреса* — http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data.

Укажите дробное число экземпляров, которые будут использоваться для обучения модуля [разделения данных][split] (например, 0,7).

![Оценка классификатора с несколькими классами](./media/evaluate-model-performance/10.png)

Рис. Оценка классификатора с несколькими классами

### <a name="inspecting-the-evaluation-results"></a>Проверка результатов оценки
Запустите эксперимент и щелкните порт вывода в модуле [Evaluate Model][evaluate-model] (Анализ модели). В этом случае результаты оценки представлены в виде матрицы неточностей. В матрице показаны фактические и прогнозируемые экземпляры для всех трех классов.

![Результаты оценки классификации по нескольким классам](./media/evaluate-model-performance/11.png)

Рис. 11. Результаты оценки классификации по нескольким классам.

### <a name="using-cross-validation"></a>Использование перекрестной проверки
Как уже упоминалось ранее, с помощью модуля [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) можно автоматически выполнять повторное обучение, оценку и анализ. Вам потребуется набор данных, необученная модель и модуль [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) (см. рисунок ниже). Снова нужно установить значение для столбца "Метка" в модуле [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели) (в данном случае индекс столбца — 5). Если по завершении эксперимента щелкнуть правый порт вывода в модуле [Cross-Validate Model][cross-validate-model] (Перекрестная проверка модели), вы увидите значения показателей для каждой свертки, а также среднее значение и стандартное отклонение. Отображаемые здесь показатели похожи на показатели, о которых шла речь в разделе, посвященном двоичной классификации. Однако в многоклассовой классификации вычисление истинных и отрицательных положительных и ложных положительных и отрицательных результатов выполняется путем подсчета для каждого класса, так как отсутствует общий положительный или отрицательный класс. Например, при вычислении точности или отзыва класса "IRI-setosa" предполагается, что это положительный класс, а все остальные — отрицательные.

![Перекрестная проверка модели классификации по нескольким классам](./media/evaluate-model-performance/12.png)

Рис. 12. Перекрестная проверка модели классификации по нескольким классам.

![Результаты перекрестной проверки модели классификации по нескольким классам](./media/evaluate-model-performance/13.png)

Рис. 13. Результаты перекрестной проверки модели классификации по нескольким классам.

<!-- Module References -->
[cross-validate-model]: /azure/machine-learning/studio-module-reference/cross-validate-model
[evaluate-model]: /azure/machine-learning/studio-module-reference/evaluate-model
[linear-regression]: /azure/machine-learning/studio-module-reference/linear-regression
[multiclass-decision-forest]: /azure/machine-learning/studio-module-reference/multiclass-decision-forest
[import-data]: /azure/machine-learning/studio-module-reference/import-data
[score-model]: /azure/machine-learning/studio-module-reference/score-model
[split]: /azure/machine-learning/studio-module-reference/split-data
[train-model]: /azure/machine-learning/studio-module-reference/train-model
[two-class-logistic-regression]: /azure/machine-learning/studio-module-reference/two-class-logistic-regression