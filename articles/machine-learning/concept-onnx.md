---
title: 'ONNX: определение высокой производительности, кросс-платформенного вывода'
titleSuffix: Azure Machine Learning
description: Узнайте, как использовать открытый обмен нейронными сетями (ONNX) для оптимизации вывода модели машинного обучения.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: prasantp
author: prasanthpul
ms.date: 06/18/2020
ms.custom: seodec18
ms.openlocfilehash: 5a1ed6c43e4d3a96e0e19d7be50e1e16bcab780e
ms.sourcegitcommit: 3af12dc5b0b3833acb5d591d0d5a398c926919c8
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/11/2021
ms.locfileid: "98071244"
---
# <a name="onnx-and-azure-machine-learning-create-and-accelerate-ml-models"></a>ONNX и Машинное обучение Azure: создание и ускорение моделей машинного обучения

Узнайте, как использовать [открытый обмен нейронными сетями](https://onnx.ai) (ONNX) для оптимизации вывода модели машинного обучения. Вывод или оценка модели — это этап, в котором развернутая модель используется для прогнозирования, чаще всего в рабочих данных. 

Оптимизация моделей машинного обучения для определения (или оценки модели) усложняется, поскольку необходимо настроить модель и библиотеку вывода, чтобы максимально эффективно использовать возможности оборудования. Проблема станет чрезвычайно трудной, если требуется обеспечить оптимальную производительность различных типов платформ (Cloud/ребр, ЦП, GPU и т. д.), так как каждый из них имеет различные возможности и характеристики. Сложность возрастает при наличии моделей из различных платформ, которые должны выполняться на различных платформах. Оптимизация всех различных сочетаний платформ и оборудования занимает очень много времени. Решение для обучения в вашей предпочитаемой платформе и выполнения в любом месте в облаке или на границе. Именно здесь ONNX входит в.

Корпорация Майкрософт и сообщество партнеров, созданных ONNX как открытый стандарт для представления моделей машинного обучения. Модели из [многих платформ](https://onnx.ai/supported-tools) , включая TensorFlow, PyTorch, SciKit-учиться, keras, Chain, MXNET, MATLAB и SparkML, можно экспортировать или преобразовать в стандартный формат ONNX. После преобразования моделей в формат ONNX их можно запускать на различных платформах и устройствах.

[Среда выполнения ONNX](https://onnxruntime.ai) — это высокопроизводительный механизм вывода для развертывания моделей ONNX в рабочей среде. Она оптимизирована как для облака, так и для пограничной работы в Linux, Windows и Mac. Написанный на C++, он также содержит интерфейсы API C, Python, C#, Java и JavaScript (Node.js) для использования в различных средах. Среда выполнения ONNX поддерживает как DNN, так и традиционные модели машинного обучения и интегрируется с ускорителями на различных оборудованиях, таких как Тенсоррт на графических процессорах NVidia, Опенвино на процессорах Intel, Директмл в Windows и т. д. С помощью среды выполнения ONNX вы можете воспользоваться обширными оптимизациями рабочего уровня, тестированием и текущими улучшениями.

Среда выполнения ONNX используется в крупномасштабных службах Майкрософт, таких как Bing, Office и Azure Cognitive Services. Выигрыш в производительности зависит от ряда факторов, но эти службы Майкрософт показали __среднее увеличение производительности ЦП на 2 раза__. Помимо Машинное обучение Azure Services, среда выполнения ONNX также работает в других продуктах, поддерживающих Машинное обучение рабочих нагрузок, в том числе:
+ Windows. Среда выполнения встроена в Windows в составе [windows машинное обучение](/windows/ai/windows-ml/) и работает на сотнях миллионов устройств. 
+ Семейство продуктов Azure SQL: выполнение собственной оценки данных в [Azure SQL](../azure-sql-edge/onnx-overview.md) и azure SQL [управляемый экземпляр](../azure-sql/managed-instance/machine-learning-services-overview.md).
+ ML.NET: [Запуск моделей ONNX в ml.NET](/dotnet/machine-learning/tutorials/object-detection-onnx).


[![Схема потока ONNX, показывающая обучение, конвертеры и развертывание](./media/concept-onnx/onnx.png)](././media/concept-onnx/onnx.png#lightbox)

## <a name="get-onnx-models"></a>Получение моделей ONNX

Модели ONNX можно получить различными способами:
+ Обучение новой модели ONNX в Машинное обучение Azure (см. примеры в нижней части этой статьи) или с помощью [автоматизированных машинное обучениеных возможностей](concept-automated-ml.md#automl--onnx)
+ Преобразование существующей модели из другого формата в ONNX (см. [учебники](https://github.com/onnx/tutorials)) 
+ Получение предварительно обученной модели ONNX из [коллекции моделей ONNX](https://github.com/onnx/models).
+ Создание настраиваемых моделей ONNX в [пользовательской службе визуального распознавания Azure](../cognitive-services/custom-vision-service/index.yml). 

Многие модели, включая классификацию изображений, обнаружение объектов и обработку текста, могут быть представлены в виде ONNX моделей. Если при работе с моделью, которая не может быть преобразована, возникла ошибка, запишите вопрос в GitHub соответствующего конвертера, который вы использовали. Вы можете продолжать использовать существующую модель форматирования, пока не будет устранена данная ошибка.

## <a name="deploy-onnx-models-in-azure"></a>Развертывание моделей ONNX в Azure

С помощью Машинное обучение Azure можно развертывать и отслеживать модели ONNX, а также управлять ими. Используя стандартный [рабочий процесс развертывания](concept-model-management-and-deployment.md) и среду выполнения ONNX, можно создать конечную точку REST, размещенную в облаке. См. пример записных книжек Jupyter в конце этой статьи, чтобы испытать ее самостоятельно. 

### <a name="install-and-use-onnx-runtime-with-python"></a>Установка и использование среды выполнения ONNX с Python

Пакеты Python для среды выполнения ONNX доступны в [PyPi.org](https://pypi.org) ([ЦП](https://pypi.org/project/onnxruntime), [GPU](https://pypi.org/project/onnxruntime-gpu)). Перед установкой ознакомьтесь с [требованиями к системе](https://github.com/Microsoft/onnxruntime#system-requirements) . 

 Чтобы установить среду выполнения ONNX для Python, используйте одну из следующих команд: 
```python   
pip install onnxruntime       # CPU build
pip install onnxruntime-gpu   # GPU build
```

Для вызова среды выполнения ONNX в сценарии Python используйте следующую команду.    
```python
import onnxruntime
session = onnxruntime.InferenceSession("path to model")
```

Входные и выходные данные для использования модели см. в документации к соответствующей модели. Для просмотра модели можно также использовать такое средство визуализации, как [Netron](https://github.com/lutzroeder/Netron). Среда выполнения ONNX также позволяет запрашивать метаданные модели и ее входные и выходные данные:    
```python
session.get_modelmeta()
first_input_name = session.get_inputs()[0].name
first_output_name = session.get_outputs()[0].name
```

Для вывода модели добавьте параметр `run` и передайте список выходных данных, которые хотите получить (не заполняйте, если вам нужны все), а также карту входных значений. Результатом будет список выходных данных.  
```python
results = session.run(["output1", "output2"], {
                      "input1": indata1, "input2": indata2})
results = session.run([], {"input1": indata1, "input2": indata2})
```

Полный справочник по API Python см. в [документации по среде выполнения ONNX](https://aka.ms/onnxruntime-python).    

## <a name="examples"></a>Примеры
См. статью [Использование-azureml/Deployment/onnx](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/deployment/onnx) , например записные книжки Python, которые создают и развертывают модели onnx.

[!INCLUDE [aml-clone-in-azure-notebook](../../includes/aml-clone-for-examples.md)]

Примеры использования на других языках можно найти в [ONNX среды выполнения GitHub](https://github.com/microsoft/onnxruntime/tree/master/samples).

## <a name="more-info"></a>Дополнительные сведения

Дополнительные сведения о **ONNX** или участия в проекте:
+ [Веб-сайт проекта ONNX](https://onnx.ai)
+ [Код ONNX в GitHub](https://github.com/onnx/onnx)

Дополнительные сведения о **среде выполнения ONNX** или участия в проекте:
+ [Веб-сайт проекта среды выполнения ONNX](https://onnxruntime.ai)
+ [Репозиторий GitHub для среды выполнения ONNX](https://github.com/Microsoft/onnxruntime)