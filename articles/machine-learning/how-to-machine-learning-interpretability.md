---
title: Интерпретируемость модели в Машинное обучение Azure (Предварительная версия)
titleSuffix: Azure Machine Learning
description: Узнайте, как понять, & Объясните, как модель машинного обучения делает прогнозы во время обучения & при помощи пакета SDK для Python Машинное обучение Azure.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.custom: how-to, responsible-ml
ms.author: mithigpe
author: minthigpen
ms.reviewer: Luis.Quintanilla
ms.date: 02/25/2021
ms.openlocfilehash: 44ccf6b6d2459b87040fcac7d9cdcd336cc7b82f
ms.sourcegitcommit: 956dec4650e551bdede45d96507c95ecd7a01ec9
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/09/2021
ms.locfileid: "102522042"
---
# <a name="model-interpretability-in-azure-machine-learning-preview"></a>Интерпретируемость модели в Машинное обучение Azure (Предварительная версия)


## <a name="model-interpretability-overview"></a>Обзор интерпретации модели

Интерпретируемость модели важна для специалистов по обработке и анализу данных, аудиторов и руководителей бизнес-решений, чтобы обеспечить соответствие политикам компании, отраслевым стандартам и государственным нормам.

+ Специалистам по обработке и анализу данных необходима возможность объяснить свои модели руководителям и заинтересованным лицам, чтобы они могли понять ценность и точность их результатов. Они также нуждаются в интерпретации для отладки своих моделей и принятия взвешенных решений о том, как их улучшить. 

+ Для юридических аудиторий требуются средства для проверки моделей с учетом соответствия нормативным требованиям и отслеживания того, как решения моделей влияют на человека. 

+ Руководителям, ответственным за принятие бизнес-решений, требуется возможность обеспечить прозрачность для конечных пользователей. Это позволяет им получать и поддерживать отношения доверия.

Включение возможности объяснения модели машинного обучения важно в ходе двух основных этапов разработки модели:

+ На этапе обучения конструкторы моделей и оценивающие могут использовать результаты интерпретации модели для проверки на наличие этих данных и создания отношений доверия с заинтересованными лицами. Они также используют аналитические данные модели для отладки, проверки поведения модели в соответствии с целями, а также для проверки недостоверности и незначительных функций модели.

+ На этапе возникновения проблемы, так как наличие прозрачности в развернутых моделях позволяет руководителям понять, как работает модель, и как ее решения будут рассматриваться и повлиять на людей в реальном времени. 

## <a name="interpretability-with-azure-machine-learning"></a>Интерпретируемость с помощью Машинное обучение Azure

Классы интерпретации модели доступны через следующий пакет SDK: (сведения об [установке пакетов SDK для машинное обучение Azure](/python/api/overview/azure/ml/install))

* `azureml.interpret`, содержит функциональные возможности, поддерживаемые корпорацией Майкрософт.

Используется `pip install azureml-interpret` для общего использования.

## <a name="how-to-interpret-your-model"></a>Как интерпретировать модель

С помощью классов и методов в пакете SDK можно:
+ Объясните Прогноз модели, создав значения важности функций для всей модели и (или) отдельных точек. 
+ Обеспечить интерпретируемость модели в реальных наборах данных в масштабе, во время обучения и вывода.
+ Использование интерактивной панели мониторинга визуализации для обнаружения закономерностей в данных и объяснениях во время обучения

В машинном обучении **функции** — это поля данных, используемые для прогнозирования целевой точки данных. Например, для прогнозирования кредитного риска можно использовать поля данных для возраста, размера учетной записи и возраста учетной записи. В этом случае срок хранения, размер учетной записи и возраст учетной записи являются **функциями**. Важность функции сообщает, как каждое поле данных затронуло прогнозы модели. Например, возраст может сильно использоваться в прогнозе, тогда как размер и возраст учетной записи не влияют на прогнозируемые значения. Этот процесс позволяет специалистам по обработке и анализу данных объяснить результирующие прогнозы, чтобы заинтересованные лица могли видеть, какие функции наиболее важны в модели.

## <a name="supported-interpretability-techniques"></a>Поддерживаемые методы интерпретации

 `azureml-interpret` использует методики интерпретации, разработанные в [интерпретированном сообществе](https://github.com/interpretml/interpret-community/), пакет Python с открытым исходным кодом для обучения интерпретируемых моделей и помогая объяснить блаккбокс AI-системы. [Интерпретатор-сообщество](https://github.com/interpretml/interpret-community/) выступает в качестве узла для поддерживаемых объяснений SDK и в настоящее время поддерживает следующие методы интерпретации:

|Методика интерпретации|Описание|Тип|
|--|--|--------------------|
|Пояснение к дереву ШАП| Пояснение к дереву [ШАП](https://github.com/slundberg/shap), в котором основное внимание уделяется алгоритму оценки скорости быстрого ШАП значений времени, характерному для **деревьев и это совокупности деревьев**.|Зависящие от модели|
|Глубокое пояснение ШАП| Основываясь на пояснениях от ШАП, глубокая пояснения — алгоритм аппроксимации с высокой скоростью для ШАП значений в моделях глубокого обучения, который строится на связи с Диплифт, описанным в [документе ШАП НИПС](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions). Модели **TensorFlow** и модели **keras** с использованием серверной части TensorFlow поддерживаются (также доступна предварительная поддержка PyTorch).|Зависящие от модели|
|Шап линейное пояснение| Линейное пояснение ШАП рассчитывает значения ШАП для **линейной модели**, при необходимости учитывая взаимные корреляции функций.|Зависящие от модели|
|Пояснение ядра ШАП| В объяснении ядра ШАП используется специально взвешенная локальная линейная регрессия для оценки значений ШАП для **любой модели**.|Не зависит от модели|
|Пояснение к процедуре "имитировать" (Глобальный суррогат)| Концепция имитируется на основе представления [глобальных суррогатных моделей](https://christophm.github.io/interpretable-ml-book/global.html) для имитации блаккбокс моделей. Глобальная суррогатная модель — это внутренняя интерпретируемая модель, которая обучена для приблизительных прогнозов **любой модели с черными ящиками** как можно точнее. Специалисты по обработке и анализу данных могут интерпретировать суррогатную модель, чтобы рисовать выводы о модели черного ящика. В качестве суррогатной модели можно использовать одну из следующих интерпретируемых моделей: LightGBM (Лгбмексплаинаблемодел), линейная регрессия (Линеарексплаинаблемодел), модель с метод стохастического градиента (Сгдексплаинаблемодел) и дерево принятия решений (ДеЦисионтриексплаинаблемодел).|Не зависит от модели|
|Пояснение по важности функции перестановки (ПФИ)| Важность функции перестановки — это методика, используемая для объяснения моделей классификации и регрессии, которые представляют собой [статью о случайных лесах бреиман](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) (см. раздел 10). На высоком уровне, как это работает, случайным образом перетасовывание данные по одному компоненту для всего набора данных и вычислению объема изменений в показателях производительности. Чем больше изменение, тем важнее компонент. ПФИ может объяснить общее поведение **любой базовой модели** , но не объясняет отдельные прогнозы. |Не зависит от модели|

Помимо методов интерпретации, описанных выше, мы поддерживаем более ШАПное объяснение, именуемое `TabularExplainer` . В зависимости от модели `TabularExplainer` использует один из поддерживаемых пояснений ШАП:

* Триексплаинер для всех моделей на основе дерева
* Дипексплаинер для моделей DNN
* Линеарексплаинер для линейных моделей
* Кернелексплаинер для всех других моделей

`TabularExplainer` также обладает значительными усовершенствованиями функций и производительности по прямым ШАП объяснениям:

* **Формирование сводных данных инициализации**. В случаях, когда скорость объяснения наиболее важна, мы суммируем набор данных инициализации и создадим небольшой набор репрезентативных образцов, который ускоряет создание общих и индивидуальных значений важности функций.
* **Выборка набора оценочных данных**. Если пользователь передает большой набор образцов оценки, но на самом деле они не требуют оценки, для параметра выборки можно задать значение true, чтобы ускорить вычисление общих объяснений модели.

На следующей схеме показана текущая структура поддерживаемых объяснений.

[![Архитектура интерпретации Машинное обучение](./media/how-to-machine-learning-interpretability/interpretability-architecture.png)](./media/how-to-machine-learning-interpretability/interpretability-architecture.png#lightbox)


## <a name="supported-machine-learning-models"></a>Поддерживаемые модели машинного обучения

`azureml.interpret`Пакет SDK поддерживает модели, обученные со следующими форматами наборов данных:
- `numpy.array`
- `pandas.DataFrame`
- `iml.datatypes.DenseData`
- `scipy.sparse.csr_matrix`

Функции пояснения принимают в качестве входных данных модели и конвейеры. Если модель предоставлена, то модель должна реализовать функцию прогнозирования `predict` или `predict_proba` соответствует соглашению Scikit. Если модель не поддерживает эту функцию, можно создать оболочку для модели в функции, которая создает тот же результат, что `predict` и или `predict_proba` в Scikit, и использовать эту функцию-оболочку с выбранным объяснением. Если указан конвейер, функция пояснения предполагает, что выполняющийся скрипт конвейера возвращает прогноз. Использование этой методики упаковки `azureml.interpret` может поддерживать модели, обученные с помощью платформ PyTorch, TensorFlow и keras Deep Train, а также классических моделей машинного обучения.

## <a name="local-and-remote-compute-target"></a>Локальный и удаленный целевые объекты вычислений

`azureml.interpret`Пакет предназначен для работы с локальными и удаленными целевыми объектами вычислений. При локальном запуске функции пакета SDK не будут обращаться к службам Azure. 

Вы можете запустить объяснение удаленно на Машинное обучение Azure вычислить и записать пояснения в службу журнала выполнения Машинное обучение Azure. После записи этих сведений отчеты и визуализации из описания можно легко найти в Машинное обучение Azure Studio для анализа пользователей.


## <a name="next-steps"></a>Дальнейшие действия

- См. [инструкции](how-to-machine-learning-interpretability-aml.md) по включению интерпретации для моделей, как локально, так и на машинное обучение Azure удаленных ресурсов вычислений. 
- Дополнительные сценарии см. в [примерах записных книжек](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/explain-model) . 
- Если вы заинтересованы в работе с текстовыми сценариями, см. раздел [интерпретируемый текст](https://github.com/interpretml/interpret-text), связанный репозиторий с открытым кодом для [интерпретации сообщества](https://github.com/interpretml/interpret-community/), для методов интерпретации для NLP. `azureml.interpret` в настоящее время пакет не поддерживает эти методы, но вы можете приступить к работе с [примером записной книжки на классификации текста](https://github.com/interpretml/interpret-text/blob/master/notebooks/text_classification/text_classification_classical_text_explainer.ipynb).