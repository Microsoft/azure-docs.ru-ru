---
title: Развертывание самостоятельно размещенного шлюза в Kubernetes | Документация Майкрософт
description: Узнайте, как развернуть компонент шлюза с самостоятельным размещением службы управления API Azure в Kubernetes
services: api-management
author: vladvino
manager: gwallace
ms.service: api-management
ms.workload: mobile
ms.topic: article
ms.author: apimpm
ms.date: 04/23/2020
ms.openlocfilehash: 023c2c89b90d6ddc71abc95db325dcdeb7684a2d
ms.sourcegitcommit: 772eb9c6684dd4864e0ba507945a83e48b8c16f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "89500136"
---
# <a name="deploy-a-self-hosted-gateway-to-kubernetes"></a>Развертывание независимого шлюза в Kubernetes

В этой статье описаны действия по развертыванию компонента самостоятельно размещенного шлюза службы управления API Azure в кластере Kubernetes.

## <a name="prerequisites"></a>Предварительные требования

- Выполните задачи в кратком руководстве по [созданию экземпляра службы управления API Azure](get-started-create-service-instance.md).
- Создайте кластер Kubernetes.
   > [!TIP]
   > [Кластеры с одним узлом](https://kubernetes.io/docs/setup/#learning-environment) хорошо работают в целях разработки и оценки. Используйте [сертифицированные Kubernetes](https://kubernetes.io/partners/#conformance) кластеры с несколькими узлами в локальной среде или в облаке для рабочих нагрузок.
- [Подготавливает ресурс шлюза с самостоятельным размещением в экземпляре управления API](api-management-howto-provision-self-hosted-gateway.md).

## <a name="deploy-to-kubernetes"></a>Развертывание в Kubernetes

1. Выберите **шлюзы** в разделе **развертывание и инфраструктура**.
2. Выберите ресурс самостоятельно размещенного шлюза, который требуется развернуть.
3. Выберите **развертывание**.
4. Маркер доступа в текстовом поле **маркера** был создан автоматически на основе значений **срока действия** по умолчанию и **секретного ключа** . При необходимости выберите значения в одном или обоих элементах управления для создания нового маркера.
5. Выберите вкладку **Kubernetes** в разделе **сценарии развертывания**.
6. Выберите ссылку на файл **\<gateway-name\> yml** и скачайте файл YAML.
7. Щелкните значок **копирования** в правом нижнем углу текстового поля **развернуть** , чтобы сохранить `kubectl` команды в буфер обмена.
8. Вставка команд в окно терминала (или команда). Первая команда создает секрет Kubernetes, который содержит маркер доступа, созданный на шаге 4. Вторая команда применяет файл конфигурации, скачанный на шаге 6, в кластер Kubernetes и ждет, что файл находится в текущем каталоге.
9. Выполните команды, чтобы создать необходимые объекты Kubernetes в [пространстве имен по умолчанию](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) и запустить собственные модули шлюзов из [образа контейнера](https://aka.ms/apim/sputnik/dhub) , скачанного из реестра контейнеров Майкрософт.
10. Выполните следующую команду, чтобы проверить, было ли развертывание выполнено. Обратите внимание, что для создания объектов и для инициализации модулей Pod может потребоваться немного времени.
    ```console
    kubectl get deployments
    NAME             READY   UP-TO-DATE   AVAILABLE   AGE
    <gateway-name>   1/1     1            1           18s
    ```
11. Выполните следующую команду, чтобы проверить, успешно ли создана служба. Обратите внимание, что IP-адреса и порты службы будут отличаться.
    ```console
    kubectl get services
    NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
    <gateway-name>   LoadBalancer   10.99.236.168   <pending>     80:31620/TCP,443:30456/TCP   9m1s
    ```
12. Вернитесь к портал Azure и выберите **Обзор**.
13. Убедитесь, что в поле **состояние** отображается зеленая галочка, а затем число узлов, соответствующее количеству реплик, указанных в файле YAML. Это состояние означает, что развернутые модули шлюзов для автономного шлюза успешно взаимодействуют со службой управления API и имеют обычный "Пульс".

    ![Состояние шлюза](media/how-to-deploy-self-hosted-gateway-kubernetes/status.png)

> [!TIP]
> Выполните <code>kubectl logs deployment/<gateway-name></code> команду, чтобы просмотреть журналы из случайного выбора Pod, если имеется более одного.
> Выполните <code>kubectl logs -h</code> полный набор параметров команды, например просмотр журналов для определенного Pod или контейнера.

## <a name="production-deployment-considerations"></a>Вопросы развертывания в рабочей среде

### <a name="access-token"></a>Маркер доступа
Без действительного маркера доступа автономный шлюз не сможет получить доступ к данным конфигурации и загрузить их из конечной точки связанной службы управления API. Маркер доступа может быть допустимым не более 30 дней. Его необходимо повторно создать, а кластер, настроенный с помощью нового маркера, вручную или через автоматизацию до истечения срока действия.

При автоматизации обновления токена используйте [эту операцию API управления](/rest/api/apimanagement/2019-12-01/gateway/generatetoken) для создания нового маркера. Сведения об управлении секретами Kubernetes см. на [веб-сайте Kubernetes](https://kubernetes.io/docs/concepts/configuration/secret).

### <a name="namespace"></a>Пространство имен
[Пространства имен](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) Kubernetes помогают разделить один кластер между несколькими командами, проектами или приложениями. Пространства имен предоставляют область для ресурсов и имен. Они могут быть связаны с квотами ресурсов и политиками управления доступом.

Портал Azure предоставляет команды для создания самостоятельно размещенных ресурсов шлюза в пространстве имен **по умолчанию** . Это пространство имен создается автоматически, существует в каждом кластере и не может быть удалено.
Рассмотрите возможность [создания и развертывания](https://kubernetesbyexample.com/ns/) автономного шлюза в отдельном пространстве имен в рабочей среде.

### <a name="number-of-replicas"></a>Количество реплик
Минимальное число реплик, подходящих для рабочей среды, равно двум.

По умолчанию автономный шлюз развертывается с помощью [стратегии](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy)развертывания **роллингупдате** . Просмотрите значения по умолчанию и рассмотрите возможность явного задания полей [максунаваилабле](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable) и [макссурже](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge) , особенно если используется большое число реплик.

### <a name="container-resources"></a>Ресурсы контейнера
По умолчанию файл YAML, указанный в портал Azure, не указывает запросы ресурсов контейнера.

Невозможно надежно предсказать и рекомендовать объем ресурсов ЦП и памяти для каждого контейнера, а также количество реплик, необходимых для поддержки конкретной рабочей нагрузки. В игре есть множество факторов, например:

- Конкретное оборудование, на котором работает кластер.
- Присутствие и тип виртуализации.
- Число и скорость параллельных подключений клиентов.
- Частота запросов.
- Тип и количество настроенных политик.
- Размер полезных данных и необходимость буферизации полезных данных или потоковой передачи.
- Задержка серверной службы.

Рекомендуется настроить запросы ресурсов на два ядра и 2 гиб в качестве отправной точки. Выполнить нагрузочный тест и увеличить или уменьшить масштаб в зависимости от результатов.

### <a name="container-image-tag"></a>Тег образа контейнера
Файл YAML, указанный в портал Azure, использует **последний** тег. Этот тег всегда ссылается на последнюю версию образа контейнера автономного шлюза.

Рекомендуется использовать тег определенной версии в рабочей среде во избежание непреднамеренного обновления до более новой версии.

Вы можете [скачать полный список доступных тегов](https://mcr.microsoft.com/v2/azure-api-management/gateway/tags/list).

### <a name="dns-policy"></a>Политика DNS
Разрешение DNS-имен играет важную роль в разрешении на самостоятельно размещении шлюза для подключения к зависимостям в Azure и диспетчеризации вызовов API к серверным службам.

Файл YAML, указанный в портал Azure, применяет политику [клустерфирст](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy) по умолчанию. Эта политика приводит к тому, что запросы разрешения имен, не разрешенные DNS-кластером, перенаправляются на вышестоящий DNS-сервер, унаследованный от узла.

Дополнительные сведения о разрешении имен в Kubernetes см. на [веб-сайте Kubernetes](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service). Рассмотрите возможность настройки [политики DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy) или [конфигурации DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-config) в соответствии с настройками.

### <a name="external-traffic-policy"></a>Политика внешнего трафика
Файл YAML, указанный в поле портал Azure Sets `externalTrafficPolicy` объекта [службы](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core) `Local` . Это сохраняет IP-адрес вызывающей стороны (доступен в [контексте запроса](api-management-policy-expressions.md#ContextVariables)) и отключает балансировку нагрузки между узлами, устраняя сетевые переходы. Имейте в виду, что этот параметр может привести к асимметричному распределению трафика в развертываниях с неравным количеством модулей шлюзов на узел.

### <a name="custom-domain-names-and-ssl-certificates"></a>Пользовательские доменные имена и SSL-сертификаты

Если для конечных точек управления API используются пользовательские доменные имена, особенно если для конечной точки управления используется пользовательское доменное имя, может потребоваться обновить значение `config.service.endpoint` в файле **\<gateway-name\> . YAML** , чтобы заменить доменное имя по умолчанию именем пользовательского домена. Убедитесь, что конечная точка управления доступна из Pod шлюза, размещенного на собственном сервере, в кластере Kubernetes.

В этом случае, если SSL-сертификат, используемый конечной точкой управления, не подписывается известным сертификатом ЦС, необходимо убедиться, что сертификат ЦС является доверенным для модуля саморазмещенного шлюза.

### <a name="configuration-backup"></a>Резервная копия конфигурации
Сведения о работе с локальным шлюзом при наличии временного сбоя подключения Azure см. в статье [Общие сведения о шлюзе для размещения](self-hosted-gateway-overview.md#connectivity-to-azure).

Настройте локальный том хранилища для контейнера самостоятельно размещенного шлюза, чтобы он мог сохранить резервную копию последней скачанной конфигурации. Если подключение не работает, том хранилища может использовать резервную копию после перезагрузки. Путь подключения тома должен иметь значение <code>/apim/config</code> . См. пример на [GitHub](https://github.com/Azure/api-management-self-hosted-gateway/blob/master/examples/self-hosted-gateway-with-configuration-backup.yaml).
Дополнительные сведения о хранилище в Kubernetes см. на [веб-сайте Kubernetes](https://kubernetes.io/docs/concepts/storage/volumes/).

### <a name="local-logs-and-metrics"></a>Локальные журналы и метрики
Автономный шлюз отправляет данные телеметрии в [Azure Monitor](api-management-howto-use-azure-monitor.md) и [Azure Application Insights](api-management-howto-app-insights.md) в соответствии с параметрами конфигурации в связанной службе управления API.
Когда [Подключение к Azure](self-hosted-gateway-overview.md#connectivity-to-azure) временно теряется, поток телеметрии в Azure прерывается и данные теряются в течение сбоя.
Рассмотрите возможность [настройки локального мониторинга](how-to-configure-local-metrics-logs.md) , чтобы обеспечить возможность наблюдения за трафиком API и предотвращения потери данных телеметрии во время простоя подключения Azure.

## <a name="next-steps"></a>Дальнейшие действия

* Дополнительные сведения о самостоятельно размещенном шлюзе см. в разделе [Обзор самостоятельно размещенного](self-hosted-gateway-overview.md)шлюза.
