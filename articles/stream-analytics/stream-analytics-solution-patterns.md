---
title: Шаблоны решений Azure Stream Analytics
description: Узнайте о стандартных шаблонах решений для Azure Stream Analytics, таких как панели мониторинга, сообщения о событиях, хранилища данных, улучшение эталонных данных и мониторинг.
author: sidramadoss
ms.author: sidram
ms.service: stream-analytics
ms.topic: conceptual
ms.date: 06/21/2019
ms.openlocfilehash: 1bd3c1099344bd266d7e3bc153613daaecfb412a
ms.sourcegitcommit: 42a4d0e8fa84609bec0f6c241abe1c20036b9575
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/08/2021
ms.locfileid: "98020321"
---
# <a name="azure-stream-analytics-solution-patterns"></a>Шаблоны решений Azure Stream Analytics

Как и многие другие службы в Azure, Stream Analytics лучше использовать с другими службами для создания полного комплексного решения. В этой статье обсуждаются простые Azure Stream Analytics решения и различные архитектурные шаблоны. На этих шаблонах можно создавать более сложные решения. Шаблоны, описанные в этой статье, можно использовать в самых разнообразных сценариях. Примеры шаблонов для конкретных сценариев доступны в [архитектурах решений Azure](https://azure.microsoft.com/solutions/architecture/?product=stream-analytics).

## <a name="create-a-stream-analytics-job-to-power-real-time-dashboarding-experience"></a>Создание Stream Analytics задания для работы с панелями мониторинга в режиме реального времени

С помощью Azure Stream Analytics можно быстро разделять панели мониторинга и оповещения в режиме реального времени. Простое решение принимает события из концентраторов событий или центра Интернета вещей и передает [Power BI панель мониторинга с набором данных потоковой передачи](/power-bi/service-real-time-streaming). Дополнительные сведения см. в подробном руководстве [анализ данных о мошеннических вызовах с помощью Stream Analytics и визуализация результатов на панели мониторинга Power BI](stream-analytics-real-time-fraud-detection.md).

![ASA Power BI панель мониторинга](media/stream-analytics-solution-patterns/power-bi-dashboard.png)

Это решение можно создавать всего за несколько минут от портал Azure. Не существует обширного кода, и для выражения бизнес-логики используется язык SQL.

Этот шаблон решения обеспечивает наименьшую задержку от источника событий к панели мониторинга Power BI в браузере. Azure Stream Analytics является единственной службой Azure с этой встроенной возможностью.

## <a name="use-sql-for-dashboard"></a>Использование SQL для панели мониторинга

Панель мониторинга Power BI имеет низкую задержку, но ее нельзя использовать для создания полноценных Power BI отчетов. Общий шаблон отчетов — сначала вывести данные в базу данных SQL. Затем используйте соединитель SQL Power BI, чтобы запросить последние данные в SQL.

![Панель мониторинга SQL ASA](media/stream-analytics-solution-patterns/sql-dashboard.png)

Использование базы данных SQL обеспечивает большую гибкость, но за счет немного более высоких задержек. Это решение оптимально для заданий с требованиями к задержке более одной секунды. С помощью этого метода можно развернуть Power BI возможности для дальнейшего среза и разделения данных для отчетов, а также много других вариантов визуализации. Кроме того, вы получаете гибкость использования других решений панели мониторинга, например Tableau.

SQL не является хранилищем данных с высокой пропускной способностью. Максимальная пропускная способность базы данных SQL от Azure Stream Analytics в настоящее время составляет около 24 Мб/с. Если источники событий в решении получают данные с более высокой частотой, необходимо использовать логику обработки в Stream Analytics, чтобы уменьшить скорость вывода в SQL. Можно использовать такие методы, как фильтрация, агрегирование окон, сопоставление шаблонов с временными объединениями и аналитические функции. Скорость вывода в SQL может быть дополнительно оптимизирована с помощью методик, описанных в [Azure Stream Analytics выходных данных в службу SQL Azure](stream-analytics-sql-output-perf.md).

## <a name="incorporate-real-time-insights-into-your-application-with-event-messaging"></a>Внедрение в реальном времени аналитических сведений о приложениях с помощью сообщений о событиях

Вторым наиболее популярным использованием Stream Analytics является создание оповещений в режиме реального времени. В этом шаблоне решения бизнес-логика в Stream Analytics можно использовать для обнаружения [временных и пространственных шаблонов](stream-analytics-geospatial-functions.md) или [аномалий](stream-analytics-machine-learning-anomaly-detection.md), а затем создавать сигналы предупреждения. Однако в отличие от решения для панели мониторинга, где Stream Analytics использует Power BI в качестве предпочтительной конечной точки, можно использовать ряд промежуточных приемников данных. Эти приемники включают концентраторы событий, служебную шину и функции Azure. Вы, как построитель приложений, должны решить, какой приемник данных лучше подходит для вашего сценария.

Для создания предупреждений в существующем бизнес-процессе необходимо реализовать логику подчиненного потребителя событий. Так как вы можете реализовать пользовательскую логику в функциях Azure, функции Azure — это самый быстрый способ для выполнения этой интеграции. Руководство по использованию функции Azure в качестве выходных данных для задания Stream Analytics можно найти в статье [Запуск функций Azure из Azure Stream Analyticsных заданий](stream-analytics-with-azure-functions.md). Функции Azure также поддерживают различные типы уведомлений, включая текст и электронную почту. Приложение логики также может использоваться для такой интеграции с концентраторами событий между Stream Analytics и приложением логики.

![Приложение для обмена сообщениями о событиях ASA](media/stream-analytics-solution-patterns/event-messaging-app.png)

Концентраторы событий, с другой стороны, предлагают наиболее гибкую точку интеграции. Многие другие службы, такие как обозреватель данных Azure и аналитика временных рядов, могут использовать события из концентраторов событий. Службы могут быть подключены непосредственно к приемнику концентраторов событий из Azure Stream Analytics для завершения решения. Концентраторы событий — это также самый высокопроизводительный брокер сообщений о пропускной способности, доступный в Azure, для таких сценариев интеграции.

## <a name="dynamic-applications-and-websites"></a>Динамические приложения и веб-сайты

Вы можете создавать пользовательские визуализации в режиме реального времени, например панель мониторинга или визуализацию карт, с помощью Azure Stream Analytics и службы Azure SignalR. С помощью SignalR можно обновлять веб-клиенты и отображать динамическое содержимое в режиме реального времени.

![ASA, динамическое приложение](media/stream-analytics-solution-patterns/dynamic-app.png)

## <a name="incorporate-real-time-insights-into-your-application-through-data-stores"></a>Внедрение аналитических сведений в приложение в режиме реального времени с помощью хранилищ данных

Сегодня большинство веб-служб и веб-приложений используют шаблон "запрос — ответ" для обслуживания уровня представления. Шаблон "запрос — ответ" прост в создании и может легко масштабироваться с малым временем отклика с помощью фонового и масштабируемого хранилищ без отслеживания состояния, например Cosmos DB.

Большой объем данных часто создает узкие места производительности в системе на базе CRUD. [Шаблон решения "источники событий](/azure/architecture/patterns/event-sourcing) " используется для устранения узких мест производительности. Временные шаблоны и аналитические сведения также сложно и неэффективно извлекать из традиционного хранилища данных. Современные приложения, управляемые большими объемами данных, часто используют основанную на потоках архитектуру. Azure Stream Analytics, так как подсистема вычислений для данных Motion является линчпин в этой архитектуре.

![Приложение для источников событий ASA](media/stream-analytics-solution-patterns/event-sourcing-app.png)

В этом шаблоне решения события обрабатываются и объединяются в хранилища данных Azure Stream Analytics. Уровень приложения взаимодействует с хранилищами данных, используя традиционный шаблон "запрос-ответ". Из-за того, что Stream Analytics "возможность обрабатывать большое количество событий в режиме реального времени, приложение обладает высокой степенью масштабируемости без необходимости выполнять резервное копирование уровня хранилища данных. Уровень хранилища данных по сути является материализованным представлением в системе. [Azure Stream Analytics выходные данные в Azure Cosmos DB](stream-analytics-documentdb-output.md) описывает, как Cosmos DB используется в качестве выходных данных Stream Analytics.

В реальных приложениях, где логика обработки сложна и требуется обновить определенные части логики независимо, несколько Stream Analytics заданий могут составляться вместе с концентраторами событий в качестве посредника промежуточных событий.

![Приложение для источников сложных событий ASA](media/stream-analytics-solution-patterns/event-sourcing-app-complex.png)

Этот шаблон повышает устойчивость и управляемость системы. Однако несмотря на то, что Stream Analytics гарантируется только после обработки, существует небольшая вероятность того, что дублирование событий может полагаться на промежуточные концентраторы событий. Это важно для подчиненного Stream Analytics задания для дедупликации событий с помощью логических ключей в окне лукбакк. Дополнительные сведения о доставке событий см. в статье Справочник по [гарантиям доставки событий](/stream-analytics-query/event-delivery-guarantees-azure-stream-analytics) .

## <a name="use-reference-data-for-application-customization"></a>Использование эталонных данных для настройки приложения

Функция ссылочных данных Azure Stream Analytics предназначена специально для настройки конечных пользователей, таких как порог предупреждений, правила обработки и [геограницы](geospatial-scenarios.md). Уровень приложения может принимать изменения параметров и сохранять их в базе данных SQL. Задание Stream Analytics периодически запрашивает изменения из базы данных и делает параметры настройки доступными через соединение ссылочных данных. Дополнительные сведения об использовании ссылочных данных для настройки приложений см. в разделе [Ссылочные данные SQL](sql-reference-data.md) и [соединение ссылочных данных](/stream-analytics-query/reference-data-join-azure-stream-analytics).

Этот шаблон можно также использовать для реализации обработчика правил, в котором пороговые значения правил определяются на основе ссылочных данных. Дополнительные сведения о правилах см. [в разделе Обработка настраиваемых правил на основе пороговых значений в Azure Stream Analytics](stream-analytics-threshold-based-rules.md).

![Приложение ссылочных данных ASA](media/stream-analytics-solution-patterns/reference-data-app.png)

## <a name="add-machine-learning-to-your-real-time-insights"></a>Добавление Машинное обучение к ценным сведениям в режиме реального времени

Встроенная [модель обнаружения аномалий](stream-analytics-machine-learning-anomaly-detection.md) Azure Stream Analytics — удобный способ создания машинное обучение приложения в режиме реального времени. Более широкий спектр Машинное обучение потребностей см. в разделе [Azure Stream Analytics интегрируется со службой оценки машинное обучение Azure](stream-analytics-machine-learning-integration-tutorial.md).

Для опытных пользователей, желающих включить интерактивное обучение и оценку в один и тот же Stream Analytics конвейер, см. Этот пример с [линейной регрессией](stream-analytics-high-frequency-trading.md).

![ASA Машинное обучение приложение](media/stream-analytics-solution-patterns/machine-learning-app.png)

## <a name="real-time-data-warehousing"></a>Хранение данных в режиме реального времени

Другим распространенным шаблоном является хранение данных в реальном времени, также называемое хранилищем потоковых данных. Помимо событий, поступающих в концентраторы событий и центр Интернета вещей из приложения, [Azure Stream Analytics, выполняющиеся на IOT Edge](stream-analytics-edge.md) , можно использовать для выполнения очистки данных, уменьшения объема данных, а также для хранения и пересылки данных. Stream Analytics, выполняющиеся на IoT Edge, может корректно справляться с ограничением пропускной способности и проблемами подключения в системе. Stream Analytics может поддерживать пропускную способность 200 МБ/с во время записи в Azure синапсе Analytics.

![Хранилище данных ASA](media/stream-analytics-solution-patterns/data-warehousing.png)


## <a name="archiving-real-time-data-for-analytics"></a>Архивация данных в режиме реального времени для аналитики

Большинство операций обработки и анализа данных и анализа по-прежнему происходят в автономном режиме. Данные можно заархивировать с помощью Azure Stream Analytics в выходных форматах Azure Data Lake Store Gen2 и Parquet. Эта возможность устраняет трение, чтобы передавать данные непосредственно в Azure Data Lake Analytics, Azure Databricks и Azure HDInsight. Azure Stream Analytics в этом решении используется в качестве подсистемы ETL практически в режиме реального времени. Архивные данные можно исследовать в Data Lake с помощью различных механизмов вычислений.

> [!div class="mx-imgBorder"]
> ![Автономная аналитика ASA](media/stream-analytics-solution-patterns/offline-analytics.png)

## <a name="use-reference-data-for-enrichment"></a>Использование эталонных данных для обогащения

Для ядер ETL часто требуется обогащение данных. Azure Stream Analytics поддерживает обогащение данных с помощью [справочных данных](stream-analytics-use-reference-data.md) из базы данных SQL и хранилища BLOB-объектов Azure. Обогащение данных можно выполнить для размещения данных как в Azure Data Lake, так и в Azure синапсе Analytics.


![Автономная аналитика ASA с расширенными данными](media/stream-analytics-solution-patterns/offline-analytics-enriched.png)

## <a name="operationalize-insights-from-archived-data"></a>Эксплуатацию Insights из архивных данных

Если шаблон автономной аналитики сочетается с шаблоном приложения практически в реальном времени, можно создать цикл обратной связи. Цикл обратной связи позволяет приложению автоматически изменять закономерности в данных. Этот цикл обратной связи может быть простым изменением порогового значения для предупреждений или сложным, как переобучение Машинное обучение моделей. Ту же архитектуру решения можно применить как к заданиям ASA, выполняемым в облаке, так и к IoT Edge.

![Эксплуатация ASA Insights](media/stream-analytics-solution-patterns/insights-operationalization.png)

## <a name="how-to-monitor-asa-jobs"></a>Мониторинг заданий ASA

Задание Azure Stream Analytics может быть запущено 24/7 для непрерывной обработки входящих событий в режиме реального времени. Его Гарантия доступности важна для работоспособности всего приложения. Хотя Stream Analytics — единственная служба Streaming Analytics в отрасли, которая предлагает  [гарантию доступности 99,9%](https://azure.microsoft.com/support/legal/sla/stream-analytics/v1_0/), вы по-прежнему можете столкнуться с определенными уровнями времени простоя. В течение многих лет Stream Analytics предоставила метрики, журналы и состояния заданий для отражения работоспособности заданий. Все они отображаются с помощью Azure Monitor службы и могут быть экспортированы в OMS. Дополнительные сведения см. в статьях [изучение мониторинга Stream Analytics заданий и мониторинг запросов](stream-analytics-monitoring.md).

![Мониторинг ASA](media/stream-analytics-solution-patterns/monitoring.png)

Отслеживать можно с двух ключевых моментов.

- [Состояние сбоя задания](job-states.md)

    В первую очередь необходимо убедиться, что задание выполняется. Без задания в состоянии выполняется новые метрики или журналы не создаются. Задания могут переходить в состояние сбоя по различным причинам, включая высокий уровень использования SU (т. е. исчерпание ресурсов).

- [Метрики задержки водяного знака](https://azure.microsoft.com/blog/new-metric-in-azure-stream-analytics-tracks-latency-of-your-streaming-pipeline/)

    Эта метрика отражает, насколько далеко от конвейера обработки находится время в стене (в секундах). Некоторые задержки приводятся к последующей логике обработки. В результате наблюдение за повышением тенденций гораздо важнее, чем мониторинг абсолютного значения. Задержка устойчивого состояния должна быть решена конструкцией приложения, а не с помощью мониторинга или оповещений.

При сбое журналы действий и [журналы диагностики](stream-analytics-job-diagnostic-logs.md) лучше всего подходят для начала поиска ошибок.

## <a name="build-resilient-and-mission-critical-applications"></a>Создание отказоустойчивых и критически важных приложений

Независимо от гарантии соглашения об уровне обслуживания Azure Stream Analytics и того, насколько внимательны при выполнении приложения, происходит простои. Если приложение является критически важным, необходимо подготовиться к простоям, чтобы обеспечить корректное восстановление.

Для приложений с предупреждениями самое важное — обнаружить следующее оповещение. Вы можете выбрать перезапуск задания из текущего времени при восстановлении, пропуская предыдущие оповещения. Семантика времени запуска задания определяется первым выходным временем, а не первым входным временем. Входные данные перевернут назад в течение соответствующего времени, чтобы гарантировать, что первый выход в указанное время будет завершен и правильным. Вы не получите частичные статистические выражения и не запустите предупреждения в результате.

Вы также можете начать вывод с определенного промежутка времени в прошлом. Как концентраторы событий, так и политики хранения в центре Интернета вещей содержат разумный объем данных, позволяющий обрабатываться в прошлом. Компромисс заключается в том, насколько быстро можно перейти к текущему времени и начать создавать своевременные новые оповещения. Данные теряют свое значение с течением времени, поэтому важно быстро отслеживать текущее время. Существует два способа быстрого захвата:

- Подготавливайте дополнительные ресурсы (SU) при их перехвате.
- Перезагрузка с текущего времени.

Перезагрузка с текущего времени достаточно проста, и компромисс в процессе обработки выходит из разрыва. Перезагрузка такого способа может быть нормальной для сценариев создания предупреждений, но может быть проблематичной для сценариев панели мониторинга и не является начальным для сценариев архивирования и хранения данных.

Подготовка дополнительных ресурсов может ускорить процесс, но воздействие на скорость обработки сложно.

- Проверьте, что задание масштабируется до большего количества служб SUs. Не все запросы являются масштабируемыми. Необходимо убедиться, что запрос является [параллельным](stream-analytics-parallelization.md).

- Убедитесь, что в вышестоящем концентраторе событий или центре Интернета вещей достаточно разделов, которые можно добавить в дополнительные единицы пропускной способности (единиц пропускной способности) для масштабирования пропускной способности ввода. Помните, что каждый концентратор событий TU снижается с частотой вывода 2 МБ/с.

- Убедитесь, что в приемниках выходных данных подготовлено достаточно ресурсов (т. е. базы данных SQL Cosmos DB), поэтому они не регулируют всплеск в выходных данных, что иногда может привести к блокировке системы.

Самое важное, что следует ожидать изменения скорости обработки, протестировать эти сценарии перед переходом в рабочую среду и подготовиться к правильному масштабированию обработки во время восстановления после сбоя.

В экстремальном сценарии, когда все входящие события откладываются, [можно удалить все отложенные события](stream-analytics-time-handling.md) , если в задание было применено окно с поздним поступлением. Удаление событий может показаться mysteriousм поведением в начале; Однако, учитывая Stream Analytics является подсистемой обработки в режиме реального времени, она ждет, что входящие события будут близки к времени работы стены. Необходимо удалить события, нарушающие эти ограничения.

### <a name="lambda-architectures-or-backfill-process"></a>Лямбда-архитектура или процесс обратной обработки

К счастью, предыдущий шаблон архивирования данных можно использовать для корректной обработки этих поздних событий. Идея состоит в том, что задание архивации обрабатывает входящие события во время прибытия и архивирует события в нужный временной контейнер в большом двоичном объекте Azure или Azure Data Lake Store с их временем события. Не имеет значения, насколько поступило событие, оно никогда не будет удалено. Он всегда будет находиться в нужном временном контейнере. Во время восстановления можно выполнить повторную обработку архивных событий и попытаться получить результаты в выбранном хранилище. Это похоже на реализацию лямбда-шаблонов.

![ASA, обратная](media/stream-analytics-solution-patterns/back-fill.png)

Процесс обратной записи должен выполняться в автономной системе пакетной обработки, которая, скорее всего, имеет другую модель программирования, чем Azure Stream Analytics. Это означает, что необходимо повторно реализовать всю логику обработки.

Для получения дополнительной информации по-прежнему важно по крайней мере временно освободить ресурсы для приемников выходных данных, чтобы обрабатывать более высокую пропускную способность по сравнению с требованиями стабильной обработки состояния.

|Сценарии  |Перезапустить только с этого момента  |Перезапустить с момента последней остановки |Перезапустить из Now + обратная засыпку с архивированными событиями|
|---------|---------|---------|---------|
|**Панелей мониторинга**   |Создает зазор    |ОК для кратковременного сбоя    |Использовать для длительного сбоя |
|**Оповещение**   |Хорошо |ОК для кратковременного сбоя    |Не требуется |
|**Приложение для источников событий** |Хорошо |ОК для кратковременного сбоя    |Использовать для длительного сбоя |
|**Хранение данных**   |Потеря данных  |Хорошо |Не требуется |
|**Автономная аналитика**  |Потеря данных  |Хорошо |Не требуется|

## <a name="putting-it-all-together"></a>Сборка

Несложно представить, что все описанные выше шаблоны решений можно объединить в комплексную сквозную систему. Объединенная система может включать панели мониторинга, предупреждения, приложение для источников событий, хранилища данных и возможности автономной аналитики.

Ключом является проектирование системы в шаблонах с композицией, поэтому каждая подсистема может быть построена, протестирована, обновлена и восстановлена независимо друг от друга.

## <a name="next-steps"></a>Дальнейшие действия

Теперь вы узнали о различных шаблонах решений, использующих Azure Stream Analytics. Теперь вы можете вникнуть в детали и создать свое первое задание Stream Analytics:

* [Руководство по созданию задания Stream Analytics с помощью портала Azure](stream-analytics-quick-create-portal.md)
* [Руководство по созданию задания Stream Analytics с помощью Azure PowerShell](stream-analytics-quick-create-powershell.md)
* [Краткое руководство. Создание задания Stream Analytics с использованием инструментов Azure Stream Analytics для Visual Studio](stream-analytics-quick-create-vs.md).
