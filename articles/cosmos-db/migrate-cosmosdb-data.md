---
title: Перенос сотен терабайт данных в Azure Cosmos DB
description: В этом документе описывается, как можно перенести 100 терабайтов данных в Cosmos DB
author: SnehaGunda
ms.author: sngun
ms.service: cosmos-db
ms.subservice: cosmosdb-sql
ms.topic: how-to
ms.date: 10/23/2019
ms.openlocfilehash: b24ea79737c9e1f64abb7f62807352dbd9573695
ms.sourcegitcommit: 42a4d0e8fa84609bec0f6c241abe1c20036b9575
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 01/08/2021
ms.locfileid: "98018077"
---
# <a name="migrate-hundreds-of-terabytes-of-data-into-azure-cosmos-db"></a>Перенос сотен терабайт данных в Azure Cosmos DB 
[!INCLUDE[appliesto-all-apis](includes/appliesto-all-apis.md)]

Azure Cosmos DB может хранить терабайты данных. Вы можете выполнить крупномасштабную миграцию данных, чтобы переместить рабочую нагрузку в Azure Cosmos DB. В этой статье описаны проблемы, связанные с крупномасштабным перемещением данных в Azure Cosmos DB, а также представлено средство, которое помогает устранять проблемы и переносить данные в Azure Cosmos DB. В этом примере клиент использовал API SQL Cosmos DB.  

Перед переносом всей рабочей нагрузки в Azure Cosmos DB можно выполнить миграцию подмножества данных для проверки некоторых аспектов, таких как выбор ключа секции, производительность запросов и моделирование данных. После проверки подтверждения концепции можно переместить всю рабочую нагрузку в Azure Cosmos DB.  

## <a name="tools-for-data-migration"></a>Средства для переноса данных 

Azure Cosmos DB стратегии миграции в настоящее время различаются в зависимости от выбора API и размера данных. Для переноса наборов данных меньшего размера — для проверки модели, производительности запросов, выбора ключа секции и т. д. — Вы можете выбрать [средство переноса данных](import-data.md) или [соединитель Azure Cosmos DB фабрики данных Azure](../data-factory/connector-azure-cosmos-db.md). Если вы знакомы с Spark, можно также использовать [соединитель Azure Cosmos DB Spark](spark-connector.md) для переноса данных.

## <a name="challenges-for-large-scale-migrations"></a>Трудности при крупномасштабных миграциях 

Существующие средства для переноса данных в Azure Cosmos DB имеют некоторые ограничения, которые становятся особенно заметными при больших масштабах:

 * **Ограниченные возможности масштабирования**. для переноса терабайтов данных в Azure Cosmos DB как можно быстрее и для эффективного использования всей подготовленной пропускной способности клиенты миграции должны иметь возможность горизонтального масштабирования.  

* **Отсутствие отслеживания хода выполнения и галочки**: важно отслеживать ход выполнения миграции и при переносе больших наборов данных указывать на контрольные значения. В противном случае любая ошибка, возникающая во время миграции, приведет к прерыванию миграции, и вам придется начать процесс с нуля. Не пришлось бы перезапускать весь процесс миграции, когда 99% от него уже завершена.  

* **Недостаток очереди недоставленных сообщений**: в больших наборах данных в некоторых случаях могут возникнуть проблемы с частями исходных данных. Кроме того, могут возникать временные проблемы с клиентом или сетью. Любой из этих случаев не должен приводить к сбою всей миграции. Хотя большинство средств миграции обладают надежными возможностями повторных попыток, которые защищены от периодических проблем, они не всегда достаточно. Например, если размер меньше 0,01% документов исходных данных превышает 2 МБ, это приведет к сбою записи документа в Azure Cosmos DB. В идеале полезно, чтобы средство миграции сохраняло неудачные документы в другой очереди недоставленных сообщений, которая может быть обработана после миграции. 

Многие из этих ограничений исправляются для таких средств, как фабрика данных Azure, службы миграции данных Azure. 

## <a name="custom-tool-with-bulk-executor-library"></a>Пользовательский инструмент с библиотекой для выполнения операций с массовым исполнителем 

Проблемы, описанные в приведенном выше разделе, можно решить с помощью пользовательского инструмента, который можно легко масштабировать между несколькими экземплярами и устойчивым к временным сбоям. Кроме того, пользовательское средство может приостановить и возобновить миграцию на различных контрольных точках. Azure Cosmos DB уже предоставляет [библиотеку полного выполнителя](./bulk-executor-overview.md) , в которой содержатся некоторые из этих функций. Например, Библиотека массового исполнителя уже имеет функции для обработки временных ошибок и может масштабировать потоки на одном узле для использования примерно 500 K 000 на каждый узел. Библиотека небольшого выполнителя также разделяет исходный набор данных на микропакеты, которые работают независимо в виде контрольных точек.  

Настраиваемое средство использует библиотеку массового исполнителя и поддерживает масштабирование на нескольких клиентах и следит за ошибками в процессе приема. Для использования этого средства исходные данные должны быть разделены на отдельные файлы в Azure Data Lake Storage (ADLS), чтобы различные работники миграции могли выбрать каждый файл и принять их в Azure Cosmos DB. Пользовательский инструмент использует отдельную коллекцию, в которой хранятся метаданные о ходе миграции для каждого отдельного исходного файла в ADLS и отслеживаются все ошибки, связанные с ними.  

На следующем рисунке описан процесс миграции с помощью этого пользовательского инструмента. Средство выполняется на наборе виртуальных машин, и каждая виртуальная машина запрашивает сбор данных отслеживания в Azure Cosmos DB, чтобы получить аренду одной из секций с исходными данными. После этого исходный раздел данных считывается средством и принимается в Azure Cosmos DB с помощью библиотеки массового выполнителя. Далее будет обновлен сбор отслеживания для записи хода приема данных и обнаруженных ошибок. После обработки секции данных средство пытается запросить следующую доступную исходную секцию. Обработка следующей исходной секции будет продолжена до тех пор, пока не будут перенесены все данные. Исходный код для средства доступен в репозитории [массового приема Azure Cosmos DB](https://github.com/Azure-Samples/azure-cosmosdb-bulkingestion) .  

 
:::image type="content" source="./media/migrate-cosmosdb-data/migrationsetup.png" alt-text="Настройка средства миграции" border="false":::
 

 

Коллекция отслеживания содержит документы, как показано в следующем примере. Вы увидите такие документы по одному для каждой секции в исходных данных.  Каждый документ содержит метаданные для секции источника данных, такие как расположение, состояние миграции и ошибки (если они есть):  

```json
{ 
  "owner": "25812@bulkimporttest07", 
  "jsonStoreEntityImportResponse": { 
    "numberOfDocumentsReceived": 446688, 
    "isError": false, 
    "totalRequestUnitsConsumed": 3950252.2800000003, 
    "errorInfo": [], 
    "totalTimeTakenInSeconds": 188, 
    "numberOfDocumentsImported": 446688 
  }, 
  "storeType": "AZURE_BLOB", 
  "name": "sourceDataPartition", 
  "location": "sourceDataPartitionLocation", 
  "id": "sourceDataPartitionId", 
  "isInProgress": false, 
  "operation": "unpartitioned-writes", 
  "createDate": { 
    "seconds": 1561667225, 
    "nanos": 146000000 
  }, 
  "completeDate": { 
    "seconds": 1561667515, 
    "nanos": 180000000 
  }, 
  "isComplete": true 
} 
```
 

## <a name="prerequisites-for-data-migration"></a>Необходимые условия для переноса данных 

Прежде чем начать перенос данных, необходимо выполнить несколько предварительных условий.  

#### <a name="estimate-the-data-size"></a>Оцените размер данных:  

Исходный размер данных может не соответствовать размеру данных в Azure Cosmos DB. Чтобы проверить размер данных в Azure Cosmos DB, можно вставить несколько примеров документов из источника. В зависимости от размера образца документа можно оценить общий размер данных в Azure Cosmos DB после миграции. 

Например, если каждый документ после миграции в Azure Cosmos DB составляет около 1 КБ и в исходном наборе данных имеется около 60 000 000 000 документов, это означает, что предполагаемый размер в Azure Cosmos DB будет близок к 60 ТБ. 

 

#### <a name="pre-create-containers-with-enough-rus"></a>Предварительно создайте контейнеры с достаточным количеством получателей: 

Хотя Azure Cosmos DB масштабирует хранилище автоматически, не рекомендуется начинать с наименьшего размера контейнера. Меньшие контейнеры имеют меньшую доступность по пропускной способности, что означает, что миграция займет значительно больше времени. Вместо этого полезно создать контейнеры с окончательным размером данных (как показано на предыдущем шаге) и убедиться, что Рабочая нагрузка миграции полностью потребляет подготовленную пропускную способность.  

На предыдущем шаге. так как размер данных приблизительно составляет около 60 ТБ, для размещения всего набора данных требуется контейнер по крайней мере 2,4 м RUs.  

 

#### <a name="estimate-the-migration-speed"></a>Оцените скорость миграции: 

При условии, что Рабочая нагрузка миграции может потреблять всю подготовленную пропускную способность, подготовленная в течение всего времени предложит оценку скорости миграции. Продолжая предыдущий пример, необходимо 5 для записи документа размером 1 КБ в Azure Cosmos DB учетной записи API SQL.  2 400 000. RUs разрешает передачу 480 000 документов в секунду (или 480 Мб/с). Это означает, что полная миграция 60 ТБ займет 125 000 секунд или около 34 часов.  

Если вы хотите, чтобы миграция была выполнена в течение одного дня, необходимо увеличить подготовленную пропускную способность до 5 000 000 RUs. 

 

#### <a name="turn-off-the-indexing"></a>Отключите индексирование.  

Так как миграция должна быть выполнена как можно скорее, рекомендуется максимально сокращать время и досрочность, потраченную на создание индексов для каждого из полученных документов.  Azure Cosmos DB автоматически индексирует все свойства, стоит по меньшей мере выполнить индексирование до выбранных нескольких терминов или полностью отключить его в процессе миграции. Политику индексирования контейнера можно отключить, изменив параметр Indexingmode задано на None, как показано ниже.  

 
```
  { 
        "indexingMode": "none" 
  } 
```
 

После завершения миграции можно обновить индексирование.  

## <a name="migration-process"></a>Процесс миграции 

После завершения предварительных требований можно выполнить миграцию данных, выполнив следующие действия.  

1. Сначала импортируйте данные из источника в хранилище BLOB-объектов Azure. Чтобы увеличить скорость миграции, рекомендуется выполнять параллелизации между различными исходными секциями. Перед началом миграции исходный набор данных должен быть разбит на файлы размером около 200 МБ.   

2. Библиотеку массового исполнителя можно масштабировать, чтобы использовать 500 000 в одной клиентской виртуальной машине. Так как доступная пропускная способность составляет 5 000 000 RUs, виртуальные машины Ubuntu 16,04 (Standard_D32_v3) должны быть подготовлены в том же регионе, где находится база данных Azure Cosmos. Необходимо подготовить эти виртуальные машины с помощью средства миграции и файла параметров.  

3. Выполните шаг очереди на одной из клиентских виртуальных машин. На этом шаге создается коллекция отслеживания, которая сканирует контейнер ADLS и создает документ отслеживания хода выполнения для каждого из файлов секции исходного набора данных.  

4. Затем выполните шаг импорта на всех виртуальных машинах клиента. Каждый из клиентов может стать владельцем исходной секции и принимать свои данные в Azure Cosmos DB. После завершения работы и обновления его состояния в коллекции отслеживания клиенты смогут запрашивать следующую доступную исходную секцию в коллекции отслеживания.  

5. Этот процесс будет продолжен до тех пор, пока не будет получен весь набор исходных секций. После обработки всех исходных секций средство следует перезапустить в режиме исправления ошибок в той же коллекции отслеживания. Этот шаг необходим для обнаружения исходных секций, которые должны быть повторно обработаны из-за ошибок.  

6. Некоторые из этих ошибок могут быть вызваны неверными документами в исходных данных. Они должны быть идентифицированы и исправлены. Затем следует повторно запустить шаг импорта в секциях, в которых произошел сбой. 

После завершения миграции можно проверить, что количество документов в Azure Cosmos DB совпадает с количеством документов в базе данных-источнике. В этом примере общий размер Azure Cosmos DB исключается до 65 терабайт. После миграции можно выборочно включить индексирование, а также уменьшить уровень, требуемый для операций рабочей нагрузки.

## <a name="next-steps"></a>Дальнейшие действия

* Дополнительные сведения см. в примерах приложений, использующих библиотеку небольшого выполнителя в [.NET](bulk-executor-dot-net.md) и [Java](bulk-executor-java.md). 
* Библиотека небольшого Исполнительного исполнителя интегрирована в соединитель Cosmos DB Spark. Дополнительные сведения см. в статье о [соединителе Azure Cosmos DB Spark](spark-connector.md) .  
* Обратитесь к группе разработчиков Azure Cosmos DB, открыв запрос в службу поддержки в подтипе проблем "Общие рекомендации" и "крупные (ТБ +) миграции" для получения дополнительной справки о крупномасштабных миграциях.