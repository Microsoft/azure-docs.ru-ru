---
title: Управление библиотеками Python для Apache Spark
description: Узнайте, как добавлять библиотеки Python и управлять ими, используемыми Apache Spark в Azure синапсе Analytics.
services: synapse-analytics
author: midesa
ms.service: synapse-analytics
ms.topic: conceptual
ms.date: 02/26/2020
ms.author: midesa
ms.reviewer: jrasnick
ms.subservice: spark
ms.openlocfilehash: 2d6ac02402414f096a46fec0340c3074d8e1784a
ms.sourcegitcommit: 772eb9c6684dd4864e0ba507945a83e48b8c16f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "104586647"
---
# <a name="manage-python-libraries-for-apache-spark-in-azure-synapse-analytics"></a>Управление библиотеками Python для Apache Spark в Azure синапсе Analytics

Библиотеки предоставляют многократно используемый код, который может потребоваться включить в программы или проекты. 

Может потребоваться обновить среду пула Apache Spark без сервера по различным причинам. Например, может оказаться, что:
- Одна из основных зависимостей только что выпустила новую версию.
- необходим дополнительный пакет для обучения модели машинного обучения или подготовки данных.
- Вы нашли более эффективный пакет и больше не нуждаются в старом пакете.

Чтобы сделать код, разработанный сторонним приложением, доступным для приложений, можно установить библиотеку на одном из бессерверных Apache Sparkных пулов или сеансов записных книжек. В этой статье мы рассмотрим, как можно управлять библиотеками Python в бессерверном Apache Spark пуле.

## <a name="default-installation"></a>Установка по умолчанию
Apache Spark в Azure синапсе Analytics содержит полный набор библиотек для общих задач проектирования данных, подготовки данных, машинного обучения и визуализации данных. Список полных библиотек можно найти по адресу [Apache Spark поддержки версий](apache-spark-version-support.md). 

При запуске экземпляра Spark эти библиотеки будут автоматически добавлены. Дополнительные Python и пользовательские пакеты, созданные на языке, можно добавить на уровне пула Spark и сеанса.

## <a name="pool-libraries"></a>Библиотеки пулов
Определив библиотеки Python, которые вы хотите использовать для приложения Spark, можно установить их в пул Spark. Библиотеки уровня пула доступны для всех записных книжек и заданий, выполняющихся в пуле.

Существует два основных способа установки библиотеки в кластере.
-  Установите библиотеку рабочей области, которая была отправлена в виде пакета рабочей области.
-  Укажите *requirements.txt* или *Conda среду. yml* для установки пакетов из репозиториев, таких как PyPI, Conda-подделка и т. д.

> [!IMPORTANT]
> - Если устанавливаемый пакет является большим или занимает много времени, это повлияет на время запуска экземпляра Spark.
> - Изменение версии PySpark, Python, Scala/Java, .NET или Spark не поддерживается.
> - Установка пакетов из внешних репозиториев, таких как PyPI, Conda-подделывать, или каналов Conda по умолчанию не поддерживается в рабочих областях с поддержкой DEP.

### <a name="install-python-packages"></a>Установка пакетов Python
Пакеты Python можно установить из таких репозиториев, как PyPI, и Conda-Forge, предоставив файл спецификации среды. 

#### <a name="environment-specification-formats"></a>Форматы спецификаций среды

##### <a name="pip-requirementstxt"></a>PIP requirements.txt
Для обновления среды можно использовать файл *requirements.txt* (выходные данные `pip freeze` команды). При обновлении пула пакеты, перечисленные в этом файле, загружаются из PyPI. Затем все зависимые компоненты кэшируются и сохраняются для последующего повторного использования пула. 

В следующем фрагменте кода показан формат файла требований. Имя пакета PyPI отображается вместе с точной версией. Этот файл соответствует формату, описанному в справочной документации по [замораживанию PIP](https://pip.pypa.io/en/stable/reference/pip_freeze/) . 

В этом примере закрепляется конкретная версия. 
```
absl-py==0.7.0
adal==1.2.1
alabaster==0.7.10
```
##### <a name="yml-format-preview"></a>Формат YML (Предварительная версия)
Кроме того, можно предоставить файл *Environment. yml* для обновления среды пула. Пакеты, перечисленные в этом файле, загружаются из каналов Conda по умолчанию, Conda-подделывать и PyPI. Можно указать другие каналы или удалить каналы по умолчанию с помощью параметров конфигурации.

В этом примере указаны каналы и зависимости Conda/PyPI. 

```
name: stats2
channels:
- defaults
dependencies:
- bokeh
- numpy
- pip:
  - matplotlib
  - koalas==1.7.0
```
Дополнительные сведения о создании среды из этого файла Environment. yml см. в разделе [Создание среды из файла Environment. yml](https://docs.conda.io/projects/conda/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually).

#### <a name="update-python-packages"></a>Обновление пакетов Python
Определив файл спецификации среды или набор библиотек, которые вы хотите установить в пуле Spark, можно обновить библиотеки пула Spark, перейдя в Azure синапсе Studio или портал Azure. Здесь можно указать спецификацию среды и выбрать библиотеки рабочей области для установки. 

После сохранения изменений задание Spark запустит установку и кэширует полученную среду для последующего повторного использования. После завершения задания новые задания Spark или сеансы записных книжек будут использовать обновленные библиотеки пула. 

##### <a name="manage-packages-from-azure-synapse-studio-or-azure-portal"></a>Управление пакетами из Azure синапсе Studio или портал Azure
Для управления библиотеками пула Spark можно использовать Azure синапсе Studio или портал Azure. 

Чтобы обновить или добавить библиотеки в пул Spark, выполните следующие действия.
1. Перейдите к рабочей области Azure синапсе Analytics из портал Azure.

    При обновлении с **портал Azure**:

    - В разделе **ресурсов синапсе** выберите вкладку **Пулы Apache Spark** и выберите пул Spark из списка.
     
    - Выберите **пакеты** из раздела **Параметры** пула Spark.
  
    ![Снимок экрана, посвященный кнопке "отправить файл конфигурации среды".](./media/apache-spark-azure-portal-add-libraries/apache-spark-add-library-azure.png "Добавление библиотек Python")
   
    При обновлении из **синапсе Studio**:
    - Выберите **Управление** на главной панели навигации, а затем выберите **Пулы Apache Spark**.

    - Выберите раздел **пакеты** для определенного пула Spark.
    ![Снимок экрана, посвященный параметру конфигурации "Отправить среду" из студии.](./media/apache-spark-azure-portal-add-libraries/studio-update-libraries.png "Добавление библиотек Python из студии")
   
2. Отправьте файл конфигурации среды с помощью средства выбора файлов в разделе "  **пакеты** " страницы.
3. Вы также можете выбрать дополнительные **пакеты рабочей области** , чтобы добавить их в пул. 
4. После сохранения изменений будет запущено системное задание для установки и кэширования указанных библиотек. Этот процесс позволяет сократить общее время запуска сеанса. 
5. После успешного завершения задания все новые сеансы будут получать обновленные библиотеки пула.

> [!IMPORTANT]
> Выбрав параметр для **принудительного создания новых параметров**, вы завершите все текущие сеансы для выбранного пула Spark. После завершения сеансов необходимо будет дождаться перезапуска пула. 
>
> Если этот параметр не установлен, необходимо дождаться завершения текущего сеанса Spark или завершить его вручную. После завершения сеанса необходимо разрешить перезапуск пула.


##### <a name="track-installation-progress-preview"></a>Отслеживать ход установки (Предварительная версия)
Зарезервированное системное задание Spark инициируется каждый раз при обновлении пула с помощью нового набора библиотек. Это задание Spark помогает отслеживать состояние установки библиотеки. Если установка завершается сбоем из-за конфликтов библиотек или других проблем, пул Spark вернется к предыдущему состоянию или по умолчанию. 

Кроме того, пользователи также могут проверить журналы установки, чтобы выявить конфликты зависимостей, или узнать, какие библиотеки были установлены во время обновления пула.

Для просмотра этих журналов выполните следующие действия.
1. Перейдите в список приложений Spark на вкладке " **монитор** ". 
2. Выберите Задание системного приложения Spark, соответствующее обновлению пула. Эти системные задания выполняются под названием *системресерведжоб-либрариманажемент* .
   ![Снимок экрана, на котором выделяется зарезервированное системное задание библиотеки.](./media/apache-spark-azure-portal-add-libraries/system-reserved-library-job.png "Просмотр задания системной библиотеки")
3. Переключитесь на просмотр журналов **драйверов** и **stdout** . 
4. В результатах вы увидите журналы, связанные с установкой зависимостей.
    ![Снимок экрана, на котором показаны результаты задания зарезервированной системной библиотеки.](./media/apache-spark-azure-portal-add-libraries/system-reserved-library-job-results.png "Просмотр хода выполнения задания системной библиотеки")

## <a name="install-wheel-files"></a>Установка колесных файлов
Файлы колес Python — это распространенный способ упаковки библиотек Python. В Azure синапсе Analytics пользователи могут загружать файлы колеса в известное расположение Azure Data Lake Storage учетной записи или отправлять с помощью интерфейса пакетов рабочей области Azure синапсе.

### <a name="workspace-packages-preview"></a>Пакеты рабочей области (Предварительная версия)
Пакеты рабочей области могут быть файлами пользовательского или закрытого колеса. Вы можете передать эти пакеты в рабочую область и позже назначить их конкретному пулу Spark.

Чтобы добавить пакеты рабочей области, выполните следующие действия.
1. Перейдите на вкладку **Управление**  >  **пакетами рабочей области** .
2. Отправьте файлы колеса с помощью средства выбора файлов.
3. После отправки файлов в рабочую область Azure синапсе эти пакеты можно добавить в заданный пул Apache Spark.

![Снимок экрана, на котором выделены пакеты рабочей области.](./media/apache-spark-azure-portal-add-libraries/studio-add-workspace-package.png "Просмотр пакетов рабочей области")

>[!WARNING]
>- В Azure синапсе пул Apache Spark может использовать пользовательские библиотеки, которые либо передаются как пакеты рабочей области, либо передаются в известный Azure Data Lake Storage путь. Однако оба этих варианта нельзя одновременно использовать в одном пуле Apache Spark. Если пакеты предоставляются с помощью обоих методов, будут установлены только файлы колеса, указанные в списке пакетов рабочей области. 
>
>- После того как пакеты рабочей области (Предварительная версия) используются для установки пакетов в заданном пуле Apache Spark, существует ограничение на то, что вы больше не можете указывать пакеты, используя путь к учетной записи хранения в том же пуле.  

### <a name="storage-account"></a>Учетная запись хранения
Пользовательские пакеты Wheel можно установить в пул Apache Spark, отгружая все файлы колеса в учетную запись Azure Data Lake Storage (Gen2), связанную с рабочей областью синапсе. 

Файлы должны быть отправлены по следующему пути в контейнере учетной записи хранения по умолчанию: 

```
abfss://<file_system>@<account_name>.dfs.core.windows.net/synapse/workspaces/<workspace_name>/sparkpools/<pool_name>/libraries/python/
```

>[!WARNING]
> В некоторых случаях может потребоваться создать путь к файлу на основе приведенной выше структуры, если она еще не существует. Например, может потребоваться добавить ```python``` папку в ```libraries``` папку, если она еще не существует.

> [!IMPORTANT]
> Чтобы установить пользовательские библиотеки с помощью метода хранилища Azure Data Lake, необходимо иметь разрешения на доступ к **данным BLOB-объектов хранилища** или **владельцу данных BLOB-объекта хранилища** в первичной учетной записи хранения Gen2, связанной с рабочей областью Azure синапсе Analytics.


## <a name="session-scoped-packages-preview"></a>Пакеты с областью действия сеанса (Предварительная версия)
Помимо пакетов уровня пула, можно также указать библиотеки с областью действия сеанса в начале сеанса записной книжки.  Библиотеки с областью действия сеанса позволяют задавать и использовать пользовательские среды Python в сеансе записной книжки. 

При использовании библиотек с областью действия сеанса важно учитывать следующие моменты.
   - При установке библиотек с областью действия сеанса доступ к указанным библиотекам будет иметь только Текущая записная книжка. 
   - Эти библиотеки не влияют на другие сеансы или задания, использующие один и тот же пул Spark. 
   - Эти библиотеки устанавливаются поверх базовой среды выполнения и библиотек уровня пула. 
   - Библиотеки записных книжек будут иметь наивысший приоритет.

Чтобы указать пакеты с областью действия сеанса, выполните следующие действия.
1.  Перейдите к выбранному пулу Spark и убедитесь, что вы включили библиотеки уровня сеанса.  Чтобы включить этот параметр, перейдите на вкладку **Управление**  >    >  **пакетами** пула Apache Spark. ![включите пакеты сеансов.](./media/apache-spark-azure-portal-add-libraries/enable-session-packages.png "Включить пакеты сеансов")
2.  После применения параметра можно открыть записную книжку и выбрать **Настройка** >  **пакетов** сеансов.
  ![Укажите пакеты сеанса.](./media/apache-spark-azure-portal-add-libraries/update-session-notebook.png "Обновление конфигурации сеанса")
3.  Здесь можно отправить файл Conda *Environment. yml* для установки или обновления пакетов в рамках сеанса. После запуска сеанса будут установлены указанные библиотеки. После завершения сеанса эти библиотеки больше не будут доступны в том виде, в каком они относятся к вашему сеансу.

## <a name="verify-installed-libraries"></a>Проверка установленных библиотек
Чтобы проверить, установлены ли правильные версии правильных библиотек из PyPI, выполните следующий код:
```python
import pkg_resources
for d in pkg_resources.working_set:
     print(d)
```
В некоторых случаях для просмотра версий пакета из Conda может потребоваться проверка версии пакета по отдельности.

## <a name="next-steps"></a>Дальнейшие действия
- Просмотр библиотек по умолчанию: [Поддержка версий Apache Spark](apache-spark-version-support.md)
- Устранение ошибок при установке библиотеки: [Устранение неполадок библиотеки](apache-spark-troubleshoot-library-errors.md)
- Создание частного канала Conda с помощью учетной записи Azure Data Lake Storage: [частные каналы Conda](./spark/../apache-spark-custom-conda-channel.md)
