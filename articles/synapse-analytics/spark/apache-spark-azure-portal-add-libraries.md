---
title: Управление библиотекой
description: Узнайте, как добавлять библиотеки, используемые Apache Spark, в Azure синапсе Analytics и управлять ими.
services: synapse-analytics
author: midesa
ms.service: synapse-analytics
ms.topic: conceptual
ms.date: 03/01/2020
ms.author: midesa
ms.reviewer: jrasnick
ms.subservice: spark
ms.openlocfilehash: 955d7f8c2d2ce5ea126d4cce67b0e4e55152ac72
ms.sourcegitcommit: c27a20b278f2ac758447418ea4c8c61e27927d6a
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/03/2021
ms.locfileid: "101695096"
---
# <a name="manage-libraries-for-apache-spark-in-azure-synapse-analytics"></a>Управление библиотеками для Apache Spark в Azure синапсе Analytics
Библиотеки предоставляют многократно используемый код, который может потребоваться включить в программы или проекты. 

Может потребоваться обновить среду пула Apache Spark без сервера по различным причинам. Например, может оказаться, что:
- Одна из основных зависимостей выпустила новую версию.
- необходим дополнительный пакет для обучения модели машинного обучения или подготовки данных.
- Вы нашли более эффективный пакет и больше не нуждаются в старом пакете.
- Ваша команда создала пользовательский пакет, который должен быть доступен в пуле Apache Spark.

Чтобы сделать код, разработанный сторонним приложением, доступным для приложений, можно установить библиотеку на одном из бессерверных Apache Sparkных пулов или сеансов записных книжек.
  
## <a name="default-installation"></a>Установка по умолчанию
Apache Spark в Azure синапсе Analytics содержит полную установку Анакондас, а также дополнительные библиотеки. Список полных библиотек можно найти по адресу [Apache Spark поддержки версий](apache-spark-version-support.md). 

При запуске экземпляра Spark эти библиотеки будут автоматически добавлены. Дополнительные пакеты можно добавить на уровне пула Spark или на уровне сеанса.

## <a name="workspace-packages"></a>Пакеты рабочей области
При разработке пользовательских приложений или моделей команда может разрабатывать различные артефакты кода, такие как колесо или JAR-файлы, для упаковки кода. 

В синапсе пакеты рабочей области могут быть пользовательскими или частными или JAR-файлами. Вы можете передать эти пакеты в рабочую область и позже назначить их конкретному пулу Spark. После назначения эти пакеты рабочей области будут автоматически установлены во всех сеансах пула Spark.

Дополнительные сведения об управлении библиотеками рабочих областей см. в следующих руководствах.
- [Пакеты рабочей области Python: ](./apache-spark-manage-python-packages.md#Install-wheel-files) Отправьте файлы колеса Python в качестве пакета рабочей области, а затем добавьте эти пакеты в отдельные независимые от сервера пулы Apache Spark.
- [Пакеты рабочей области Scala/Java (Предварительная версия): ](./apache-spark-manage-scala-packages.md#Workspace-packages) Отправьте JAR-файлы Scala и Java в качестве пакета рабочей области, а затем добавьте эти пакеты в отдельные независимые от сервера пулы Apache Spark.

## <a name="pool-management"></a>Управление пулом
В некоторых случаях может потребоваться стандартизировать набор пакетов, используемых в заданном пуле Apache Spark. Эта стандартизация может быть полезной, если одни и те же пакеты часто устанавливаются несколькими пользователями в группе. 

С помощью возможностей управления пулом Azure синапсе Analytics можно настроить набор библиотек по умолчанию, которые необходимо установить на заданном сервере Apache Sparkном пуле. Эти библиотеки устанавливаются поверх [базовой среды выполнения](./apache-spark-version-support.md). 

В настоящее время Управление пулом поддерживается только для Python. Для Python пулы синапсе Spark используют Conda для установки и управления зависимостями пакетов Python. При указании библиотек уровня пула теперь можно указать requirements.txt или среду. yml. Этот файл конфигурации среды используется при каждом создании экземпляра Spark из этого пула Spark. 

Дополнительные сведения об этих возможностях см. в документации по [управлению пулом в Python](./apache-spark-manage-python-packages.md#Pool-libraries).

> [!IMPORTANT]
> - Если устанавливаемый пакет является большим или занимает много времени, это повлияет на время запуска экземпляра Spark.
> - Изменение версии PySpark, Python, Scala/Java, .NET или Spark не поддерживается.
> - Установка пакетов из PyPI не поддерживается в рабочих областях с поддержкой DEP.

## <a name="session-scoped-packages"></a>Пакеты с областью действия сеанса
Часто при интерактивном анализе данных или машинном обучении может оказаться, что вы хотите испытать новые пакеты или потребуются пакеты, которые еще не доступны в пуле Apache Spark. Вместо обновления конфигурации пула пользователи теперь могут использовать пакеты уровня сеанса для добавления, управления и обновления зависимостей сеансов.

Пакеты с областью действия сеанса позволяют пользователям определять зависимости пакетов в начале сеанса. При установке пакета уровня сеанса только текущий сеанс имеет доступ к указанным пакетам. В результате эти пакеты уровня сеанса не будут влиять на другие сеансы или задания, использующие один и тот же пул Apache Spark. Кроме того, эти библиотеки устанавливаются поверх базовой среды выполнения и пакетов уровня пула. 

Дополнительные сведения об управлении пакетами с областью действия сеанса см. в следующих руководствах:
- [Пакеты сеанса Python (Предварительная версия):](./apache-spark-manage-python-packages.md#Session-scoped-libraries-(preview)) В начале сеанса укажите *среду Conda. yml* , чтобы установить дополнительные пакеты Python из популярных репозиториев. 
- [Пакеты сеансов Scala/Java: ](./apache-spark-manage-scala-packages.md#Workspace-packages) В начале сеанса укажите список JAR-файлов для установки с помощью ```%%configure``` .

## <a name="next-steps"></a>Дальнейшие действия
- Просмотр библиотек по умолчанию: [Поддержка версий Apache Spark](apache-spark-version-support.md)
