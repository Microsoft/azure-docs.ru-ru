---
title: Устранение ошибок при установке библиотеки
description: В этом учебнике содержатся общие сведения об устранении ошибок при установке библиотеки.
services: synapse-analytics
author: midesa
ms.author: midesa
ms.service: synapse-analytics
ms.subservice: spark
ms.topic: conceptual
ms.date: 01/04/2021
ms.openlocfilehash: 60ea97ea2df271f867febec3fa0f0826a18dbbbf
ms.sourcegitcommit: d4734bc680ea221ea80fdea67859d6d32241aefc
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/14/2021
ms.locfileid: "100417951"
---
# <a name="troubleshoot-library-installation-errors"></a>Устранение ошибок при установке библиотеки 
Чтобы сделать код, разработанный сторонним приложением, доступным для приложений, можно установить библиотеку на одном из несерверных пулов Apache Spark. Пакеты, перечисленные в файле requirements.txt, загружаются из PyPi во время запуска пула. Этот файл требований используется при каждом создании экземпляра Spark из пула Spark. После установки библиотеки для пула Spark она будет доступна для всех сеансов, использующих тот же пул. 

В некоторых случаях библиотека, которую вы пытаетесь установить, может не отображаться в пуле Apache Spark. Такая ситуация часто возникает при возникновении ошибки в предоставленных requirements.txt или указанных библиотеках. При возникновении ошибки в процессе установки библиотеки Apache Spark пул вернется к библиотекам, указанным в базовой среде выполнения синапсе.

Цель этого документа — предоставить распространенные проблемы и упростить отладку ошибок при установке библиотеки.

## <a name="force-update-your-apache-spark-pool"></a>Принудительное обновление пула Apache Spark
При обновлении библиотек в пуле Apache Spark эти изменения будут отобраны после перезапуска пула. Если у вас есть активные задания, эти задания будут по-прежнему выполняться в исходной версии пула Spark.

Можно принудительно применить изменения, выбрав параметр для **принудительного применения новых параметров**. Этот параметр приведет к завершению всех текущих сеансов для выбранного пула Spark. После завершения сеансов необходимо будет дождаться перезапуска пула. 

![Добавление библиотек Python](./media/apache-spark-azure-portal-add-libraries/update-libraries.png "Добавление библиотек Python")

## <a name="validate-your-permissions"></a>Проверка разрешений
Чтобы установить и обновить библиотеки, необходимо иметь разрешения на доступ к **данным BLOB-объектов хранилища** или **владельцу данных BLOB-объекта** хранилища в учетной записи хранения PRIMARY Azure Data Lake Storage 2-го поколения, связанной с рабочей областью Azure синапсе Analytics.

Чтобы проверить наличие этих разрешений, можно выполнить следующий код:

```python
from pyspark.sql.types import StructType,StructField, StringType, IntegerType
data2 = [("James","Smith","Joe","4355","M",3000),
    ("Michael","Rose","Edward","40288","F",4000)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])
 
df = spark.createDataFrame(data=data2,schema=schema)

df.write.csv("abfss://<<ENTER NAME OF FILE SYSTEM>>@<<ENTER NAME OF PRIMARY STORAGE ACCOUNT>>.dfs.core.windows.net/validate_permissions.csv")

```
Если появляется сообщение об ошибке, вероятно, отсутствуют необходимые разрешения. Сведения о том, как получить необходимые разрешения, см. в следующем документе: [назначение хранилищу данных BLOB-объекта хранилища или разрешение владельца данных BLOB-объекта хранилища](https://docs.microsoft.com/azure/storage/common/storage-auth-aad-rbac-portal#assign-an-azure-built-in-role).

Кроме того, если вы используете конвейер, MSI рабочей области должен иметь разрешения владельца данных BLOB-объекта хранилища или права доступа к BLOB-объектам хранилища. Чтобы узнать, как предоставить удостоверению рабочей области это разрешение, перейдите [по адресу предоставление разрешений управляемому удостоверению рабочей области](../security/how-to-grant-workspace-managed-identity-permissions.md).

## <a name="check-the-requirements-file"></a>Проверка файла требований
Для обновления виртуальной среды можно использовать файл ***requirements.txt*** (выходные данные команды PIP Freeze). Этот файл соответствует формату, описанному в справочной документации по [замораживанию PIP](https://pip.pypa.io/en/stable/reference/pip_freeze/) .

Важно отметить следующие ограничения.
   -  Имя пакета PyPI должно указываться вместе с точной версией. 
   -  Содержимое файла требований не должно содержать лишних пустых строк или символов. 
   -  [Среда выполнения синапсе](apache-spark-version-support.md) включает набор библиотек, предварительно устанавливаемых на каждый серверный пул Apache Spark. Пакеты, которые предварительно установлены в базовой среде выполнения, не могут быть понижены. Пакеты можно добавлять или обновлять только.
   -  Изменение версии PySpark, Python, Scala/Java, .NET или Spark не поддерживается.

В следующем фрагменте кода показан требуемый формат файла требований.

```
absl-py==0.7.0
adal==1.2.1
alabaster==0.7.10
```

## <a name="validate-wheel-files"></a>Проверка файлов колеса
Синапсе пулы бессерверных Apache Spark основаны на дистрибутиве Linux. При скачивании и установке файлов колеса непосредственно из PyPI необходимо выбрать версию, созданную на платформе Linux и работающую в той же версии Python, что и пул Spark.

>[!IMPORTANT]
>Пользовательские пакеты можно добавлять или изменять между сеансами. Тем не менее для просмотра обновленного пакета необходимо подождать, пока пул и сеанс перезапускаются.

## <a name="check-for-dependency-conflicts"></a>Проверка конфликтов зависимостей
 В общем случае управление разрешениями зависимости Python может оказаться непростой задачей. Чтобы упростить отладку конфликтов зависимостей, можно создать собственную виртуальную среду на основе среды выполнения синапсе и проверить изменения.

Повторное создание среды и проверка обновлений:
 1. [Скачайте](https://github.com/Azure-Samples/Synapse/blob/main/Spark/Python/base_environment.yml) шаблон, чтобы локально воссоздать среду выполнения синапсе. Между шаблоном и реальной средой синапсе могут возникнуть небольшие различия.
   
 2. Создайте виртуальную среду, выполнив [следующие инструкции](https://docs.conda.io/projects/conda/latest/user-guide/tasks/manage-environments.html). Эта среда позволяет создать изолированную установку Python с указанным списком библиотек. 
    
    ```
    conda myenv create -f environment.yml
    conda activate myenv
    ```
   
 3. Используйте ``pip install -r <provide your req.txt file>`` для обновления виртуальной среды с помощью указанных пакетов. Если установка приводит к ошибке, то может возникнуть конфликт между предварительно установленной средой выполнения синапсе и указанным в предоставленном файле требований. Эти конфликты зависимостей должны быть разрешены для получения обновленных библиотек в бессерверном пуле Apache Spark.

## <a name="next-steps"></a>Следующие шаги
- Просмотр библиотек по умолчанию: [Поддержка версий Apache Spark](apache-spark-version-support.md)

