---
title: Создание конвейера данных с помощью API сборщика данных
description: API сборщика данных HTTP в Azure Monitor можете использовать для добавления данных POST JSON в рабочую область Log Analytics из любого клиента, который может вызывать REST API. В статье содержится описание автоматической отправки данных, которые хранятся в файлах.
ms.topic: conceptual
author: bwren
ms.author: bwren
ms.date: 08/09/2018
ms.openlocfilehash: ab2e9c691f17b8f0891ecbc82ff42cd3529a1328
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/20/2021
ms.locfileid: "102031197"
---
# <a name="create-a-data-pipeline-with-the-data-collector-api"></a>Создание конвейера данных с помощью API сборщика данных

[API сборщика данных Azure Monitor](data-collector-api.md) позволяет импортировать любые пользовательские данные журнала в рабочую область Log Analytics в Azure Monitor. Единственным требованием к данным является то, что они должны находиться в формате JSON и быть разделены пакетами размером не более 30 МБ. Это полностью гибкий механизм, который можно подключить многими способами: от передачи данных непосредственно из приложения до одноразовых отправок adhoc. В этой статье будут описаны некоторые отправные точки для общего сценария, например, необходимость отправлять данные, хранящиеся в файлах, на регулярной, автоматизированной основе. Хотя представленный здесь конвейер не будет наиболее эффективным или оптимизированным, он призван служить отправной точкой для создания собственного рабочего конвейера.

[!INCLUDE [azure-monitor-log-analytics-rebrand](../../../includes/azure-monitor-log-analytics-rebrand.md)]

## <a name="example-problem"></a>Пример проблемы
В оставшейся части этой статьи будут рассмотрены данные просмотра страниц в Application Insights. В нашем гипотетическом сценарии мы хотим сопоставить географические сведения, собираемые по умолчанию пакетом SDK для Application Insights, с пользовательскими данными, содержащими совокупность каждой страны или региона в мире, с целью определения того, где следует потратить на большинство рыночных долларов. 

Для этой цели используются такие публичные источники данных, как [World Population Prospects 2017](https://esa.un.org/unpd/wpp/) (Мировые демографические перспективы 2017). Данные будут выглядеть следующим образом.

![Пример простой схемы](./media/create-pipeline-datacollector-api/example-simple-schema-01.png)

В данном примере предполагается, что будет выполнена отправка нового файла, который содержит информацию за последний год, с данными, как только они станут доступными.

## <a name="general-design"></a>Общий макет
Чтобы сконструировать конвейер, используется стандартный тип данных формата ETL. Архитектура будет выглядеть следующим образом.

![Архитектура конвейера сбора данных](./media/create-pipeline-datacollector-api/data-pipeline-dataflow-architecture.png)

Дополнительные сведения о создании данных см. в статье [Передача данных изображений в облако с помощью службы хранилища Azure](../../storage/blobs/storage-upload-process-images.md). Здесь описан процесс выбора потока, после того как новый файл был отправлен в большой двоичный объект. Порядок действий следующий.

1. Процесс определит, что новые данные были отправлены.  В приведенном примере используется [приложение логики Azure](../../logic-apps/logic-apps-overview.md), обладающее триггером обнаружения новых данных, которые были отправлены в большой двоичный объект.

2. Процессор считывает эти новые данные и преобразует их в формат JSON, требуемый для Azure Monitor. [Функции Azure](../../azure-functions/functions-overview.md), используемые в этом примере, являются легким и экономичным способом выполнения кода обработки. Эта функция запускается одним и тем же логическим приложением, которое было использовано для обнаружения новых данных.

3. Как только объект JSON станет доступен, он отправляется в Azure Monitor. Одно и то же приложение логики использует встроенное действие сборщика данных Log Analytics для отправки данных в Azure Monitor.

Хотя подробные установки хранилища BLOB-объектов, приложения логики или Функции Azure не описаны в этой статье, подробные инструкции к ним доступны на страницах указанных продуктов.

Чтобы отследить конвейер, ми используем средство Application Insights для отслеживания Функции Azure (дополнительные сведения см. [здесь](../../azure-functions/functions-monitoring.md)) и службу Azure Monitor для отслеживания приложения логики (дополнительные сведения см. [здесь](../../logic-apps/monitor-logic-apps-log-analytics.md)). 

## <a name="setting-up-the-pipeline"></a>Установка конвейера
Чтобы задать конвейер, сначала необходимо убедиться, что контейнер больших двоичных объектов создан и настроен. Аналогичным образом следует убедиться, что была создана рабочая область Log Analytics, в которою планируется отправлять данные.

## <a name="ingesting-json-data"></a>Прием данных JSON
Прием данных в формате JSON с помощью приложений логики является обычным процессом, а поскольку преобразование здесь не требуется, то весь конвейер можно добавить в одно приложение логики. После настройки контейнера больших двоичных объектов и рабочей области Log Analytics создайте новое приложение логики и настройте его следующим образом.

![Примеры рабочего процесса приложений логики](./media/create-pipeline-datacollector-api/logic-apps-workflow-example-01.png)

Сохраните приложение логики и перейдите к его проверке.

## <a name="ingesting-xml-csv-or-other-formats-of-data"></a>Прием данных форматов XML, CSV или других форматов
На данный момент в Logic Apps отсутствуют встроенные возможности преобразования форматов XML, CSV или других типов форматов в JSON. Таким образом, для выполнения этого преобразования требуется использование сторонних средств. В этой статье были использованы возможности бессерверных вычислений Функций Azure, которые является очень легким и дешевым способом это реализовать. 

В этом примере был проведен анализ CSV-файла. При этом любой другой тип файла можно обрабатывать аналогичным образом. Для этого следует изменить десериализующую часть функции Azure, что позволит отобразить правильную логику указанного типа данных.

1.  При появлении запроса создайте новую функцию Azure, используя Функцию среды выполнения версии 1 и потребление.  В качестве отправной точки C#, которая настраивает привязки по мере необходимости, выберите шаблон **Триггер HTTP**. 
2.  Чтобы создать файл **project.json** и вставить следующий код из используемого пакета NuGet, используйте вкладку **Просмотреть файлы**, находящуюся в правой области панели.

    ![Пример проекта Функций Azure](./media/create-pipeline-datacollector-api/functions-example-project-01.png)
    
    ``` JSON
    {
      "frameworks": {
        "net46":{
          "dependencies": {
            "CsvHelper": "7.1.1",
            "Newtonsoft.Json": "11.0.2"
          }  
        }  
       }  
     }  
    ```

3. В правой области панели перейдите к файлу **run.csx** и замените код, который находится там по умолчанию, следующим кодом. 

    >[!NOTE]
    >Для текущего проекта требуется заменить модель записи (класс PopulationRecord) собственной схемой данных.
    >

    ```   
    using System.Net;
    using Newtonsoft.Json;
    using CsvHelper;
    
    class PopulationRecord
    {
        public String Location { get; set; }
        public int Time { get; set; }
        public long Population { get; set; }
    }

    public static async Task<HttpResponseMessage> Run(HttpRequestMessage req, TraceWriter log)
    {
        string filePath = await req.Content.ReadAsStringAsync(); //get the CSV URI being passed from Logic App
        string response = "";

        //get a stream from blob
        WebClient wc = new WebClient();
        Stream s = wc.OpenRead(filePath);         

        //read the stream
        using (var sr = new StreamReader(s))
        {
            var csvReader = new CsvReader(sr);
    
            var records = csvReader.GetRecords<PopulationRecord>(); //deserialize the CSV stream as an IEnumerable
    
            response = JsonConvert.SerializeObject(records); //serialize the IEnumerable back into JSON
        }    

        return response == null
            ? req.CreateResponse(HttpStatusCode.BadRequest, "There was an issue getting data")
            : req.CreateResponse(HttpStatusCode.OK, response);
     }  
    ```

4. Сохраните функцию.
5. Чтобы убедиться, что код работает правильно, проверьте функцию. Перейдите на вкладку **Тест** в правой области панели, настроив тестирование следующим образом. Ссылку следует поместить в текстовое поле **Тело запроса** большого двоичного объекта, в котором находится пример данных. Щелкнув **Выполнить**, в поле **Вывод** должны появиться выходные данные JSON.

    ![Проверка кода в приложении-функции](./media/create-pipeline-datacollector-api/functions-test-01.png)

Теперь необходимо вернуться назад и изменить приложение логики, которое создавалось ранее, чтобы включить данные, полученные и преобразованные в формат JSON.  Используйте "Конструктор представлений", чтобы настроить и сохранить приложение логики следующим образом.

![Полный пример рабочего процесса приложения логики](./media/create-pipeline-datacollector-api/logic-apps-workflow-example-02.png)

## <a name="testing-the-pipeline"></a>Тестирование конвейера
Теперь новый файл может быть отправлен в ранее установленный большой двоичный объект, а его мониторинг можно выполнять с помощью приложения логики. Вскоре появится новый экземпляр запущенного приложения логики, который вызывает Функцию Azure, а затем успешно отправляет данные в Azure Monitor. 

>[!NOTE]
>При первой отправке нового типа данных их отображение в Azure Monitor может занять до 30 минут.


## <a name="correlating-with-other-data-in-log-analytics-and-application-insights"></a>Сопоставление с другими данными в Log Analytics и Application Insights
Чтобы завершить задачу по сопоставлению данных просмотра страницы Application Insights с данными о населении, которые были приняты от пользовательского источника данных, необходимо выполнить следующий запрос из окна аналитики Application Insights или из рабочей области Log Analytics.

``` KQL
app("fabrikamprod").pageViews
| summarize numUsers = count() by client_CountryOrRegion
| join kind=leftouter (
   workspace("customdatademo").Population_CL
) on $left.client_CountryOrRegion == $right.Location_s
| project client_CountryOrRegion, numUsers, Population_d
```

В выходных данных должны отображаться два присоединенных источника данных.  

![Пример сопоставления отсоединенных данных в результатах поиска](./media/create-pipeline-datacollector-api/correlating-disjoined-data-example-01.png)

## <a name="suggested-improvements-for-a-production-pipeline"></a>Предлагаемые улучшения для рабочего конвейера
В этой статье представлен рабочий прототип, логика которого может быть применена в истинном решении промышленного качества. Для такого решения промышленного качества рекомендуется использовать следующие улучшения.

* Добавьте обработку ошибок и логику повторных попыток в приложение логики и функцию Azure.
* Добавьте логику, чтобы гарантировать, что не превышено ограничение на 30 МБ за один вызов API приема данных от Log Analytics. При необходимости разделите данные на сегменты меньшего размера.
* Настройте политику очистки облачного хранилища. Если нет необходимости хранить необработанные данные с архивными целями, то после успешной отправки данных в рабочую область Log Analytics нет никаких оснований продолжать их хранение. 
* Включите мониторинг по всему конвейеру. При необходимости можно добавить точки трассировки и предупреждения.
* Используйте системы управления версиями для управления кодом функции и приложения логики.
* Убедитесь, что соблюдена правильная политика управления изменениями, которая позволяет изменять схемы, функции и приложения логики соответствующим образом.
* Отправляя несколько разных типов данных, разделите их на отдельные папки в контейнере больших двоичных объектов и создайте логику, чтобы разделить логику на основе типа данных. 


## <a name="next-steps"></a>Дальнейшие действия
Дополнительные сведения о том, как записать данные из любого клиента REST API в рабочую область Log Analytics, см. в статье [Отправка данных в Log Analytics с помощью API сборщика данных HTTP (общедоступная предварительная версия)](data-collector-api.md).
