---
title: Управление использованием и затратами для журналов Azure Monitor
description: Узнайте, как изменить ценовой план, а также управлять объемом данных и политикой хранения рабочей области Log Analytics в Azure Monitor.
services: azure-monitor
documentationcenter: azure-monitor
author: bwren
manager: carmonm
editor: ''
ms.assetid: ''
ms.service: azure-monitor
ms.workload: na
ms.tgt_pltfrm: na
ms.topic: conceptual
ms.date: 03/03/2021
ms.author: bwren
ms.openlocfilehash: 5048364aed1eea8d0c32d9134a4ba5a22d28b989
ms.sourcegitcommit: f0a3ee8ff77ee89f83b69bc30cb87caa80f1e724
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/26/2021
ms.locfileid: "105560460"
---
# <a name="manage-usage-and-costs-with-azure-monitor-logs"></a>Управление использованием и затратами с помощью журналов Azure Monitor    

> [!NOTE]
> В этой статье описывается, как оценить и контролировать затраты на журналы Azure Monitor. В связанной статье [Мониторинг использования и ожидаемых затрат в Azure Monitor](../usage-estimated-costs.md) описано, как можно просмотреть сведения об использовании и оценить затраты по нескольким функциям мониторинга Azure для разных моделей ценообразования. Все цены и затраты в этой статье приведены только для примера. 

Журналы Azure Monitor предназначены для масштабирования и поддержки сбора, индексирования и хранения больших объемов данных в день из любого источника в вашей организации или в развертывании в Azure.  Хотя это может быть определяющим фактором для вашей организации, в конечном счете основной является экономическая эффективность. Поэтому важно понять, что стоимость рабочей области Log Analytics зависит не только от объема собранных данных, но также от выбранного плана и от того, как долго будут храниться данные из ваших подключенных источников.  

В этой статье мы рассмотрим, как можно в упреждающем режиме отслеживать увеличение объема полученных и хранимых данных, а также определять ограничения для контроля этих связанных затрат. 

## <a name="pricing-model"></a>Модель ценообразования

По умолчанию для Log Analytics используется модель оплаты **по мере использования**, основанная на объеме полученных данных. При необходимости эта модель может использоваться для более длительного хранения данных. Объем данных измеряется как размер данных, которые будут храниться в ГБ (10 ^ 9 байт). Каждая рабочая область Log Analytics оплачивается как отдельная служба и включается в счет за подписку Azure. Объем принятых данных может быть значительным в зависимости от следующих факторов: 

  - число включенных решений по управлению и их конфигурация;
  - число отслеживаемых виртуальных машин;
  - тип данных, собираемых с каждой отслеживаемой виртуальной машины. 
  
Кроме модели с оплатой по мере использования служба Log Analytics имеет уровни **резервирования мощности**, которые позволяют сэкономить до 25 % по сравнению с оплатой по мере использования. Цены на резервирование мощности дают возможность приобрести резервирование, начиная со 100 ГБ в день. При превышении уровня использования резервирования будет взиматься плата по мере использования. Ценовые категории на резервирование мощности имеют обязательный 31-дневный период. В течение обязательного периода вы можете перейти на более высокий уровень резервирования мощности (что повторно активирует действие обязательного 31-дневного периода), но вы не сможете вернуться к оплате по мере использования или на более низкий уровень резервирования мощности до тех пор, пока не завершится обязательный период. Выставление счетов за уровни резервирования мощности выполняется ежедневно. Дополнительные сведения об оплате по мере использования и резервирования мощности Log Analytics см. [здесь](https://azure.microsoft.com/pricing/details/monitor/). 

Для всех ценовых категорий размер данных события вычисляется на основе строкового представления свойств, которые хранятся в Log Analytics для этого события, независимо от того, отправляются ли данные из агента или добавляются в процессе приема. Сюда входят все [Настраиваемые поля](custom-fields.md) , добавляемые в виде данных, которые собираются, а затем сохраняются в log Analytics. Некоторые свойства, общие для всех типов данных, включая некоторые [log Analytics стандартные свойства](./log-standard-columns.md), исключаются при вычислении размера события. Сюда входят `_ResourceId` , `_SubscriptionId` , `_ItemId` , `_IsBillable` `_BilledSize` и `Type` . Все остальные свойства, хранящиеся в Log Analytics, включаются в вычисление размера события. Некоторые типы данных свободны от расходов на прием данных, например AzureActivity, пульс и типы использования. Чтобы определить, было ли событие исключено из выставления счетов для приема данных, можно использовать `_IsBillable` свойство, как показано [ниже](#data-volume-for-specific-events). Сведения об использовании выводятся в ГБ (1,0 E9 байт). 

Обратите внимание, что некоторые решения, такие как [Центр безопасности Azure](https://azure.microsoft.com/pricing/details/security-center/), [Azure Sentinel](https://azure.microsoft.com/pricing/details/azure-sentinel/) и [Управление конфигурацией](https://azure.microsoft.com/pricing/details/automation/), имеют собственные модели ценообразования. 

### <a name="log-analytics-dedicated-clusters"></a>Выделенные кластеры Log Analytics

Выделенные кластеры Log Analytics представляют собой коллекции рабочих областей в едином управляемом кластере Azure Data Explorer для поддержки таких расширенных сценариев, как [управляемые клиентом ключи](customer-managed-keys.md).  Log Analytics выделенных кластерах используется модель ценообразования резервирования мощностей, которая должна быть настроена как минимум 1000 ГБ в день. Этот уровень емкости имеет скидку 25% по сравнению с ценами оплаты по мере использования. При превышении уровня использования резервирования будет взиматься плата по мере использования. При резервировании мощности кластера после увеличения уровня резервирования действует обязательный 31-дневный период. В течение обязательного периода уровень резервирования мощности невозможно уменьшить, но его можно увеличить в любое время. Если рабочие области связаны с кластером, выставление счетов за получение данных для этих рабочих областей выполняется на уровне кластера с использованием настроенного уровня резервирования емкости. Изучите дополнительные сведения о [создании кластеров Log Analytics](customer-managed-keys.md#create-cluster) и [их связывании с рабочими областями](customer-managed-keys.md#link-workspace-to-cluster). Сведения о ценах на резервирование емкости доступны на [странице цен на Azure Monitor]( https://azure.microsoft.com/pricing/details/monitor/).  

Уровень резервирования емкости кластера настраивается программным путем с Azure Resource Manager с помощью `Capacity` параметра в разделе `Sku` . `Capacity` указывается в ГБ и может иметь значения 1000 ГБ/день или более с шагом приращения 100 ГБ/день. Это подробно описано в [Azure Monitor ключе, управляемом клиентом](customer-managed-keys.md#create-cluster). Если для кластера требуется резервирование свыше 2000 Гб в день, свяжитесь с нами по адресу [LAIngestionRate@microsoft.com](mailto:LAIngestionRate@microsoft.com).

Существует два режима выставления счетов за использование в кластере. Они могут быть заданы `billingType` параметром при [настройке кластера](customer-managed-keys.md#customer-managed-key-operations). Доступны два режима. 

1. **Кластер**. в данном случае (по умолчанию) выставление счетов за принимаемые данные выполняется на уровне кластера. Полученные объемы данных из каждой рабочей области, связанной с кластером, суммируются для вычисления ежедневного счета за кластер. Обратите внимание, что распределения по отдельным узлам из [Центра безопасности Azure](../../security-center/index.yml) применяются на уровне рабочей области перед этой статистической обработкой агрегированных данных во всех рабочих областях в кластере. 

2. **Рабочие области**. затраты на резервирование ресурсов для кластера настраиваются пропорционально рабочим областям в кластере (после учета для каждого узла из [центра безопасности Azure](../../security-center/index.yml) для каждой рабочей области). Если общий объем данных, принимаемый в рабочую область в течение дня, меньше, чем резервирование емкости, то каждая рабочая область оплачивается по полученным данным в соответствии с коэффициентом резервирования емкости за ГБ за счет выставления счета за долю резервирования емкости, а неиспользуемая часть резервирования емкости оплачивается ресурсом кластера. Если общий объем данных, принимаемый в рабочую область в течение дня, больше, чем резервирование емкости, то каждая рабочая область оплачивается за долю резервирования емкости на основе доли полученных данных в день, а каждая рабочая область — для доли полученных данных над резервированием емкости. Плата за ресурс кластера не взимается, если общий объем данных, принимаемый в рабочую область в течение дня, превышает резервирование емкости.

В параметрах выставления счетов кластера за хранение данных взимается плата за рабочую область. Обратите внимание, что плата за кластер начисляется при его создании независимо от того, связаны ли рабочие области с кластером. Кроме того, обратите внимание, что рабочие области, связанные с кластером, больше не имеют ценовой категории.

## <a name="estimating-the-costs-to-manage-your-environment"></a>Оценка затрат на управление средой 

Если вы еще не используете журналы Azure Monitor, для оценки затрат на Log Analytics можно использовать [калькулятор цен Azure Monitor](https://azure.microsoft.com/pricing/calculator/?service=monitor). Сначала введите в поле поиска Azure Monitor и щелкните на отобразившуюся плитку Azure Monitor. Прокрутите страницу вниз до Azure Monitor и выберите Log Analytics в раскрывающемся списке типов.  Здесь можно ввести количество виртуальных машин и ГБ данных, которые вы собираетесь получать из каждой виртуальной машины. Обычно в месяц от стандартной виртуальной машины Azure принимаются 1–3 ГБ данных. Если вы уже оцениваете журналы Azure Monitor, вы можете использовать статистику данных из собственной среды. Ниже приведены сведения о том, как определить [число отслеживаемых виртуальных машин](#understanding-nodes-sending-data) и [объем данных, принимаемых рабочей областью](#understanding-ingested-data-volume). 

## <a name="understand-your-usage-and-estimate-costs"></a>Просмотр сведений об использовании и оценка затрат

Если вы используете журналы Azure Monitor, то легко узнать о возможных затратах, основываясь на последних шаблонах использования. Вы можете просматривать и анализировать использование данных с помощью сведений, приведенных в статье **Мониторинг использования и ожидаемых затрат в Azure Monitor**. Здесь вы узнаете, какой объем данных был собран каждым решением, какой объем данных сохраняется, и получите оценку затрат на основании объема полученных данных и дополнительных объемов хранения сверх включенных в тариф.

:::image type="content" source="media/manage-cost-storage/usage-estimated-cost-dashboard-01.png" alt-text="Использование и ожидаемые затраты":::

Чтобы подробнее изучить данные, щелкните значок в верхней правой части любой диаграммы на странице **Использование и ожидаемые затраты**. Теперь вы можете доработать этот запрос, чтобы получить дополнительные сведения о потреблении.  

:::image type="content" source="media/manage-cost-storage/logs.png" alt-text="Представление журналов":::

На странице **Использование и оценка затрат** можно просмотреть сведения о томах данных за месяц. Сюда входят все оплачиваемые данные, полученные и сохраняемые в рабочей области Log Analytics.  
 
Оплата за использование Log Analytics добавляется в счет Azure. Дополнительную информацию о счете за подписку на Azure можно просмотреть в разделе выставления счетов портала Azure или на [Портале управления счетами и подписками Azure](https://account.windowsazure.com/Subscriptions).  

## <a name="viewing-log-analytics-usage-on-your-azure-bill"></a>Просмотр использования Log Analytics в счете Azure 

Azure предоставляет большое количество полезных функций в концентраторе [управления затратами и выставления счетов Azure](../../cost-management-billing/costs/quick-acm-cost-analysis.md?toc=%2fazure%2fbilling%2fTOC.json). Например, функция "Анализ затрат" позволяет просматривать затраты на ресурсы Azure. Сначала добавьте фильтр по "типу ресурса" (в Microsoft. operationalinsights/Workspace для Log Analytics и Microsoft. operationalinsights/кластер для Log Analytics кластеров) позволит вам относиться к Log Analytics затратам. Затем для параметра "Группировать по" выберите "Категория учета" или "Счетчик".  Обратите внимание, что за использование других служб, таких как Центр безопасности Azure и Azure Sentinel, также взимается плата в соответствии с ресурсами рабочей области Log Analytics. Чтобы увидеть сопоставление с именем службы, можно выбрать табличное представление вместо диаграммы. 

Чтобы получить более полное представление об использовании, вы можете [скачать сведения об использовании на портале Azure](../../cost-management-billing/manage/download-azure-invoice-daily-usage-date.md#download-usage-in-azure-portal). В скачанной электронной таблице вы можете просмотреть сведения об использовании каждого ресурса Azure (например, рабочей области Log Analytics) в день. В этой таблице Excel использование рабочих областей Log Analytics можно найти с помощью первой фильтрации в столбце "Категория счетчика", чтобы отобразить "Log Analytics", "Insight and Analytics (используется в некоторых устаревших ценовых категориях) и" Azure Monitor "(используется в ценовых категориях резервирования емкости), а затем добавляя фильтр в столбец" идентификатор экземпляра ", который содержит" Рабочая область "или" содержит кластер "(последний включает использование Log Analytics кластера). Данные об использовании отображаются в столбце "Использованное количество", а единица измерения для каждой записи отображается в столбце "Единица измерения".  Дополнительные сведения о счете за использование Microsoft Azure см. в [этой статье](../../cost-management-billing/understand/review-individual-bill.md). 

## <a name="changing-pricing-tier"></a>Изменение ценовой категории.

Чтобы изменить ценовую категорию Log Analytics рабочей области, выполните следующие действия: 

1. На портале Azure откройте **Использование и ожидаемые затраты** из рабочей области. Вы увидите список всех ценовых категорий, доступных для этой рабочей области.

2. Ознакомьтесь с расчетной стоимостью каждой ценовой категории. Эта оценка основана на последних 31 дне использования. Поэтому в ней предполагается, что последние 31 день отображают ваше типичное использование. В приведенном ниже примере можно увидеть, что оплата по мере использования этой рабочей области на основе шаблонов данных за последние 31 день будет меньше (#1) по сравнению с уровнем резервирования мощности 100 ГБ/день (#2).  

:::image type="content" source="media/manage-cost-storage/pricing-tier-estimated-costs.png" alt-text="Ценовые категории":::
    
3. После просмотра оценки затрат на основе последних 31 дня использования щелкните **Выбрать**, если вы решили изменить ценовую категорию.  

Вы также можете [задать ценовую категорию через Azure Resource Manager](./resource-manager-workspace.md) с помощью параметра `sku` (`pricingTier` в шаблоне Azure Resource Manager). 

## <a name="legacy-pricing-tiers"></a>Устаревшие ценовые категории

Подписки, в которых были рабочая область Log Analytics или ресурс Application Insights до 2 апреля 2018, или которые связаны с Соглашением Enterprise, действие которого началось до 1 февраля 2019, будут по-прежнему иметь возможность использовать прежние ценовые категории: **Бесплатный**, **Автономный (за ГБ)** и **За узел (OMS)** .  Для рабочих областей в ценовой категории "Бесплатный" будет действовать ограничение ежедневного приема данных на 500 МБ (за исключением типов данных безопасности, собранных [Центром безопасности Azure](../../security-center/index.yml)), а срок хранения данных ограничен 7 днями. Ценовая категория "Бесплатный" предназначена только для ознакомительных целей. Для рабочих областей в ценовой категории "Автономный" или "За узел" предусмотрен настраиваемый период хранения данных от 30 до 730 дней.

За использование в автономной ценовой категории оплачивается объем полученных данных. Он сообщается в службе **log Analytics** , а счетчик называется "анализируемые данные". 

Плата в ценовой категории "За узел" взимается за каждую отслеживаемую виртуальную машину (узел) по часам. Рабочей области для каждого отслеживаемого узла выделяется 500 МБ данных в день, за которые плата не взимается. Это выделение вычисляется с почасовой гранулярностью и статистически вычисляется на уровне рабочей области каждый день. За полученные данные, превышающие агрегированное ежедневное распределение данных, плата взимается за ГБ как за избыточный объем данных. Обратите внимание, что в счете для Log Analytics будет использоваться услуга **Аналитика**, если рабочая область имеет ценовую категорию "За узел". Сведения об использовании отображаются на трех метрах:

1. Узел. используется для количества отслеживаемых узлов (ВМ) в единицах времени node * months.
2. Превышение объема данных на узел: это количество ГБ данных, полученных за пределами агрегированного распределения данных.
3. Данные, включаемые на узел: это объем принимаемых данных, охваченных агрегированным распределением данных. Этот счетчик также используется, если Рабочая область находится во всех ценовых категориях для отображения объема данных, покрываемых центром безопасности Azure.

> [!TIP]
> Если в вашей рабочей области есть доступ к ценовой категории **За узел**, но вы хотите узнать, будет ли стоимость меньше в ценовой категории с оплатой по мере использования, вы можете [выполнить приведенный ниже запрос](#evaluating-the-legacy-per-node-pricing-tier), чтобы получить рекомендацию. 

В рабочих областях, созданных до апреля 2016 г., также можно получить доступ к исходным ценовым категориям **Стандартный** и **Премиум** с фиксированным сроком хранения данных 30 и 365 дней соответственно. Новые рабочие области нельзя создать в ценовой категории **Стандартный** или **Премиум**. Если рабочая область перемещена из этих уровней, ее нельзя вернуть обратно. Счетчики приема данных для этих устаревших уровней называются "анализируемыми данными".

Существует также несколько расширений функциональности между использованием устаревших уровней Log Analytics и тем, как выставляется счет за использование [Центра безопасности Azure](../../security-center/index.yml). 

1. Если рабочая область относится к категории "Стандартный" или "Премиум", то для Центра безопасности Azure будет выставляться счет только за прием данных Log Analytics, а не за узел.
2. Если рабочая область относится к устаревшей категории "За узел", плата за Центр безопасности Azure будет взиматься с помощью текущей [модели ценообразования на основе узла Центра безопасности Azure](https://azure.microsoft.com/pricing/details/security-center/). 
3. В других ценовых категориях (включая резервирования мощностей), если Центр безопасности Azure был включен до 19 июня 2017 г., плата будет взиматься только за прием данных Log Analytics. В противном случае плата за использование Центра безопасности Azure будет взиматься с помощью текущей модели ценообразования на основе узла Центра безопасности Azure.

Дополнительные сведения об ограничениях ценовых категорий см. в [подписке Azure и ограничениях, квотах и ограничениях службы](../../azure-resource-manager/management/azure-subscription-service-limits.md#log-analytics-workspaces).

Ни одна из устаревших ценовых категорий не имеет ценообразования на основе региональных стандартов.  

> [!NOTE]
> Чтобы использовать права, полученные при покупке подписки OMS E1, OMS E2 или настройки OMS для System Center, выберите ценовой уровень Log Analytics *за узле*.

## <a name="log-analytics-and-security-center"></a>Log Analytics и центр безопасности

Выставление счетов в [центре безопасности Azure](../../security-center/index.yml) тесно связано с log Analytics выставления счетов. Центр безопасности обеспечивает выделение 500 МБ/с на узел/день для следующего подмножества [типов данных безопасности](/azure/azure-monitor/reference/tables/tables-category#security) (Виндовсевент, Секуритялерт, SecurityBaseline, SecurityBaselineSummary, Секуритидетектион, SecurityEvent, брандмауэра Windows, МалиЦиаусипкоммуникатион, Линуксаудитлог, SysmonEvent, состояние защиты) и типов данных Update и UpdateSummary, если решение Управление обновлениями не работает в рабочей области или целевом решении. Если Рабочая область находится в ценовой категории устаревший на узел, центр безопасности и Log Analytics распределения объединяются и применяются совместно ко всем оплачиваемым принимаемым данным.  

## <a name="change-the-data-retention-period"></a>Изменение срока хранения данных

В следующих шагах описана настройка периода хранения данных журнала в рабочей области. Срок хранения данных на уровне рабочей области можно настроить от 30 до 730 дней (2 года) для всех рабочих областей, если они не используют устаревшую ценовую категорию "бесплатный". Для хранения отдельных типов данных можно задать как минимум 4 дня. Дополнительные сведения о ценах на длительное хранение данных см. [здесь](https://azure.microsoft.com/pricing/details/monitor/).  Чтобы хранить данные дольше 730 дней, рассмотрите возможность использования [log Analytics экспорта данных рабочей области](logs-data-export.md).

### <a name="workspace-level-default-retention"></a>Хранение по умолчанию на уровне рабочей области

Чтобы задать срок хранения данных по умолчанию для рабочей области, выполните следующие действия: 
 
1. На портале Azure в рабочей области выберите **Использование и ожидаемые затраты** на левой панели.
2. В верхней части страницы **Usage and estimated costs** (Использование и ожидаемые затраты) щелкните **Хранение данных**.
3. В области воспользуйтесь ползунком, чтобы увеличить или уменьшить количество дней, а затем нажмите кнопку **ОК**.  Если вы используете уровень *Бесплатный*, то вы не сможете изменить срок хранения данных. Необходимо перейти на платный уровень, чтобы управлять этим параметром.

:::image type="content" source="media/manage-cost-storage/manage-cost-change-retention-01.png" alt-text="Изменение параметра хранения данных рабочей области":::

Когда срок хранения уменьшится, в течение нескольких дней льготного периода до тех пор, пока не будет удален новый параметр хранения. 

Страница " **хранение данных** " позволяет выполнять параметры хранения 30, 31, 60, 90, 120, 180, 270, 365, 550 и 730 дней. Если требуется другой параметр, его можно настроить с помощью [Azure Resource Manager](./resource-manager-workspace.md) с помощью `retentionInDays` параметра. Если для параметра период хранения данных задано значение 30 дней, можно немедленно запустить немедленную очистку старых данных, используя `immediatePurgeDataOn30Days` параметр (за исключением периода отсрочки в течение нескольких дней). Это может быть полезно для сценариев, связанных с соответствием, когда немедленное удаление данных является обязательным. Эта функция немедленной очистки предоставляется только через Azure Resource Manager. 

Рабочие области с периодом удержания в 30 дней могут фактически хранить данные в течение 31 дня. Если необходимо хранить данные только в течение 30 дней, используйте Azure Resource Manager, чтобы задать период хранения в 30 дней и с `immediatePurgeDataOn30Days` параметром.  

Два типа данных (`Usage` и `AzureActivity`) по умолчанию хранятся в течение не менее 90 дней, а плата за этот период хранения данных не взимается. Если срок хранения данных рабочей области увеличен до более 90 дней, то срок хранения этих типов данных также увеличится.  Плата за прием данных для этих типов данных не взимается. 

Типы данных в ресурсах Application Insights на основе рабочих областей (`AppAvailabilityResults`, `AppBrowserTimings`, `AppDependencies`, `AppExceptions`, `AppEvents`, `AppMetrics`, `AppPageViews`, `AppPerformanceCounters`, `AppRequests`, `AppSystemEvents` и `AppTraces`) также хранятся в течение 90 дней по умолчанию, плата за этот срок хранения данных не взимается. Срок их хранения можно изменить с помощью функции хранения по типу данных. 

Обратите внимание, что [интерфейс API очистки](/rest/api/loganalytics/workspacepurge/purge) Log Analytics не влияет на выставление счетов за хранение данных и предназначен для использования в строго определенных случаях. Чтобы сократить счет за хранение, необходимо уменьшить срок хранения для рабочей области или для конкретных типов данных. 

### <a name="retention-by-data-type"></a>Хранение по типу данных

Можно также указать различные параметры хранения для отдельных типов данных от 4 до 730 дней (за исключением рабочих областей в устаревшей ценовой категории "бесплатный"), которые переопределяют хранение по умолчанию на уровне рабочей области. Каждый тип данных является подресурсом рабочей области. Например, к таблице SecurityEvent можно обратиться в [Azure Resource Manager](../../azure-resource-manager/management/overview.md), выполнив следующую команду:

```
/subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent
```

Обратите внимание, что тип данных (таблица) учитывает регистр.  Чтобы получить текущие параметры хранения для каждого типа данных определенного типа (в этом примере это SecurityEvent), используйте следующую команду:

```JSON
    GET /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview
```

> [!NOTE]
> Значение retention возвращается только для типа данных, если для него явно задано сохранение.  Типы данных, для которых не было явно задано хранение (и, таким образом, наследуют хранение рабочей области), не возвращают ничего из этого вызова. 

Чтобы получить текущие параметры хранения типа данных для всех типов данных в рабочей области с набором хранения для каждого типа данных, просто исключите конкретный тип данных, например:

```JSON
    GET /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables?api-version=2017-04-26-preview
```

Чтобы задать для срока хранения определенного типа данных (в этом примере — это SecurityEvent) значение 730 дней, выполните следующую команду:

```JSON
    PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview
    {
        "properties": 
        {
            "retentionInDays": 730
        }
    }
```

Допустимые значения для `retentionInDays`: от 30 до 730 дней.

Типы данных `Usage` и `AzureActivity` не могут быть заданы с помощью настраиваемого срока хранения данных. Они будут принимать максимальное значение хранения рабочей области по умолчанию или значение 90 дней. 

Отличным инструментом для прямого подключения к Azure Resource Manager, чтобы настроить хранение по типу данных, является инструмент OSS [ARMclient](https://github.com/projectkudu/ARMClient).  Дополнительные сведения о ARMclient см. в статьях [Дэвида Эббо](http://blog.davidebbo.com/2015/01/azure-resource-manager-client.html) (David Ebbo) и [Дениэла Бовбайс](https://blog.bowbyes.co.nz/2016/11/02/using-armclient-to-directly-access-azure-arm-rest-apis-and-list-arm-policy-details/) (Daniel Bowbyes).  Ниже приведен пример использования ARMClient, в котором для данных SecurityEvent задано значение 730 дней хранения:

```
armclient PUT /subscriptions/00000000-0000-0000-0000-00000000000/resourceGroups/MyResourceGroupName/providers/Microsoft.OperationalInsights/workspaces/MyWorkspaceName/Tables/SecurityEvent?api-version=2017-04-26-preview "{properties: {retentionInDays: 730}}"
```

> [!TIP]
> Настройку хранения отдельных типов данных можно использовать для снижения затрат на хранение данных.  Для данных, собранных начиная с октября 2019 г. (когда эта функция была выпущена), уменьшение срока хранения для некоторых их типов может снизить стоимость хранения.  Для данных, собранных ранее, установка более низкого срока хранения для отдельного типа не повлияет на стоимость хранения.  

## <a name="manage-your-maximum-daily-data-volume"></a>Управление максимальным ежедневным объемом данных

Вы можете настроить ежедневное ограничение и ограничить ежедневный прием данных для своей рабочей области, но будьте осторожны, так как ваша цель не должна превышать дневной лимит.  В противном случае в этот момент вы потеряете данные за оставшуюся часть дня, что может повлиять на доступность в рабочей области служб и решений Azure, функциональные возможности которых зависят от актуальных данных.  В результате вы не сможете наблюдать за условиями работоспособности ресурсов, поддерживающих ИТ-службы, и получать соответствующие уведомления.  Ежедневное ограничение предназначено для использования в качестве способа управления **непредвиденным увеличением** объема данных из управляемых ресурсов, а также за пределами вашего ограничения или за ограничение незапланированных затрат для рабочей области. Нецелесообразно устанавливать ежедневное ограничение каждый день в рабочей области.

Каждая Рабочая область имеет ежедневное ограничение, примененное к другому часу дня. Часы сброса отображаются на странице **ежедневное ограничение** (см. ниже). Этот час сброса не может быть настроен. 

По достижении ежедневного ограничения сбор платных типов данных прекращается до конца дня. Задержка в применении ежедневного ограничения означает, что ограничение не применяется точно на заданном уровне ежедневного ограничения. В верхней части страницы для выбранной рабочей области Log Analytics появится предупреждающий баннер и событие операции будет отправлено в таблицу *Операция* в категории **LogManagement**. Сбор данных возобновится по наступлении времени сброса, определенного в разделе *Daily limit will be set at* (Ежедневное ограничение будет установлено в). Мы рекомендуем определить правило генерации оповещений на основе этого события операции, настроенного для уведомления о достижении дневного лимита данных (см. [ниже](#alert-when-daily-cap-reached)). 

> [!NOTE]
> Ежедневное ограничение не может прерывать сбор данных в соответствии с заданным уровнем закрепления и ожидаемыми избыточными данными, особенно если Рабочая область получает большие объемы данных. [Ниже](#view-the-effect-of-the-daily-cap) приведен запрос, который полезен для изучения режима ежедневного ограничения. 

> [!WARNING]
> Ежедневное ограничение не останавливает сбор типов данных Виндовсевент, Секуритялерт, SecurityBaseline, SecurityBaselineSummary, Секуритидетектион, SecurityEvent, брандмауэра Windows, МалиЦиаусипкоммуникатион, Линуксаудитлог, SysmonEvent, состояние защиты, Update и UpdateSummary, за исключением рабочих областей, в которых центр безопасности Azure был установлен до 19 июня 2017. 

### <a name="identify-what-daily-data-limit-to-define"></a>Определение ежедневного ограничения по сбору данных

Сведения о тенденциях приема данных и определении ежедневного ограничения для объема см. в статье [Анализ использования данных в службе Log Analytics](../usage-estimated-costs.md). Их необходимо тщательно рассмотреть, так как вы не сможете контролировать свои ресурсы после достижения предела. 

### <a name="set-the-daily-cap"></a>Установка ежедневного ограничения

В следующих шагах показано, как настроить ограничения для управления объемом данных, которые рабочая область Log Analytics будет принимать в день.  

1. В рабочей области на панели слева выберите пункт **Usage and estimated costs** (Использование и ожидаемые затраты).
2. На странице **использование и предполагаемые затраты** для выбранной рабочей области щелкните **ограничение данных** в верхней части страницы. 
3. Ежедневное ограничение по умолчанию имеет значение **ОТКЛ.** Щелкните **ВКЛ.** , чтобы включить его, а затем настройте лимит для объема данных (ГБ/день).

:::image type="content" source="media/manage-cost-storage/set-daily-volume-cap-01.png" alt-text="Настройка ограничения сбора данных в Log Analytics":::
    
Ежедневное ограничение можно настроить с помощью ARM, задав `dailyQuotaGb` параметр в `WorkspaceCapping` соответствии с описанием в разделе [рабочие области — создание или обновление](/rest/api/loganalytics/workspaces/createorupdate#workspacecapping). 

### <a name="view-the-effect-of-the-daily-cap"></a>Просмотр результата ежедневного ограничения

Чтобы просмотреть результат ежедневного ограничения, важно учитывать типы данных безопасности, не включаемых в ежедневное ограничение, и сбросить часы для рабочей области. Час сброса ежедневного ограничения отображается на странице **ежедневное ограничение** .  Следующий запрос можно использовать для мониторинга объемов данных в соответствии с ежедневным ограничением между сбросом ежедневного ограничения. В этом примере часы сброса рабочей области — 14:00.  Вам потребуется обновить его для вашей рабочей области.

```kusto
let DailyCapResetHour=14;
Usage
| where Type !in ("SecurityAlert", "SecurityBaseline", "SecurityBaselineSummary", "SecurityDetection", "SecurityEvent", "WindowsFirewall", "MaliciousIPCommunication", "LinuxAuditLog", "SysmonEvent", "ProtectionStatus", "WindowsEvent")
| extend TimeGenerated=datetime_add("hour",-1*DailyCapResetHour,TimeGenerated)
| where TimeGenerated > startofday(ago(31d))
| where IsBillable
| summarize IngestedGbBetweenDailyCapResets=sum(Quantity)/1000. by day=bin(TimeGenerated, 1d) | render areachart  
```

(В типе данных использования единицы `Quantity` находятся в МБ.)

### <a name="alert-when-daily-cap-reached"></a>Оповещение при достижении ежедневного ограничения

Когда достигнуто ограничение сбора данных, на портале Azure отображается соответствующее сообщение. Однако, возможно, вы управляете операционными проблемами, требующими немедленного внимания, иным образом.  Чтобы получать оповещение, вы можете создать новое правило генерации оповещений в Azure Monitor.  Дополнительные сведения см. в статье [Создание и просмотр оповещений метрик, а также управление ими с помощью Azure Monitor](../alerts/alerts-metric.md).

Чтобы приступить к работе, рекомендуем ознакомиться с рекомендациями по выполнению предупреждений о запросе к `Operation` таблице с помощью `_LogOperation` функции. 

- Цель: выбор ресурса Log Analytics.
- Критерии: 
   - "Название сигнала" — "Поиск по пользовательским журналам";
   - Поисковый запрос: `_LogOperation | where Operation == "Data Collection Status" | where Detail contains "OverQuota"`
   - "На основе" — Количество результатов
   - "Условие" — Больше
   - "Пороговое значение" — 0
   - "Период" — 5 (минут);
   - "Частота" — 5 (минут);
- "Имя правила генерации оповещений" — "Достигнут ежедневный предел сбора данных";
- "Уровень серьезности" — "Предупреждение (серьезность 1)"

Как только будет определено оповещение и достигнут предел, активируется предупреждение и будет выполнен ответ, определенный в группе действий. Ваша команда может получать сообщение по электронной почте и текстовое сообщение, могут быть автоматизированы действия с помощью веб-перехватчиков, модулей Runbook службы автоматизации или же выполнена [интеграция с внешним решением ITSM](../alerts/itsmc-definition.md#create-itsm-work-items-from-azure-alerts). 

## <a name="troubleshooting-why-usage-is-higher-than-expected"></a>Превышенный объем данных: причины и устранение

Превышенное использование вызывается одной (или двумя) причинами:
- превышено число узлов, отправляющих данные в рабочую область Log Analytics;
- отправлен превышенный объем данных в рабочую область Log Analytics (возможно, из-за начала использования нового решения или изменения конфигурации существующего решения).

## <a name="understanding-nodes-sending-data"></a>Общие сведения об узлах, отправляющих данные

Чтобы узнать сколько узлов, отправляли пакеты пульса от агента каждый день в прошлом месяце, выполните следующий запрос:

```kusto
Heartbeat 
| where TimeGenerated > startofday(ago(31d))
| summarize nodes = dcount(Computer) by bin(TimeGenerated, 1d)    
| render timechart
```
Чтобы узнать количество узлов, отправляющих данные за последние 24 часа, выполните следующий запрос: 

```kusto
find where TimeGenerated > ago(24h) project Computer
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| where computerName != ""
| summarize nodes = dcount(computerName)
```

Чтобы получить список узлов, отправляющих любые данные (и объем данных, отправленных каждым из них), выполните следующий запрос:

```kusto
find where TimeGenerated > ago(24h) project _BilledSize, Computer
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| where computerName != ""
| summarize TotalVolumeBytes=sum(_BilledSize) by computerName
```

### <a name="nodes-billed-by-the-legacy-per-node-pricing-tier"></a>Узлы, за которые взимается плата за использование устаревшей ценовой категории

В [категории "устаревшие" на узел](#legacy-pricing-tiers) выставляются счета за узлы с почасовой гранулярностью, а также не учитываются узлы, отправляющие только набор типов данных безопасности. Его ежедневное число узлов было бы близко к следующему запросу:

```kusto
find where TimeGenerated >= startofday(ago(7d)) and TimeGenerated < startofday(now()) project Computer, _IsBillable, Type, TimeGenerated
| where Type !in ("SecurityAlert", "SecurityBaseline", "SecurityBaselineSummary", "SecurityDetection", "SecurityEvent", "WindowsFirewall", "MaliciousIPCommunication", "LinuxAuditLog", "SysmonEvent", "ProtectionStatus", "WindowsEvent")
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| where computerName != ""
| where _IsBillable == true
| summarize billableNodesPerHour=dcount(computerName) by bin(TimeGenerated, 1h)
| summarize billableNodesPerDay = sum(billableNodesPerHour)/24., billableNodeMonthsPerDay = sum(billableNodesPerHour)/24./31.  by day=bin(TimeGenerated, 1d)
| sort by day asc
```

Количество единиц в счете находится в единицах измерения node * months, представленных `billableNodeMonthsPerDay` в запросе. Если в рабочей области установлено Управление обновлениями решение, добавьте типы данных Update и UpdateSummary в список в предложении WHERE приведенного выше запроса. Наконец, существует дополнительная сложность в фактическом алгоритме выставления счетов, когда используется целевое решение, не представленное в приведенном выше запросе. 


> [!TIP]
> Используйте эти запросы `find` только в случае необходимости, так как сканирование по типам данных [требует больших затрат ресурсов](./query-optimization.md#query-performance-pane) на выполнение. Если результаты **на компьютер** не нужны, выполните запрос по типу данных об использовании (см. ниже).

## <a name="understanding-ingested-data-volume"></a>Основные сведения о принимаемом объеме данных

На странице **Использование и ожидаемые затраты** есть диаграмма *Data ingestion per solution* (Прием данных по решениям), которая позволяет отобразить объем отправленных данных в целом и для каждого решения отдельно. Это позволяет выявить некоторые тенденции, например оценить изменения общего объема используемых данных (или по определенному решению). 

### <a name="data-volume-for-specific-events"></a>Объем данных для конкретных событий

Чтобы просмотреть размер принимаемых данных для определенного набора событий, можно запросить определенную таблицу (в этом примере `Event`), а затем ограничить запрос интересующими событиями (в этом примере это событие с идентификатором 5145 или 5156):

```kusto
Event
| where TimeGenerated > startofday(ago(31d)) and TimeGenerated < startofday(now()) 
| where EventID == 5145 or EventID == 5156
| where _IsBillable == true
| summarize count(), Bytes=sum(_BilledSize) by EventID, bin(TimeGenerated, 1d)
``` 

Обратите внимание, что предложение `where _IsBillable = true` отсеивает типы данных из определенных решений, для которых не взимается плата за прием данных. Дополнительные [сведения](./log-standard-columns.md#_isbillable) о `_IsBillable` .

### <a name="data-volume-by-solution"></a>объем данных для каждого решения;

Запрос, используемый для просмотра объема оплачиваемых данных по решению за последний месяц (за исключением части последнего дня):

```kusto
Usage 
| where TimeGenerated > ago(32d)
| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())
| where IsBillable == true
| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), Solution 
| render columnchart
```

Предложение с `TimeGenerated` предназначено только для того, чтобы убедиться, что на портале Azure запрос выполнит ретроспективный поиск по умолчанию за 24 часа. При использовании типа данных об использовании `StartTime` и `EndTime` представляют временные периоды, для которых выводятся результаты. 

### <a name="data-volume-by-type"></a>Объем данных по типу

Вы можете изучить данные еще подробнее, чтобы найти тенденции для определенных типов данных, выполнив следующий запрос:

```kusto
Usage 
| where TimeGenerated > ago(32d)
| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())
| where IsBillable == true
| summarize BillableDataGB = sum(Quantity) / 1000. by bin(StartTime, 1d), DataType 
| render columnchart
```

Или чтобы просмотреть таблицу по решению и типу за прошлый месяц, выполните следующий запрос:

```kusto
Usage 
| where TimeGenerated > ago(32d)
| where StartTime >= startofday(ago(31d)) and EndTime < startofday(now())
| where IsBillable == true
| summarize BillableDataGB = sum(Quantity) / 1000 by Solution, DataType
| sort by Solution asc, DataType asc
```

### <a name="data-volume-by-computer"></a>Объем данных на компьютер

Тип данных `Usage` не содержит сведений на уровне компьютера. Чтобы увидеть **размер** полученных данных для каждого компьютера, используйте [свойство](./log-standard-columns.md#_billedsize) `_BilledSize`, которое предоставляет размер в байтах:

```kusto
find where TimeGenerated > ago(24h) project _BilledSize, _IsBillable, Computer
| where _IsBillable == true 
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| summarize BillableDataBytes = sum(_BilledSize) by  computerName 
| sort by BillableDataBytes nulls last
```

[Свойство](./log-standard-columns.md#_isbillable) `_IsBillable` указывает, будет ли взиматься плата за полученные данные. 

Чтобы узнать **количество** полученных оплачиваемых событий для каждого компьютера, выполните следующий запрос: 

```kusto
find where TimeGenerated > ago(24h) project _IsBillable, Computer
| where _IsBillable == true 
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| summarize eventCount = count() by computerName  
| sort by eventCount nulls last
```

> [!TIP]
> Используйте эти запросы `find` только в случае необходимости, так как сканирование по типам данных [требует больших затрат ресурсов](./query-optimization.md#query-performance-pane) на выполнение. Если результаты **для каждого компьютера** не нужны, выполните запрос по типу данных об использовании.

### <a name="data-volume-by-azure-resource-resource-group-or-subscription"></a>Объем данных по ресурсам, группам ресурсов или подпискам Azure

Для данных из узлов, размещенных в Azure, можно определить **размер** полученных данных __для каждого компьютера__. Используйте [свойство](./log-standard-columns.md#_resourceid) _ResourceId, которое предоставляет полный путь к ресурсу:

```kusto
find where TimeGenerated > ago(24h) project _ResourceId, _BilledSize, _IsBillable
| where _IsBillable == true 
| summarize BillableDataBytes = sum(_BilledSize) by _ResourceId | sort by BillableDataBytes nulls last
```

Для данных из узлов, размещенных в Azure, можно получить **Размер** принимаемых данных для __каждой подписки Azure__. для этого используйте `_SubscriptionId` свойство следующим образом:

```kusto
find where TimeGenerated > ago(24h) project _BilledSize, _IsBillable, _SubscriptionId
| where _IsBillable == true 
| summarize BillableDataBytes = sum(_BilledSize) by _SubscriptionId | sort by BillableDataBytes nulls last
```

Чтобы получить объем данных по группе ресурсов, можно выполнить синтаксический анализ `_ResourceId` :

```kusto
find where TimeGenerated > ago(24h) project _ResourceId, _BilledSize, _IsBillable
| where _IsBillable == true 
| summarize BillableDataBytes = sum(_BilledSize) by _ResourceId
| extend resourceGroup = tostring(split(_ResourceId, "/")[4] )
| summarize BillableDataBytes = sum(BillableDataBytes) by resourceGroup | sort by BillableDataBytes nulls last
```

Кроме того, при необходимости можно выполнить синтаксический анализ более полного синтаксического анализа `_ResourceId` с помощью

```Kusto
| parse tolower(_ResourceId) with "/subscriptions/" subscriptionId "/resourcegroups/" 
    resourceGroup "/providers/" provider "/" resourceType "/" resourceName   
```

> [!TIP]
> Используйте эти запросы `find` только в случае необходимости, так как сканирование по типам данных [требует больших затрат ресурсов](./query-optimization.md#query-performance-pane) на выполнение. Если вам не нужны результаты для каждой подписки, группы ресурсов или каждого имени ресурса, выполните запрос по типу данных об использовании.

> [!WARNING]
> Некоторые поля с типом данных Usage (Потребление) уже устарели и данные в них не заполняются, хотя они пока сохраняются в схеме. Например, сюда относятся поля **Computer** и ряд данных о приеме данных (**TotalBatches**, **BatchesWithinSla**, **BatchesOutsideSla**, **BatchesCapped** и **AverageProcessingTimeMs**).


### <a name="querying-for-common-data-types"></a>Запросы для общих типов данных

Чтобы получить более подробную информацию об источнике данных по определенному типу данных, воспользуйтесь приведенными ниже примерами запросов.

+ Ресурсы **рабочей области Application Insights**.
  - Дополнительные сведения см. в статье [Управление использованием и затратами Application Insights](../app/pricing.md#data-volume-for-workspace-based-application-insights-resources)
+ Решение по **безопасности**
  - `SecurityEvent | summarize AggregatedValue = count() by EventID`
+ Решение для **управления журналами**
  - `Usage | where Solution == "LogManagement" and iff(isnotnull(toint(IsBillable)), IsBillable == true, IsBillable == "true") == true | summarize AggregatedValue = count() by DataType`
+ Тип данных **Perf**
  - `Perf | summarize AggregatedValue = count() by CounterPath`
  - `Perf | summarize AggregatedValue = count() by CounterName`
+ Тип данных **Event**
  - `Event | summarize AggregatedValue = count() by EventID`
  - `Event | summarize AggregatedValue = count() by EventLog, EventLevelName`
+ Тип данных **Syslog**
  - `Syslog | summarize AggregatedValue = count() by Facility, SeverityLevel`
  - `Syslog | summarize AggregatedValue = count() by ProcessName`
+ Тип данных **AzureDiagnostics**
  - `AzureDiagnostics | summarize AggregatedValue = count() by ResourceProvider, ResourceId`

## <a name="tips-for-reducing-data-volume"></a>Советы по снижению объемов данных

Вот несколько рекомендаций, которые помогут снизить объем собираемых журналов.

| Источник превышенного объема данных | Как сократить объем данных |
| -------------------------- | ------------------------- |
| Аналитика контейнеров         | [Настройте контейнерную аналитику](../containers/container-insights-cost.md#controlling-ingestion-to-reduce-cost) для получения только необходимых данных. |
| События безопасности            | Выберите [события со стандартным или минимальным уровнем безопасности](../../security-center/security-center-enable-data-collection.md#data-collection-tier). <br> Измените политику аудита безопасности таким образом, чтобы собирать только необходимые события. В частности проверьте необходимость сбора следующих событий: <br> - [аудит платформы фильтрации](/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/dd772749(v=ws.10)); <br> - [аудит реестра](/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/dd941614(v%3dws.10));<br> - [аудит файловой системы](/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/dd772661(v%3dws.10));<br> - [аудит объектов ядра](/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/dd941615(v%3dws.10));<br> - [аудит работы с дескрипторами](/previous-versions/windows/it-pro/windows-server-2008-R2-and-2008/dd772626(v%3dws.10));<br> — аудит съемных носителей. |
| Счетчики производительности       | Измените [конфигурацию счетчика производительности](../agents/data-sources-performance-counters.md), чтобы <br> уменьшить частоту сбора или <br> сократить число счетчиков производительности. |
| Журналы событий                 | Измените [конфигурацию журнала событий](../agents/data-sources-windows-events.md), чтобы <br> сократить число собранных журналов событий или <br> выполнять сбор только необходимых уровней событий. Например, не выполняйте сбор событий уровня *сведений*. |
| Системный журнал                     | Измените [конфигурацию системного журнала](../agents/data-sources-syslog.md), чтобы <br> сократить число собранных объектов или <br> выполнять сбор только необходимых уровней событий. Например, не выполняйте сбор событий уровня *сведений* и *отладки*. |
| AzureDiagnostics           | Изменить [коллекцию журналов ресурсов](../essentials/diagnostic-settings.md#create-in-azure-portal) на: <br> Уменьшить число ресурсов, отправляющих журналы в Log Analytics. <br> Выполнять сбор только необходимых журналов. |
| Данные решений с компьютеров, которым не требуется решение | Используйте [нацеливание решений](../insights/solution-targeting.md), чтобы выполнять сбор данных только в нужных группах компьютеров. |
| Application Insights | Параметры проверки [https://docs.microsoft.com/azure/azure-monitor/app/pricing#managing-your-data-volume](managing Application Insights data volume) |
| [Аналитика SQL](../insights/azure-sql.md) | Для настройки параметров аудита используйте [Set-азсклсервераудит](/powershell/module/az.sql/set-azsqlserveraudit) . |
| Azure Sentinel | Проверьте все [Источники данных Sentinel](../../sentinel/connect-data-sources.md) , которые вы недавно включили в качестве источников дополнительного тома данных. |

### <a name="getting-nodes-as-billed-in-the-per-node-pricing-tier"></a>Получение узлов с выставленным счетом в ценовой категории "За узел"

Чтобы получить список компьютеров, которые будут оплачиваться как узлы, если рабочая область находится в устаревшей ценовой категории "За узел", найдите узлы, которые отправляют **счет за типы данных** (некоторые типы данных бесплатные). Для этого используйте [свойство](./log-standard-columns.md#_isbillable) `_IsBillable` и крайнее левое поле полного доменного имени. При этом возвращается число компьютеров с оплатой данных за час (степень детализации, при которой подсчитываются узлы и за них выставляются счета):

```kusto
find where TimeGenerated > ago(24h) project Computer, TimeGenerated
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| where computerName != ""
| summarize billableNodes=dcount(computerName) by bin(TimeGenerated, 1h) | sort by TimeGenerated asc
```

### <a name="getting-security-and-automation-node-counts"></a>Получение количества узлов безопасности и автоматизации

Чтобы просмотреть количество отдельных узлов безопасности, выполните такой запрос:

```kusto
union
(
    Heartbeat
    | where (Solutions has 'security' or Solutions has 'antimalware' or Solutions has 'securitycenter')
    | project Computer
),
(
    ProtectionStatus
    | where Computer !in~
    (
        (
            Heartbeat
            | project Computer
        )
    )
    | project Computer
)
| distinct Computer
| project lowComputer = tolower(Computer)
| distinct lowComputer
| count
```

Чтобы просмотреть количество отдельных узлов службы автоматизации, выполните такой запрос:

```kusto
 ConfigurationData 
 | where (ConfigDataType == "WindowsServices" or ConfigDataType == "Software" or ConfigDataType =="Daemons") 
 | extend lowComputer = tolower(Computer) | summarize by lowComputer 
 | join (
     Heartbeat 
       | where SCAgentChannel == "Direct"
       | extend lowComputer = tolower(Computer) | summarize by lowComputer, ComputerEnvironment
 ) on lowComputer
 | summarize count() by ComputerEnvironment | sort by ComputerEnvironment asc
```

## <a name="evaluating-the-legacy-per-node-pricing-tier"></a>Оценка устаревшей ценовой категории "За узел"

Оценка того, являются ли рабочие области более эффективными в устаревшей ценовой категории **За узел** или в текущих категориях **Резервирование мощности** или **с оплатой по мере использования**, вызывает у клиентов значительные трудности.  Для этого необходимо добиться компромисса между фиксированными затратами за отслеживаемый узел в ценовой категории "За узел" с включенным распределением данных 500 МБ/узел/день и затратами на получение данных в ценовой категории с оплатой по мере использования (за ГБ). 

Для упрощения этой оценки можно использовать следующий запрос, чтобы создать рекомендацию для оптимальной ценовой категории на основе шаблонов использования рабочей области.  Этот запрос анализирует отслеживаемые узлы и данные, полученные в рабочей области за последние семь дней, и каждый день оценивает, какая ценовая категория была бы оптимальной. Чтобы использовать запрос, необходимо:

1. указать, использует ли рабочая область Центр безопасности Azure, установив для `workspaceHasSecurityCenter` значение `true` или `false`; 
2. обновить цены, если у вас есть определенные скидки;
3. указать число дней для ретроспективного анализа, установив `daysToEvaluate`. Это полезно, если запрос занимает слишком много времени, анализируя данные за 7 дней. 

Ниже приведен запрос рекомендации ценовой категории.

```kusto
// Set these parameters before running query
// Pricing details available at https://azure.microsoft.com/en-us/pricing/details/monitor/
let daysToEvaluate = 7; // Enter number of previous days to analyze (reduce if the query is taking too long)
let workspaceHasSecurityCenter = false;  // Specify if the workspace has Azure Security Center
let PerNodePrice = 15.; // Enter your montly price per monitored nodes
let PerNodeOveragePrice = 2.30; // Enter your price per GB for data overage in the Per Node pricing tier
let PerGBPrice = 2.30; // Enter your price per GB in the Pay-as-you-go pricing tier
let CarRes100Price = 196.; // Enter your price for the 100 GB/day Capacity Reservation
let CarRes200Price = 368.; // Enter your price for the 200 GB/day Capacity Reservation
let CarRes300Price = 540.; // Enter your price for the 300 GB/day Capacity Reservation
let CarRes400Price = 704.; // Enter your price for the 400 GB/day Capacity Reservation
let CarRes500Price = 865.; // Enter your price for the 500 GB/day Capacity Reservation
// ---------------------------------------
let SecurityDataTypes=dynamic(["SecurityAlert", "SecurityBaseline", "SecurityBaselineSummary", "SecurityDetection", "SecurityEvent", "WindowsFirewall", "MaliciousIPCommunication", "LinuxAuditLog", "SysmonEvent", "ProtectionStatus", "WindowsEvent", "Update", "UpdateSummary"]);
let StartDate = startofday(datetime_add("Day",-1*daysToEvaluate,now()));
let EndDate = startofday(now());
union * 
| where TimeGenerated >= StartDate and TimeGenerated < EndDate
| extend computerName = tolower(tostring(split(Computer, '.')[0]))
| where computerName != ""
| summarize nodesPerHour = dcount(computerName) by bin(TimeGenerated, 1h)  
| summarize nodesPerDay = sum(nodesPerHour)/24.  by day=bin(TimeGenerated, 1d)  
| join kind=leftouter (
    Heartbeat 
    | where TimeGenerated >= StartDate and TimeGenerated < EndDate
    | where Computer != ""
    | summarize ASCnodesPerHour = dcount(Computer) by bin(TimeGenerated, 1h) 
    | extend ASCnodesPerHour = iff(workspaceHasSecurityCenter, ASCnodesPerHour, 0)
    | summarize ASCnodesPerDay = sum(ASCnodesPerHour)/24.  by day=bin(TimeGenerated, 1d)   
) on day
| join (
    Usage 
    | where TimeGenerated >= StartDate and TimeGenerated < EndDate
    | where IsBillable == true
    | extend NonSecurityData = iff(DataType !in (SecurityDataTypes), Quantity, 0.)
    | extend SecurityData = iff(DataType in (SecurityDataTypes), Quantity, 0.)
    | summarize DataGB=sum(Quantity)/1000., NonSecurityDataGB=sum(NonSecurityData)/1000., SecurityDataGB=sum(SecurityData)/1000. by day=bin(StartTime, 1d)  
) on day
| extend AvgGbPerNode =  NonSecurityDataGB / nodesPerDay
| extend OverageGB = iff(workspaceHasSecurityCenter, 
             max_of(DataGB - 0.5*nodesPerDay - 0.5*ASCnodesPerDay, 0.), 
             max_of(DataGB - 0.5*nodesPerDay, 0.))
| extend PerNodeDailyCost = nodesPerDay * PerNodePrice / 31. + OverageGB * PerNodeOveragePrice
| extend billableGB = iff(workspaceHasSecurityCenter,
             (NonSecurityDataGB + max_of(SecurityDataGB - 0.5*ASCnodesPerDay, 0.)), DataGB )
| extend PerGBDailyCost = billableGB * PerGBPrice
| extend CapRes100DailyCost = CarRes100Price + max_of(billableGB - 100, 0.)* PerGBPrice
| extend CapRes200DailyCost = CarRes200Price + max_of(billableGB - 200, 0.)* PerGBPrice
| extend CapRes300DailyCost = CarRes300Price + max_of(billableGB - 300, 0.)* PerGBPrice
| extend CapRes400DailyCost = CarRes400Price + max_of(billableGB - 400, 0.)* PerGBPrice
| extend CapResLevel500AndAbove = max_of(floor(billableGB, 100),500)
| extend CapRes500AndAboveDailyCost = CarRes500Price*CapResLevel500AndAbove/500 + max_of(billableGB - CapResLevel500AndAbove, 0.)* PerGBPrice
| extend MinCost = min_of(
    PerNodeDailyCost,PerGBDailyCost,CapRes100DailyCost,CapRes200DailyCost,
    CapRes300DailyCost, CapRes400DailyCost, CapRes500AndAboveDailyCost)
| extend Recommendation = case(
    MinCost == PerNodeDailyCost, "Per node tier",
    MinCost == PerGBDailyCost, "Pay-as-you-go tier",
    MinCost == CapRes100DailyCost, "Capacity Reservation (100 GB/day)",
    MinCost == CapRes200DailyCost, "Capacity Reservation (200 GB/day)",
    MinCost == CapRes300DailyCost, "Capacity Reservation (300 GB/day)",
    MinCost == CapRes400DailyCost, "Capacity Reservation (400 GB/day)",
    MinCost == CapRes500AndAboveDailyCost, strcat("Capacity Reservation (",CapResLevel500AndAbove," GB/day)"),
    "Error"
)
| project day, nodesPerDay, ASCnodesPerDay, NonSecurityDataGB, SecurityDataGB, OverageGB, AvgGbPerNode, PerGBDailyCost, PerNodeDailyCost, 
    CapRes100DailyCost, CapRes200DailyCost, CapRes300DailyCost, CapRes400DailyCost, CapRes500AndAboveDailyCost, Recommendation 
| sort by day asc
//| project day, Recommendation // Comment this line to see details
| sort by day asc
```

Этот запрос не является точной репликацией того, как вычисляются данные об использовании, но он подойдет для предоставления рекомендаций по ценовым категориям в большинстве случаев.  

> [!NOTE]
> Чтобы использовать права, полученные при покупке подписки OMS E1, OMS E2 или настройки OMS для System Center, выберите ценовой уровень Log Analytics *за узле*.

## <a name="create-an-alert-when-data-collection-is-high"></a>Создание оповещения при высоком уровне сбора данных

В этом разделе описывается создание предупреждения о том, что объем данных за последние 24 часа превысил указанный объем, с помощью Azure Monitor [оповещений журнала](../alerts/alerts-unified-log.md). 

Чтобы предупредить, если объем оплачиваемых данных, принятых за последние 24 часа, был больше 50 ГБ, выполните следующие действия. 

- Для параметра **Определение условия оповещения** укажите вашу рабочую область Log Analytics в качестве целевого ресурса.
- Для **критериев оповещения** укажите следующее:
   - для **названия сигнала** выберите значение **Пользовательский поиск по журналам**;
   - **Поисковый запрос** к `Usage | where IsBillable | summarize DataGB = sum(Quantity / 1000.) | where DataGB > 50` . Если требуется другой 
   - **логика оповещений** должна быть **основана на** *числе результатов*, а значение **условия** должно быть *больше* **порогового значения** *0*;
   - **Период времени** *1440* минут и **периодичность оповещений** каждые *1440* минут для запуска один раз в день.
- В разделе **Определение сведений об оповещении** задайте такие значения:
   - **Имя** для *объема оплачиваемых данных, превышающих 50 ГБ за 24 часа*
   - **серьезности** — *предупреждение*;

Укажите существующую или создайте новую [группу действий](../alerts/action-groups.md), чтобы получать соответствующее уведомление, когда оповещение журнала соответствует заданным критериям.

При получении предупреждения выполните действия, описанные в разделах выше, чтобы узнать, почему использование выше ожидаемого.

## <a name="data-transfer-charges-using-log-analytics"></a>Плата за передачу данных с помощью Log Analytics

Отправка данных в Log Analytics может повлечь за собой плату за пропускную способность данных, однако она ограничена виртуальными машинами, на которых установлен агент Log Analytics, и не применяется при использовании параметров диагностики или с другими соединителями, встроенными в Azure Sentinel. Как описано на [странице цен на пропускную способность Azure](https://azure.microsoft.com/pricing/details/bandwidth/), передача данных между службами Azure, расположенными в двух регионах, оплачивается как передача исходящих данных по стандартной цене. Передача входящих данных предоставляется бесплатно. Однако эта плата очень мала (несколько %) по сравнению с затратами на прием данных Log Analytics. Следовательно, управление затратами на Log Analytics необходимо сосредоточиться на принимаемом [объеме данных](#understanding-ingested-data-volume). 


## <a name="troubleshooting-why-log-analytics-is-no-longer-collecting-data"></a>Почему Log Analytics больше не собирает данные

Если вы используете устаревшую бесплатную ценовую категорию и отправили больше 500 МБ данных за день, сбор данных останавливается до конца дня. Достижение ежедневного ограничения является распространенной причиной, по которой Log Analytics прекращает сбор данных или по которой данные отсутствуют.  Log Analytics создает событие типа "Операция", когда сбор данных начинается и останавливается. Выполните следующий запрос в поле поиска, чтобы проверить, достигнут ли лимит и отсутствуют ли данные: 

```kusto
Operation | where OperationCategory == 'Data Collection Status'
```

При остановке сбора данных параметр OperationStatus принимает значение **Warning**. В начале сбора данных параметр OperationStatus принимает значение **Succeeded**. В следующей таблице описаны причины, по которым сбор данных останавливается, и приведены рекомендуемые действия, чтобы его возобновить:  

|Причина прекращения сбора| Решение| 
|-----------------------|---------|
|Достигнуто ежедневное ограничение рабочей области.|Дождитесь автоматического перезапуска сбора или увеличьте предел ежедневного объема собираемых данных, как описано в разделе управления максимальным ежедневным объемом данных. Время сброса ежедневного ограничения отображается на странице **ежедневное ограничение** . |
| Ваша рабочая область достигла [скорости приема данных](../service-limits.md#log-analytics-workspaces) | По умолчанию ограничения частоты на объем приема данных, отправляемых из ресурсов Azure с помощью параметров диагностики, составляет примерно 6 ГБ в минуту на рабочую область. Это приблизительное значение, так как фактический размер может различаться в зависимости от типов данных, которые в свою очередь зависят от длины журнала и его коэффициента сжатия. Это ограничение не применяется к данным, отправляемым из агентов или API сборщика данных. Если вы отправите данные с более высокой частотой в одну рабочую область, некоторые данные будут удалены, а событие будет отправляться в таблицу операций в рабочей области каждые 6 часов, пока пороговое значение не будет превышено. Если объем приема данных превышает ограничение частоты или это ограничение будет превышено в ближайшее время, вы можете запросить увеличение рабочей области, отправив сообщение электронной почты по адресу LAIngestionRate@microsoft.com или открыв запрос на поддержку. Требуемое событие, которое указывает на ограничение частоты приема данных, можно найти с помощью запроса `Operation | where OperationCategory == "Ingestion" | where Detail startswith "The rate of data crossed the threshold"`. |
|Достигнут ежедневный предел устаревшей бесплатной ценовой категории. |Дождитесь следующего дня для автоматического перезапуска сбора или перейдите на платную ценовую категорию.|
|Подписка Azure находится в состоянии "Приостановлено" по причине:<br> Период бесплатной пробной версии завершен<br> Истек срок действия Azure Pass<br> Достигнут лимит ежемесячной суммы расходов (например, на подписку MSDN или Visual Studio)|Измените подписку на платную<br> Удалите ограничение или подождите, пока оно сбросится|

Чтобы получить уведомление о прекращении сбора данных, выполните действия, описанные в предупреждении *о создании ограничения ежедневного сбора данных*. Выполните действия, описанные в статье [Создание групп действий и управление ими на портале Azure](../alerts/action-groups.md), чтобы настроить действия электронной почты, веб-перехватчика или runbook для правила генерации оповещений. 

## <a name="limits-summary"></a>Сводная таблица ограничений

Существует несколько дополнительных ограничений Log Analytics, некоторые из которых зависят от ценовой категории Log Analytics. Они описаны в [подписке Azure, ограничениях, квотах и ограничениях службы](../../azure-resource-manager/management/azure-subscription-service-limits.md#log-analytics-workspaces).


## <a name="next-steps"></a>Дальнейшие действия

- Ознакомьтесь со статьей [Общие сведения о запросах журналов в Azure Monitor](../logs/log-query-overview.md), чтобы узнать, как использовать язык поиска. Вы можете использовать поисковые запросы, чтобы выполнить дополнительный анализ данных об использовании.
- Выполните действия, описанные в разделе о [создании оповещений журналов](../alerts/alerts-metric.md), чтобы получать уведомления при выполнении условий поиска.
- Используйте [нацеливание решений](../insights/solution-targeting.md), чтобы выполнять сбор данных только в нужных группах компьютеров.
- Сведения о настройке эффективной политики сбора событий см. в статье [Сбор данных в Центре безопасности Azure](../../security-center/security-center-enable-data-collection.md).
- Измените [конфигурацию счетчика производительности](../agents/data-sources-performance-counters.md).
- Сведения об изменении параметров сбора событий см. в статье [Источники данных для журнала событий Windows в Log Analytics](../agents/data-sources-windows-events.md).
- Сведения об изменении конфигурации системного журнала см. в статье [Источники данных для журнала событий Windows в Log Analytics](../agents/data-sources-syslog.md).
- Сведения об изменении конфигурации системного журнала см. в статье [Источники данных для журнала событий Windows в Log Analytics](../agents/data-sources-syslog.md).
