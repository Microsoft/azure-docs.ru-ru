---
title: Периодическое извлечение больших наборов данных о затратах с помощью функции экспорта
description: Узнайте, как регулярно экспортировать большие объемы данных с помощью функции экспорта данных из Управления затратами Azure.
author: bandersmsft
ms.author: banders
ms.date: 03/08/2021
ms.topic: conceptual
ms.service: cost-management-billing
ms.subservice: cost-management
ms.reviewer: adwise
ms.openlocfilehash: 465225341bdffc984ac6cbc82ba94eb656ad60df
ms.sourcegitcommit: 15d27661c1c03bf84d3974a675c7bd11a0e086e6
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/09/2021
ms.locfileid: "102509649"
---
# <a name="retrieve-large-cost-datasets-recurringly-with-exports"></a>Периодическое извлечение больших наборов данных о затратах с помощью функции экспорта

Узнайте, как регулярно экспортировать большие объемы данных с помощью функции экспорта данных из Управления затратами Azure. Экспорт — это рекомендуемый способ извлечения неагрегированных данных о затратах, особенно если файлы данных о потреблении слишком велики для надежного вызова и загрузки с помощью API сведений о потреблении. Экспортированные данные помещаются в выбранную учетную запись службы хранилища Azure. Из этой учетной записи вы можете загружать такие данные в свои системы и анализировать их по мере необходимости. Сведения о настройке функции экспорта на портале Azure см. в разделе [Экспорт данных](tutorial-export-acm-data.md).

Если вы хотите автоматизировать операции экспорта в различных областях, для начала можно воспользоваться образцом запроса API, приведенным в следующем разделе. Вы можете использовать API экспорта для создания операций автоматического экспорта в рамках общей конфигурации среды. Операции автоматического экспорта помогают обеспечить наличие необходимых данных. Вы можете использовать их в системах своей организации по мере расширения использования Azure.

## <a name="common-export-configurations"></a>Общие конфигурации экспорта

Прежде чем создавать первую операцию экспорта, проанализируйте сценарий и параметры конфигурации, необходимые для его реализации. Изучите приведенные ниже параметры экспорта.

- **Повторение**. Определяет частоту выполнения задания экспорта и момент, когда файл помещается в учетную запись службы хранилища Azure. Вы можете выбрать выполнение ежедневно, еженедельно или ежемесячно. Попробуйте настроить повторение так, чтобы оно соответствовало заданиям импорта данных, используемым внутренней системой вашей организации.
- **Период повторения**. Определяет время, в течение которого экспорт остается действительным. Файлы экспортируются только в течение периода повторения.
- **Интервал времени**. Определяет объем данных, генерируемых операцией экспорта для данного запуска. Распространенными параметрами являются MonthToDate и WeekToDate.
- **Дата начала**. Позволяет задать начало для расписания экспорта. Экспорт создается на дату начала (StartDate), а затем с периодичностью, заданной параметром повторения.
- **Тип**. Существует три типа экспорта.
  - ActualCost — показывает общее потребление и затраты за указанный период, по мере их начисления, а также указывает их в счете.
  - AmortizedCost — показывает общее потребление и затраты за указанный период с учетом амортизации, применяемой к соответствующим затратам на покупку резервирования.
  - Usage — все операции экспорта, созданные до 20 июля 2020 года, принадлежат к этому типу. Обновите все запланированные операции экспорта, установив для них тип ActualCost или AmortizedCost.
- **Столбцы**. Определяет поля данных, которые необходимо добавить в файл экспорта. Они соответствуют полям, доступным в API сведений о потреблении. Подробные сведения см. на странице [описания API сведений о потреблении](/rest/api/consumption/usagedetails/list).

## <a name="create-a-daily-month-to-date-export-for-a-subscription"></a>Создание ежедневного экспорта за текущий месяц для подписки

URL-адрес запроса: `PUT https://management.azure.com/{scope}/providers/Microsoft.CostManagement/exports/{exportName}?api-version=2020-06-01`

```json
{
  "properties": {
    "schedule": {
      "status": "Active",
      "recurrence": "Daily",
      "recurrencePeriod": {
        "from": "2020-06-01T00:00:00Z",
        "to": "2020-10-31T00:00:00Z"
      }
    },
    "format": "Csv",
    "deliveryInfo": {
      "destination": {
        "resourceId": "/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/MYDEVTESTRG/providers/Microsoft.Storage/storageAccounts/{yourStorageAccount} ",
        "container": "{yourContainer}",
        "rootFolderPath": "{yourDirectory}"
      }
    },
    "definition": {
      "type": "ActualCost",
      "timeframe": "MonthToDate",
      "dataSet": {
        "granularity": "Daily",
        "configuration": {
          "columns": [
            "Date",
            "MeterId",
            "ResourceId",
            "ResourceLocation",
            "Quantity"
          ]
        }
      }
    }
}
```

## <a name="copy-large-azure-storage-blobs"></a>Копирование больших двоичных объектов из хранилища Azure

Вы можете использовать Управление затратами, чтобы запланировать экспорт сведений о потреблении ресурсов Azure в учетные записи службы хранилища Azure в виде больших двоичных объектов. Объем полученных таким образом больших двоичных объектов может достигать несколько гигабайт. Команды разработки Управления затратами Azure и службы хранилища Azure протестировали функцию копирования больших двоичных объектов в службу хранилища Azure. Полученные результаты описаны в следующих разделах. Вы можете рассчитывать на аналогичные показатели при копировании больших двоичных объектов хранилища из одного региона Azure в другой.

Чтобы проверить производительность, мы передавали большие двоичные объекты из учетных записей хранения, расположенных в регионе "Западная часть США", в тот же регион и другие регионы. Показатели скорости колебались в диапазоне от 2 ГБ в секунду (в пределах региона) до 150 МБ в секунду (в учетные записи хранения в регионе "Юго-Восточная Азия").

### <a name="test-configuration"></a>Конфигурация теста

Для измерения скорости передачи больших двоичных объектов мы создали простое консольное приложение .NET на основе последней версии (2.0.1) библиотеки перемещения данных Azure (DLM), подключенной через NuGet. DLM представляет собой пакет SDK, предоставляемый командой службы хранилища Azure для упрощения программного доступа к службам передачи данных. Затем были созданы учетные записи хранения категории "Стандартный" 2-й версии в нескольких регионах, а в качестве исходного был выбран регион "Западная часть США". В учетных записях хранения были размещены контейнеры, содержащие по 10 блочных BLOB-объектов размером 2 ГБ. Эти контейнеры копировались в другие учетные записи хранения с помощью метода _TransferManager.CopyDirectoryAsync()_ библиотеки DLM с параметром _CopyMethod.ServiceSideSyncCopy_. Тесты проводились на компьютере под управлением Windows 10 с 12 ядрами и одним сетевым подключением 1-GbE.

Использовались следующие параметры приложения:

- _TransferManager.Configurations.ParallelOperations_ = _Environment.ProcessorCount \* 32._ Мы обнаружили, что этот параметр больше всего влияет на общую пропускную способность. Выбранный вариант (значение, в 32 раза превышающее число ядер) обеспечил самую высокую пропускную способность для тестового клиента.
- _ServicePointManager.DefaultConnectionLimit = int.MaxValue._ При настройке максимального значения управление параллелизмом передается указанному выше параметру _ParallelOperations_.
- _TransferManager.Configurations.BlockSize = 4 194 304._ Этот параметр немного влияет на скорость передачи, и в тесте размер 4 МБ оказался оптимальным.

Дополнительные сведения и примеры кода можно найти по ссылкам в разделе [Дальнейшие действия](#next-steps).

### <a name="test-results"></a>Результаты испытаний

| **Номер теста** | **Целевой регион** | **BLOB-объекты** | **Время (с)** | **Скорость (МБ/с)** | **Комментарии** |
| --- | --- | --- | --- | --- | --- |
| 1 | WestUS | 2 ГБ x 10 | 10 | 2 000 |   |
| 2 | WestUS2 | 2 ГБ x 10 | 33 | 600 |   |
| 3 | EastUS | 2 ГБ x 10 | 67 | 300 |   |
| 4 | EastUS | 2 ГБ x 10 x 4 | 99 | 200 | 4 параллельные передачи с использованием 8 учетных записей хранения (по 4 в западном и восточном регионах); приводятся усредненные данные для одной передачи. |
| 6 | EastUS | 2 ГБ x 10 x 4 | 92 | 870 | 4 параллельные передачи из одной учетной записи хранения в другую. |
| 5 | EastUS | 2 ГБ x 10 x 8 | 148 | 135 | 8 параллельных передач с использованием 8 учетных записей хранения (4 в западном и 4×2 в восточном регионах); приводятся средние данные для одной передачи. |
| 7 | Юго-Восточная Азия | 2 ГБ x 10 | 133 | 150 |   |
| 8 | Юго-Восточная Азия | 2 ГБ x 10 x 4 | 444 | 180 | 4 параллельные передачи из одной учетной записи хранения в другую. |

### <a name="sync-transfer-characteristics"></a>Параметры синхронной передачи

Ниже приведены некоторые характеристики синхронной передачи на стороне службы, которые используются для DML и имеют отношение к работе с ней.

- DML может передавать один большой двоичный объект или каталог. Для передачи каталогов можно использовать шаблон поиска, чтобы сопоставлять большие двоичные объекты по префиксу.
- Передача блочных BLOB-объектов происходит параллельно. Все передачи завершаются ближе к концу процесса передачи. Отдельные блочные BLOB-объекты передаются параллельно.
- Перемещение выполняется на клиенте асинхронно. Состояние передачи можно периодически проверять с помощью обратного вызова к методу, который можно определить в объекте _TransferContext_.
- Операция передачи создает контрольные точки и предоставляет объект _TransferCheckpoint_. Этот объект представляет последнюю контрольную точку через объект _TransferContext_. Если объект _TransferCheckpoint_ сохранен до отмены или прерывания передачи, процесс можно возобновить из этой контрольной точки в срок течение семи дней. Передачу можно возобновить из любой контрольной точки, а не только из последней.
- Если процесс перемещения на клиенте будет завершен и перезапущен без применения контрольных точек:
  - если передача всех больших двоичных объектов еще не завершилась, процесс будет запущен снова;
  - если некоторые большие двоичные объекты уже были переданы, процесс будет запущен снова только для незавершенных больших двоичных объектов.
- Приостановка работы клиента приостанавливает и передачу данных.
- Механизм перемещения больших двоичных объектов прозрачно для клиента обрабатывает временные сбои. Например, применение регулирования к учетной записи хранения обычно не приводит к сбою передачи, а только снижает ее скорость.
- При передаче на стороне службы создается низкая нагрузка на ресурсы ЦП и памяти клиента, пропускную способность сети и сетевые подключения.

### <a name="async-transfer-characteristics"></a>Параметры асинхронной передачи

Вы можете вызвать метод _TransferManager.CopyDirectoryAsync()_ с параметром _CopyMethod.ServiceSideAsyncCopy_. С точки зрения клиента он работает так же, как и механизм синхронной передачи, но в его работе есть следующие важные отличия:

- скорость передачи значительно ниже, чем скорость синхронной передачи в аналогичных условиях (обычно не выше 10 МБ/с);
- передача будет продолжена даже при завершении клиентского процесса;
- поддерживаются контрольные точки, но возобновление передачи на основе объекта _TransferCheckpoint_ не возвращает процесс к этой контрольной точке, а продолжает его из текущего состояния.

### <a name="test-summary"></a>Итоги теста

Хранилище BLOB-объектов Azure поддерживает высокую скорость передачи данных при выполнении синхронной передачи на стороне службы. Использование этой возможности в приложениях .NET с помощью библиотеки перемещения данных выполняется очень просто. Экспорт в Управлении затратами позволяет менее чем за час надежно скопировать сотни гигабайт данных в учетную запись хранения, расположенную в любом регионе.

## <a name="next-steps"></a>Дальнейшие действия

- Изучите исходный код [библиотеки перемещения данных службы хранилища Microsoft Azure](https://github.com/Azure/azure-storage-net-data-movement).
- [Перенесите данные с помощью библиотеки перемещения данных.](../../storage/common/storage-use-data-movement-library.md)
- Изучите код [примера приложения AzureDmlBackup](https://github.com/markjbrown/AzureDmlBackup).
- Узнайте, как обеспечить [высокую пропускную способность при работе с Хранилищем BLOB-объектов Azure](https://azure.microsoft.com/blog/high-throughput-with-azure-blob-storage).