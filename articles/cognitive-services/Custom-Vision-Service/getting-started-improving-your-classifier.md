---
title: Улучшение модели Пользовательская служба визуального распознавания
titleSuffix: Azure Cognitive Services
description: В этой статье вы узнаете, как объем, качество и разнообразие данных могут улучшить качество модели в службе Пользовательское визуальное распознавание.
services: cognitive-services
author: PatrickFarley
manager: nitinme
ms.service: cognitive-services
ms.subservice: custom-vision
ms.topic: conceptual
ms.date: 02/09/2021
ms.author: pafarley
ms.custom: cog-serv-seo-aug-2020
ms.openlocfilehash: ae0112292994fdcf88e80abff8ab52e5971cb0ed
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/20/2021
ms.locfileid: "101731091"
---
# <a name="how-to-improve-your-custom-vision-model"></a>Как улучшить модель Пользовательское визуальное распознавание

В этом руководство вы узнаете, как улучшить качество модели Пользовательская служба визуального распознавания. Качество [классификатора](./getting-started-build-a-classifier.md) или [детектора объектов](./get-started-build-detector.md) зависит от объема, качества и разнообразных данных, которые вы предоставляете, и от того, как сбалансирован общий набор данных. Хорошая модель имеет сбалансированный набор данных для обучения, который отражает, что будет отправлено в него. Процесс создания такой модели является итеративным; обычно требуется несколько циклов обучения для достижения ожидаемых результатов.

Ниже приведен общий шаблон, помогающий обучить более точную модель:

1. Первый сеанс обучения
1. Добавьте дополнительные изображения и сбалансируйте данные. Повторите обучение.
1. Добавьте изображения с различным фоном, освещением, размером объекта, углом камеры и стилем. Повторите обучение.
1. Используйте новые изображения для тестирования прогноза.
1. Измените существующие обучающие данные в соответствии с результатами прогноза.

## <a name="prevent-overfitting"></a>Запретить перегонку

В некоторых случаях модель будет научиться создавать прогнозы на основе произвольных характеристик, общих для изображений. Например, при создании классификатора яблок и цитрусовых, имея изображения яблок в руках, а цитрусовых на белых тарелках, классификатор может сосредоточиться на руках и белых тарелках вместо яблок и цитрусовых.

![Изображение неожиданной классификации](./media/getting-started-improving-your-classifier/unexpected.png)

Чтобы устранить эту проблему, укажите изображения с различными углами, фоном, размером объекта, группами и другими вариантами. Следующие разделы посвящены этим концепциям.

## <a name="data-quantity"></a>Количество данных

Число обучающих изображений является наиболее важным фактором для набора данных. В качестве отправной точки рекомендуется использовать по крайней мере 50 изображений на метку. При меньшем числе изображений существует более высокий риск появления лжевзаимосвязей, и хотя показатели производительности могут указывать на хорошее качество модели, она может испытывать трудности при обработке реальных данных. 

## <a name="data-balance"></a>Сбалансированность данных

Важно также учесть относительный объем обучающих данных. Например, использование 500 изображений для одной метки и 50 — для другой приведет к созданию несбалансированного обучающего набора данных. В результате модель будет прогнозировать одну метку точнее, чем другую. Если оставить соотношение 1:2 между метками с наименьшим количеством изображений и метками с наибольшим количеством изображений, то скорее всего, вы увидите наилучший результат. Например, если наибольшим числом изображений для какой-либо метки является 500, то наименьшим числом изображений для другой метки должно быть не менее 250.

## <a name="data-variety"></a>Разнообразие данных

Предоставьте изображения, максимально похожие на конкретное содержимое, которое будет передаваться в классификатор при его использовании. В противном случае ваша модель может научиться создавать прогнозы на основе произвольных характеристик, общих для изображений. Например, при создании классификатора яблок и цитрусовых, имея изображения яблок в руках, а цитрусовых на белых тарелках, классификатор может сосредоточиться на руках и белых тарелках вместо яблок и цитрусовых.

![Изображение неожиданной классификации](./media/getting-started-improving-your-classifier/unexpected.png)

Чтобы устранить эту проблему, включите различные образы, чтобы обеспечить правильную подготовку модели. Некоторые из способов, с помощью которых можно сделать набор для обучения более разнообразным, приведены ниже.

* __Фон:__ Укажите изображения объекта перед различными фоновыми рисунками. Фотографии в естественном окружении лучше, чем фотографии на нейтральном фоне, так как они предоставляют больше информации для классификатора.

    ![Изображение примеров фонов](./media/getting-started-improving-your-classifier/background.png)

* __Освещение:__ Предоставление изображений с разнообразным освещением (то есть с помощью Flash, высокой экспозицией и т. д.), особенно если изображения, используемые для прогнозирования, имеют разное освещение. Полезно также использовать изображения с разнообразной насыщенностью, оттенком и яркостью.

    ![Изображение примеров освещения](./media/getting-started-improving-your-classifier/lighting.png)

* __Размер объекта:__ Укажите изображения, в которых объекты различаются по размеру и числу (например, фотографии с полукруглыми крупный планами и с одним «полукруглым»). Различные размеры объектов помогают классификатору делать общие выводы.

    ![Изображение примеров размеров](./media/getting-started-improving-your-classifier/size.png)

* __Угол камеры__. Предоставьте изображения объекта, снятые под разными углами. Кроме того, если все фотографии должны быть сделаны с помощью стационарных камер (например, камер видеонаблюдения), обязательно назначьте постоянным объектам в кадре разные метки. Это позволит избежать формирования лжевзаимосвязей &mdash; то есть интерпретации несвязанных объектов (например, фонарных столбов) как ключевого признака.

    ![Изображение примеров объектов под разными углами](./media/getting-started-improving-your-classifier/angle.png)

* __Стиль:__ Предоставьте изображения различных стилей одного класса (например, различные разновидности одного и того же типа фруктов). Тем не менее, если присутствуют изображения объектов совершенно разных стилей (например, Микки-Маус и реальная мышь), рекомендуется отметить их как отдельные классы, что позволит лучше представлять их различные признаки.

    ![Изображение примеров стилей](./media/getting-started-improving-your-classifier/style.png)

## <a name="negative-images-classifiers-only"></a>Отрицательные изображения (только классификаторы)

Если вы используете классификатор изображений, может потребоваться добавить _отрицательные примеры_ , чтобы сделать классификатор более точным. Отрицательные примеры — это изображения, которые не соответствуют ни одному из других тегов. Отправляя такие изображения, примените к ним специальную метку **Negative** (Негативные).

Средство обнаружения объектов обрабатывает отрицательные выборки автоматически, поскольку любые области изображения за пределами рисуемых ограничивающих прямоугольников считаются отрицательными.

> [!NOTE]
> Пользовательская служба визуального распознавания поддерживает автоматическое применение отрицательных примеров изображений. Например, когда вы компилируете классификатор для различения винограда и банана, то для фотографии ботинка он должен предоставить оценку 0 % по обеим категориям.
> 
> С другой стороны, в тех случаях, когда отрицательные изображения являются лишь вариацией изображений, используемых в обучении, вполне вероятно, что из-за большого сходства модель будет классифицировать отрицательные изображения как отмеченные классы. Например, если классификатору апельсина и грейпфрута предложить для сравнения изображение клементина, он может оценить его как апельсин, потому что многие черты клементина похожи на апельсины. Если отрицательные изображения имеют такую природу, мы рекомендуем создать один или несколько дополнительных тегов (например, **Другие**) и во время обучения пометить ими отрицательные изображения. Такие действия помогут модели лучше различать эти классы.

## <a name="consider-occlusion-and-truncation-object-detectors-only"></a>Рассмотрите возможность перекрытия и усечения (только для обнаружения объектов)

Если необходимо, чтобы средство обнаружения объектов определяло усеченные объекты (объект частично удаляется из образа) или объекты перекрыто (объект частично заблокирован другим объектом на изображении), необходимо включить обучающие изображения, охватывающие эти случаи.

> [!NOTE]
> Проблемы с объектами, перекрыто другими объектами, не следует путать с **пороговым значением перекрытия**, параметром для производительности модели оценки. Ползунок **порога перекрытия** на [веб-сайте пользовательское визуальное распознавание](https://customvision.ai) зависит от того, насколько прогнозируемый ограничивающий прямоугольник должен перекрываться с помощью настоящего ограничивающего прямоугольника, чтобы считаться правильным.

## <a name="use-prediction-images-for-further-training"></a>Использование прогнозных изображений для дальнейшего обучения

При использовании или тестировании модели путем отправки изображений в конечную точку прогнозирования Служба Пользовательское визуальное распознавание хранит эти образы. Затем их можно использовать для улучшения модели.

1. Чтобы просмотреть изображения, отправленные в модель, откройте [веб-страницу пользовательское визуальное распознавание](https://customvision.ai), перейдите к своему проекту и перейдите на вкладку __прогнозы__ . Представление по умолчанию показывает изображения из текущей итерации. Раскрывающееся меню __Iteration__ (Итерация) позволяет перейти к изображениям, переданным в предыдущих итерациях.

    ![Снимок экрана: вкладка "Predictions" (Прогнозы) с изображениями в представлении](./media/getting-started-improving-your-classifier/predictions.png)

2. Наведите указатель мыши на изображение, чтобы увидеть теги, прогнозируемые моделью. Изображения сортируются таким образом, чтобы в верхней части были перечислены те, которые могут привести к наибольшим улучшениям модели. Чтобы использовать другой метод сортировки, выберите соответствующий параметр в разделе __Sort__ (Сортировка). 

    Чтобы добавить изображение в существующий обучающий набор данных, выберите изображение, укажите правильные теги и нажмите кнопку __Save and close__ (Сохранить и закрыть). Выбранное изображение будет удалено из списка __Predictions__ (Прогнозы) и добавлено в набор обучающих изображений. Теперь его можно просмотреть на вкладке __Training Images__ (Изображения для обучения).

    ![Изображение страницы для управления тегами](./media/getting-started-improving-your-classifier/tag.png)

3. Затем с помощью кнопки " __обучение__ " переучить модель.

## <a name="visually-inspect-predictions"></a>Визуальная проверка прогнозов

Чтобы просмотреть прогнозы изображений, перейдите на вкладку __Training Images__ (Обучающие изображения), выберите предыдущую итерацию обучения в раскрывающемся меню **Iteration** (Итерация) и установите флажок для одного или нескольких тегов в разделе **Tags** (Теги). Теперь представление должно отобразить красные рамки вокруг каждого изображения, для которого модели не удалось правильно спрогнозировать заданный тег.

![Изображение журнала итераций](./media/getting-started-improving-your-classifier/iteration.png)

В некоторых случаях визуальная проверка позволяет заметить в этих ошибках закономерность и исправить ее, добавив дополнительные обучающие данные или изменив существующие данные. Например, классификатор яблок и лаймов может неправильно отметить все зеленые яблоки и обозначить их как лаймы. Чтобы устранить эту проблему, следует добавить и применить обучающие данные с отмеченными тегом изображениями зеленых яблок.

## <a name="next-steps"></a>Дальнейшие действия

В этом разделе вы узнали несколько способов повышения точности модели классификации пользовательских изображений или модели детектора объектов. Далее вы можете узнать, как программно тестировать изображения, отправляя их в API прогнозирования.

> [!div class="nextstepaction"]
> [Использование API прогнозирования](use-prediction-api.md)