---
title: Персонализатор. Обучение с подкреплением
titleSuffix: Azure Cognitive Services
description: Для улучшения предложений относительно ранжирования Персонализатор использует сведения о действиях и текущем контексте. Сведения об этих действиях и контексте — это атрибуты или свойства, которые называются функциями.
services: cognitive-services
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/07/2019
ms.openlocfilehash: 8b97221de4921e06ddfab610618f37683b990181
ms.sourcegitcommit: 772eb9c6684dd4864e0ba507945a83e48b8c16f0
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/19/2021
ms.locfileid: "87132744"
---
# <a name="what-is-reinforcement-learning"></a>Что такое обучение с подкреплением?

Обучение с подкреплением — это подход к машинному обучению, при котором поведение изучается путем получения обратной связи во время его использования.
 
Принципы работы обучения с подкреплением:

* обеспечение возможности или степени свободы для поведения, такого как принятие решения или осуществление выбора;
* предоставление контекстных сведений о среде и выборе;
* предоставление отзыва о том, насколько хорошо поведение достигает определенной цели.

Несмотря на то что существует множество подтипов и стилей обучения с подкреплением, в службе "Персонализатор" эта концепция работает так:

* Приложение предоставляет возможность отображения фрагмента содержимого из списка альтернативных вариантов.
* Оно также предоставляет информацию о каждом из альтернативных вариантов и контексте пользователя.
* Ваше приложение определяет _оценку вознаграждения_.

В отличие от других подходов к обучению с подкреплением, Персонализатор не требует симуляции для работы. Его алгоритмы обучения предназначены для реагирования на внешний мир (вместо управления им) и изучения каждой точки данных с пониманием того, что это уникальная возможность, которая требует времени и денег для создания, и что существует ненулевое сожаление (потеря возможного вознаграждения), если возникает неоптимальная производительность.

## <a name="what-type-of-reinforcement-learning-algorithms-does-personalizer-use"></a>Какой тип алгоритмов обучения с подкреплением использует служба "Персонализатор"?

Текущая версия службы "Персонализатор" использует **контекстные бандиты** — подход к обучению с подкреплением, который основан на принятии решений или выборе дискретных действий в данном контексте.

_Память принятия решений_ (модель, обученная для получения наилучшего возможного решения с учетом контекста) использует набор линейных моделей. Такой подход неоднократно демонстрировал результаты в бизнесе. Этот подход является проверенным отчасти благодаря быстрой обучаемости в реальном мире без необходимости использовать многовводное обучение, а также благодаря тому, что может дополнять контролируемые модели обучения и модели глубоких нейронных сетей.

Распределение трафика изучения и использования производится случайным образом в соответствии с процентом, установленным для изучения, а алгоритм изучения по умолчанию — эпсилон-жадный.

### <a name="history-of-contextual-bandits"></a>Журнал контекстных бандитов

Джон Лэнгфорд (John Langford) придумал название "контекстные бандиты" (Langford and Zhang [2007]) для описания гибкого подмножества обучения с подкреплением и работал над многочисленными научными публикациями, которые улучшают понимание того, как происходит обучение в этой парадигме.

* Beygelzimer et al. [2011]
* Dudík et al. [2011a,b]
* Agarwal et al. [2014, 2012]
* Beygelzimer and Langford [2009]
* Li et al. [2010]

Джон также написал несколько учебных пособий на такие темы, как совместное прогнозирование (ICML 2015), теория контекстных бандитов (NIPS 2013), активное обучение (ICML 2009) и границы эталонной сложности (ICML 2003)

## <a name="what-machine-learning-frameworks-does-personalizer-use"></a>Какие платформы машинного обучения использует служба "Персонализатор"?

В настоящее время как основу для машинного обучения Персонализатор использует [Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit/wiki). Эта платформа обеспечивает максимальную пропускную способность и минимальную задержку при ранжировании персонализации и обучении модели на всех событиях.

## <a name="references"></a>Ссылки

* [Принятие контекстных решений с низким техническим долгом](https://arxiv.org/abs/1606.03966)
* [Подход сокращения к справедливой классификации](https://arxiv.org/abs/1803.02453)
* [Эффективные контекстные бандиты в нестабильных мирах](https://arxiv.org/abs/1708.01799)
* [Прогноз потери остатков: подкреплением: обучение без добавочных отзывов](https://openreview.net/pdf?id=HJNMYceCW)
* [Инструкции по сопоставлению и визуальные наблюдения для действий в обучении с подкреплением](https://arxiv.org/abs/1704.08795)
* [Научись искать лучше, чем твой учитель](https://arxiv.org/abs/1502.02206)

## <a name="next-steps"></a>Дальнейшие действия

[Автономная оценка](concepts-offline-evaluation.md) 