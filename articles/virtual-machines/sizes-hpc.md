---
title: Размеры виртуальных машин Azure — HPC | Документация Майкрософт
description: Список различных размеров, доступных для высокопроизводительных вычислительных виртуальных машин в Azure. Сведения о количестве виртуальных ЦП, дисков данных и сетевых адаптеров, а также о пропускной способности хранилища и сети для размеров виртуальных машин этой серии.
author: vermagit
ms.service: virtual-machines
ms.subservice: vm-sizes-hpc
ms.topic: conceptual
ms.workload: infrastructure-services
ms.date: 03/19/2021
ms.author: amverma
ms.reviewer: jushiman
ms.openlocfilehash: a41dce28427db40dfd19879e4ada95add64009c3
ms.sourcegitcommit: 2c1b93301174fccea00798df08e08872f53f669c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/22/2021
ms.locfileid: "104772439"
---
# <a name="high-performance-computing-vm-sizes"></a>Размеры виртуальных машин высокопроизводительных вычислений

Виртуальные машины Azure серии H предназначены для обеспечения производительности, масштабируемости и экономичности для различных рабочих нагрузок HPC.

[Серия HBv3](hbv3-series.md) Виртуальные машины оптимизированы для приложений HPC, таких как жидкие динамические, явные и неявные анализы конечных элементов, моделирование погоды, пластового обработки, моделирование молекулярного и имитация RTL. HBv3 VMS до 120 AMD ЕПИК™ 7003-Series (Милан) ядер ЦП, 448 ГБ ОЗУ и без технологии Hyper-Threading. Виртуальные машины серии HBv3 также предоставляют 350 ГБ/с для пропускной способности памяти, до 32 МБ кэш-памяти третьего уровня на ядро, до 7 ГБ/с для процессора блочного устройства и тактовые частоты до 3,675 ГГц. 

Все виртуальные машины серии HBv3 200 Гбит/с в HDR InfiniBand из сети NVIDIA позволяют выполнять крупномасштабные рабочие нагрузки MPI. Эти виртуальные машины подключены в неблокирующем дереве FAT для обеспечения оптимальной и стабильной производительности RDMA. Структура HDR InfiniBand также поддерживает адаптивную маршрутизацию и динамический подключенный транспорт (ДКТ в дополнение к стандартным транспортам RC и обновления). Эти функции улучшают производительность, масштабируемость и согласованность приложений, и их использование настоятельно рекомендуется.

[Серия HBv2](hbv2-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью пропускной способности памяти, например "жидкие", "неограниченный" анализ элементов и симуляцию молекулярного. HBv2 VMS 120 ядра процессора AMD ЕПИК 7742, 4 ГБ ОЗУ на ядро ЦП и без одновременной многопоточности. Каждая виртуальная машина HBv2 предоставляет до 340 ГБ/с для пропускной способности памяти и до 4 операций FP64 вычислений.

HBv2 VMS 200 ГБ/с Mellanox HDR InfiniBand, а для виртуальных машин серии ХБ и HC — 100 ГБ/с Mellanox ЕДР InfiniBand. Каждый из этих типов виртуальных машин подключен в неблокирующем дереве FAT для обеспечения оптимальной и стабильной производительности RDMA. HBv2 виртуальные машины поддерживают адаптивную маршрутизацию и динамический подключенный транспорт (ДКТ в дополнение к стандартным транспортам RC и обновления). Эти функции улучшают производительность, масштабируемость и согласованность приложений, и их использование настоятельно рекомендуется.

[Серия ХБ](hb-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью пропускной способности памяти, например для жидкости Dynamics, явного анализа конечных элементов и моделирования погоды. ХБ VMS 60 ядра процессора AMD ЕПИК 7551, 4 ГБ ОЗУ на ядро ЦП и без технологии hypering. Платформа AMD ЕПИК предоставляет более 260 ГБ/с для пропускной способности памяти.

[Серия HC](hc-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью сжатых вычислений, например неявного анализа конечных элементов, молекулярное Dynamics и вычислительных химия. HC VMS 44 процессорных ядер Intel Xeon Platinum 8168, 8 ГБ ОЗУ на ядро ЦП и без технологии hypering. Платформа Intel Xeon Platinum поддерживает обширную экосистему программных средств Intel, например библиотеку Intel Math Kernel.

[Серия H](h-series.md) Виртуальные машины оптимизированы для приложений, которые управляются высокой частотой ЦП или большими объемами памяти на базовые требования. Виртуальные машины серии H с 8 или 16 процессоров Intel Xeon 3 2667 v3, 7 или 14 ГБ ОЗУ на ядро ЦП и без технологии Hyper-Threading. Функции серии H 56 ГБ/с Mellanox FDR InfiniBand в неблокирующей конфигурации дерева FAT для обеспечения постоянной производительности RDMA. Виртуальные машины серии H поддерживают Intel MPI 5. x и MS-MPI.

> [!NOTE]
> Все виртуальные машины серии HBv3, HBv2, ХБ и HC имеют эксклюзивный доступ к физическим серверам. На одном физическом сервере имеется только 1 виртуальная машина, и для этих ВИРТУАЛЬНЫХ машин не существует общей многопользовательской Организации с другими виртуальными машинами.

> [!NOTE]
> [Виртуальные машины A8 – A11](./sizes-previous-gen.md#a-series---compute-intensive-instances) будут списаны в 3/2021. Новые развертывания виртуальных машин с такими размерами теперь невозможно. Если у вас уже есть виртуальные машины, см. уведомления по электронной почте для дальнейших действий, включая переход на другие размеры виртуальных машин в [руководстве по миграции HPC](https://azure.microsoft.com/resources/hpc-migration-guide/).

## <a name="rdma-capable-instances"></a>Экземпляры с поддержкой RDMA

Большинство размеров виртуальных машин HPC имеют сетевой интерфейс для подключения удаленного доступа к памяти (RDMA). Выбранные размеры [серии N](./nc-series.md) , обозначенные r, также поддерживают RDMA. Этот интерфейс является дополнением к стандартному сетевому интерфейсу Azure Ethernet, доступному в других размерах виртуальных машин.

Этот дополнительный интерфейс позволяет экземплярам, поддерживающим RDMA, взаимодействовать через сеть InfiniBand (с геочастотой), работать с тарифными курсами для HBv3, HBv2, ЕДР и ставок хб, HC, NDv2 и FDR для H16r, H16mr и других виртуальных машин серии N с поддержкой RDMA. Эти возможности RDMA могут повысить масштабируемость и производительность приложений, основанных на интерфейсе передачи сообщений (MPI).

> [!NOTE]
> **Поддержка SR-IOV**. в Azure HPC в настоящее время существует два класса виртуальных машин в зависимости от того, включены ли они в SR-IOV для InfiniBand. В настоящее время почти все более новые виртуальные машины с поддержкой RDMA или InfiniBand в Azure включены в SR-IOV, кроме H16r, H16mr и NC24r.
> RDMA доступен только для сети InfiniBand (превышена) и поддерживается для всех виртуальных машин с поддержкой RDMA.
> Переработка IP-адресов поддерживается только на виртуальных машинах с поддержкой SR-IOV.
> RDMA не включен по сети Ethernet.

-  Обычно используются дистрибутивы Linux, такие как CENTOS, RHEL, Ubuntu, SUSE. Windows Server 2016 и более поздние версии поддерживаются на всех виртуальных машинах серии HPC. Windows Server 2012 R2 и Windows Server 2012 также поддерживаются на виртуальных машинах, не поддерживающих SR-IOV. Обратите внимание, что [Windows Server 2012 R2 не поддерживается в HBv2, так как размеры виртуальных машин с более чем 64 (виртуальными или физическими) ядрами](/windows-server/virtualization/hyper-v/supported-windows-guest-operating-systems-for-hyper-v-on-windows). Список поддерживаемых образов виртуальных машин в Marketplace и способ их настройки можно найти в разделе [образы виртуальных](./workloads/hpc/configure.md) машин. На страницах соответствующих размеров виртуальной машины также находится поддержка программного стека.

- **InfiniBand и Drivers** — на виртуальных машинах с поддержкой InfiniBand для включения RDMA требуются соответствующие драйверы. Список поддерживаемых образов виртуальных машин в Marketplace и способ их настройки можно найти в разделе [образы виртуальных](./workloads/hpc/configure.md) машин. См. также раздел [Включение InfiniBand](./workloads/hpc/enable-infiniband.md) для получения сведений о РАСШИРЕНИЯХ виртуальных машин или установке драйверов InfiniBand вручную.

- **MPI** . размеры виртуальных машин с поддержкой SR-IOV в Azure поддерживают почти любую версию MPI для использования с Mellanox офед. На виртуальных машинах, не использующих SR-IOV, поддерживаемые реализации MPI используют интерфейс Microsoft Network Direct (ND) для обмена данными между виртуальными машинами. Поэтому поддерживаются только Intel MPI 5. x и Microsoft MPI (MS-MPI) 2012 R2 или более поздних версий. Более поздние версии библиотеки среды выполнения Intel MPI могут быть несовместимыми с драйверами Azure RDMA. Дополнительные сведения о настройке MPI на виртуальных машинах HPC в Azure см. в статье [Настройка MPI для HPC](./workloads/hpc/setup-mpi.md) .

  > [!NOTE]
  > **Адресное пространство сети RDMA**: сеть RDMA в Azure резервирует адресное пространство 172.16.0.0/16. Чтобы выполнять приложения MPI в экземплярах, развернутых в виртуальной сети Azure, убедитесь, что адресное пространство виртуальной сети не пересекается с сетью RDMA.

## <a name="cluster-configuration-options"></a>Параметры конфигурации кластера

Azure предоставляет несколько вариантов создания кластеров виртуальных машин HPC, которые могут обмениваться данными с помощью сети RDMA, в том числе: 

- **Виртуальные машины**  . развертывайте виртуальные машины HPC с поддержкой RDMA в одном масштабируемом наборе или группе доступности (при использовании модели развертывания Azure Resource Manager). Если вы используете классическую модель развертывания, разверните виртуальные машины в одну облачную службу.

- **Масштабируемые наборы виртуальных машин** . в масштабируемом наборе виртуальных машин убедитесь, что развертывание выполняется в одну группу размещения для связи InfiniBand в масштабируемом наборе. Например, в шаблоне Resource Manager задайте значение `true` для свойства `singlePlacementGroup`. Обратите внимание, что максимальный размер масштабируемого набора, который может быть `singlePlacementGroup=true` включен в 100 виртуальных машин по умолчанию, ограничен. Если потребность в масштабе задания HPC превышает 100 виртуальных машин в одном клиенте, вы можете запросить увеличение, бесплатно [открыв запрос в службу поддержки клиентов](../azure-portal/supportability/how-to-create-azure-support-request.md) . Ограничение на количество виртуальных машин в одном масштабируемом наборе можно увеличить до 300. Обратите внимание, что при развертывании виртуальных машин с использованием групп доступности максимальное ограничение составляет 200 виртуальных машин на группу доступности.

  > [!NOTE]
  > **MPI между виртуальными машинами**. Если между виртуальными машинами требуется RDMA (например, с использованием MPI-соединения), убедитесь, что виртуальные машины находятся в одном масштабируемом наборе виртуальных машин или группе доступности.

- **Azure циклеклауд** . Создание кластера HPC с помощью [Azure циклеклауд](/azure/cyclecloud/) для выполнения заданий MPI.

- **Пакетная служба Azure** . Создайте пул [пакетной службы Azure](../batch/index.yml) для выполнения рабочих нагрузок MPI. Сведения об использовании экземпляров для ресурсоемких вычислений при запуске приложений MPI с использованием пакетной службы Azure см. в статье [Использование задач с несколькими экземплярами для запуска приложений с интерфейсом передачи сообщений в пакетной службе](../batch/batch-mpi.md).

- **Пакет**  -  Microsoft HPC [Пакет HPC](/powershell/high-performance-computing/overview) включает среду выполнения для MS-MPI, которая использует сеть Azure RDMA при развертывании на виртуальных машинах Linux с поддержкой RDMA. Примеры развертываний см. [в разделе Настройка кластера RDMA Linux с пакетом HPC для запуска приложений MPI](/powershell/high-performance-computing/hpcpack-linux-openfoam).

## <a name="deployment-considerations"></a>Рекомендации по развертыванию

- **Подписка Azure.** Чтобы развернуть большое число экземпляров для ресурсоемких вычислений, рекомендуем подписку с оплатой по мере использования или другие варианты покупки. Если вы используете [бесплатную учетную запись Azure](https://azure.microsoft.com/free/), вам доступно ограниченное количество вычислительных ядер Azure.

- **Цены и доступность** . Проверьте цены и [доступность](https://azure.microsoft.com/global-infrastructure/services/) [виртуальных машин](https://azure.microsoft.com/pricing/details/virtual-machines/linux/) по регионам Azure.

- **Квота ядер**. Вам может потребоваться увеличить стандартную квоту на число ядер в подписке Azure. Кроме того, количество ядер, которые можно развернуть для некоторых семейств размеров виртуальных машин (включая серию H), может быть ограничено условиями вашей подписки. Чтобы увеличить квоту, [отправьте запрос в службу поддержки](../azure-portal/supportability/how-to-create-azure-support-request.md). Это бесплатная услуга. (Ограничения по умолчанию могут быть разными в зависимости от категории подписки).

  > [!NOTE]
  > Если вам нужны ресурсы в очень большом объеме, обратитесь в службу поддержки Azure. Квоты Azure — это ограничения по кредитам, а не гарантированная емкость. Вне зависимости от квоты с вас будет взиматься плата только за используемые ядра.
  
- **Виртуальная сеть** — [виртуальная сеть](https://azure.microsoft.com/documentation/services/virtual-network/) Azure не требуется для использования ресурсоемких экземпляров. Но для нескольких развертываний вам потребуется по крайней мере облачная виртуальная сеть Azure или подключение типа "сеть — сеть", если нужен доступ к локальным ресурсам. При необходимости создайте виртуальную сеть, чтобы развернуть экземпляры. Добавление виртуальных машин для ресурсоемких вычислений в виртуальную сеть в территориальной группе не поддерживается.

- **Изменение размера** . из-за специального оборудования можно изменять только ресурсоемкие экземпляры в пределах одного семейства размеров (серии H или N). Например, можно изменить только размер виртуальной машины серии H (один размер из серии H на другой размер из этой же серии). Для некоторых виртуальных машин может потребоваться рассмотреть дополнительные рекомендации по поддержке драйвера InfiniBand и дисков NVMe.


## <a name="other-sizes"></a>Остальные размеры

- [Универсальные](sizes-general.md)
- [Оптимизированные для вычислений](sizes-compute.md)
- [Оптимизированные для памяти](sizes-memory.md)
- [Оптимизированные для хранилища](sizes-storage.md)
- [Оптимизированные для GPU](sizes-gpu.md)
- [Предыдущие поколения](sizes-previous-gen.md)

## <a name="next-steps"></a>Дальнейшие действия

- Узнайте больше о [настройке виртуальных машин](./workloads/hpc/configure.md), [включении INFINIBAND](./workloads/hpc/enable-infiniband.md), [настройке MPI](./workloads/hpc/setup-mpi.md) и оптимизации приложений HPC для [рабочих нагрузок](./workloads/hpc/overview.md)Azure в HPC.
- Ознакомьтесь с обзором [HBv3-Series](./workloads/hpc/hbv3-series-overview.md) и [описанием серии HC](./workloads/hpc/hc-series-overview.md).
- Ознакомьтесь с последними объявлениями, примерами рабочей нагрузки HPC и результатами производительности в [блогах сообщества разработчиков Azure](https://techcommunity.microsoft.com/t5/azure-compute/bg-p/AzureCompute).
- Сведения о более высоком уровне архитектурного представления выполнения рабочих нагрузок HPC см. в статье [Высокопроизводительные вычисления (HPC) в Azure](/azure/architecture/topics/high-performance-computing/).
