---
title: Размеры виртуальных машин Azure — HPC | Документация Майкрософт
description: Список различных размеров, доступных для высокопроизводительных вычислительных виртуальных машин в Azure. Сведения о количестве виртуальных ЦП, дисков данных и сетевых адаптеров, а также о пропускной способности хранилища и сети для размеров виртуальных машин этой серии.
author: vermagit
ms.service: virtual-machines
ms.subservice: hpc
ms.topic: conceptual
ms.workload: infrastructure-services
ms.date: 12/09/2020
ms.author: amverma
ms.reviewer: jushiman
ms.openlocfilehash: 0ccd7a2ff1d4948858e62e224f1376379b4335ec
ms.sourcegitcommit: b4647f06c0953435af3cb24baaf6d15a5a761a9c
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/02/2021
ms.locfileid: "101669151"
---
# <a name="high-performance-computing-vm-sizes"></a>Размеры виртуальных машин с высокой производительностью

Виртуальные машины Azure серии H предназначены для предоставления лидерам производительности, масштабируемости и экономичности для различных реальных рабочих нагрузок HPC.

[Серия HBv2](hbv2-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью пропускной способности памяти, например "жидкие", "неограниченный" анализ элементов и симуляцию молекулярного. HBv2 VMS 120 ядра процессора AMD ЕПИК 7742, 4 ГБ ОЗУ на ядро ЦП и без одновременной многопоточности. Каждая виртуальная машина HBv2 предоставляет до 340 ГБ/с для пропускной способности памяти и до 4 операций FP64 вычислений.

HBv2 VMS 200 ГБ/с Mellanox HDR InfiniBand, а для виртуальных машин серии ХБ и HC — 100 ГБ/с Mellanox ЕДР InfiniBand. Каждый из этих типов виртуальных машин подключен в неблокирующем дереве FAT для обеспечения оптимальной и стабильной производительности RDMA. HBv2 виртуальные машины поддерживают адаптивную маршрутизацию и динамический подключенный транспорт (ДКТ, дополнительно к стандартным транспортам RC и обновления). Эти функции улучшают производительность, масштабируемость и согласованность приложений, а их использование настоятельно рекомендуется.

[Серия ХБ](hb-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью пропускной способности памяти, например для жидкости Dynamics, явного анализа конечных элементов и моделирования погоды. ХБ VMS 60 ядра процессора AMD ЕПИК 7551, 4 ГБ ОЗУ на ядро ЦП и без технологии hypering. Платформа AMD ЕПИК предоставляет более 260 ГБ/с для пропускной способности памяти.

[Серия HC](hc-series.md) Виртуальные машины оптимизированы для приложений, управляемых с помощью сжатых вычислений, например неявного анализа конечных элементов, молекулярное Dynamics и вычислительных химия. HC VMS 44 процессорных ядер Intel Xeon Platinum 8168, 8 ГБ ОЗУ на ядро ЦП и без технологии hypering. Платформа Intel Xeon Platinum поддерживает обширную экосистему программных средств Intel, например библиотеку Intel Math Kernel.

[Серия H](h-series.md) Виртуальные машины оптимизированы для приложений, которые управляются высокой частотой ЦП или большими объемами памяти на базовые требования. Виртуальные машины серии H с 8 или 16 процессоров Intel Xeon 3 2667 v3, 7 или 14 ГБ ОЗУ на ядро ЦП и без технологии Hyper-Threading. Функции серии H 56 ГБ/с Mellanox FDR InfiniBand в неблокирующей конфигурации дерева FAT для обеспечения постоянной производительности RDMA. Виртуальные машины серии H поддерживают Intel MPI 5. x и MS-MPI.

> [!NOTE]
> Все виртуальные машины серии HBv2, ХБ и HC имеют эксклюзивный доступ к физическим серверам. На одном физическом сервере имеется только 1 виртуальная машина, и для этих ВИРТУАЛЬНЫХ машин не существует общей многопользовательской Организации с другими виртуальными машинами.

> [!NOTE]
> [Виртуальные машины A8 – A11](./sizes-previous-gen.md#a-series---compute-intensive-instances) планируется выпустить в 3/2021. Дополнительные сведения см. в разделе [Руководство по миграции HPC](https://azure.microsoft.com/resources/hpc-migration-guide/).

## <a name="rdma-capable-instances"></a>Экземпляры с поддержкой RDMA

Большинство размеров виртуальных машин HPC (HBv2, хб, HC, H16r, H16mr, A8 и A9) имеют сетевой интерфейс для подключения удаленного доступа к памяти (RDMA). Выбранные размеры [серии N](./nc-series.md) , обозначенные "r" (ND40rs_v2, ND24rs, NC24rs_v3, NC24rs_v2 и NC24r), также поддерживают RDMA. Этот интерфейс является дополнением к стандартному сетевому интерфейсу Azure Ethernet, доступному в других размерах виртуальных машин.

Этот интерфейс позволяет экземплярам, поддерживающим RDMA, обмениваться данными по сети InfiniBand (с геочастотой), работать с тарифами HDR для HBv2, ЕДР rates for хб, HC, NDv2, частотой FDR для H16r, H16mr и другими виртуальными машинами серии N, поддерживающими RDMA, и частотой QDR для виртуальных машин A8 и A9. Эти возможности RDMA позволяют увеличить масштабируемость и производительность определенных приложений с интерфейсом MPI.

> [!NOTE]
> В Azure HPC существует два класса виртуальных машин в зависимости от того, включены ли они в SR-IOV для InfiniBand. В настоящее время почти все более новые виртуальные машины с поддержкой RDMA или InfiniBand в Azure включены в SR-IOV, за исключением H16r, H16mr, NC24r, A8, A9.
> RDMA доступен только для сети InfiniBand (превышена) и поддерживается для всех виртуальных машин с поддержкой RDMA.
> Переработка IP-адресов поддерживается только на виртуальных машинах с поддержкой SR-IOV.
> RDMA не включен по сети Ethernet.

- **Операционная система** — Linux очень хорошо поддерживается для ВИРТУАЛЬНЫХ машин HPC; обычно используются дистрибутивов, такие как CentOS, RHEL, Ubuntu и SUSE. В отношении поддержки Windows Windows Server 2016 и более поздние версии поддерживаются на всех виртуальных машинах серии HPC. Windows Server 2012 R2, Windows Server 2012 также поддерживаются на виртуальных машинах, не использующих SR-IOV (H16r, H16mr, A8 и A9). Обратите внимание, что [Windows Server 2012 R2 не поддерживается на HBv2 и других виртуальных машинах с более чем 64 (виртуальными или физическими) ядрами](/windows-server/virtualization/hyper-v/supported-windows-guest-operating-systems-for-hyper-v-on-windows). Список поддерживаемых образов виртуальных машин в Marketplace и способ их настройки можно найти в разделе [образы виртуальных](./workloads/hpc/configure.md) машин.

- **InfiniBand и Drivers** — на виртуальных машинах с поддержкой InfiniBand для включения RDMA требуются соответствующие драйверы. В Linux для виртуальных машин с поддержкой SR-IOV и без SR-IOV образы виртуальных машин CentOS-HPC в Marketplace предварительно настроены с соответствующими драйверами. Образы виртуальных машин Ubuntu можно настроить с помощью правильных драйверов, выполнив приведенные [здесь инструкции](https://techcommunity.microsoft.com/t5/azure-compute/configuring-infiniband-for-ubuntu-hpc-and-gpu-vms/ba-p/1221351). Дополнительные сведения о готовых к использованию образах ОС Linux см. в статье [Настройка и оптимизация виртуальных машин для ОС Linux](./workloads/hpc/configure.md) .

   В Linux [расширение виртуальной машины инфинибанддриверлинукс](./extensions/hpc-compute-infiniband-linux.md) можно использовать для установки драйверов Mellanox офед и включения InfiniBand на виртуальных машинах серии H и N с поддержкой SR-IOV. Узнайте больше о включении InfiniBand на виртуальных машинах с поддержкой RDMA на [рабочих нагрузках HPC](./workloads/hpc/enable-infiniband.md).

   В Windows [расширение виртуальной машины инфинибанддривервиндовс](./extensions/hpc-compute-infiniband-windows.md) устанавливает драйверы Windows Network Direct (на виртуальных машинах без SR-IOV) или драйверы Mellanox офед (на виртуальных машинах SR-IOV) для подключения RDMA. В некоторых развертываниях экземпляров A8 и A9 расширение HpcVmDrivers добавляется автоматически. Обратите внимание, что расширение виртуальной машины HpcVmDrivers является устаревшим. Он не будет обновлен.

   Чтобы добавить в виртуальную машину расширение виртуальной машины, можно использовать командлеты [Azure PowerShell](/powershell/azure/). Дополнительные сведения см. в статье [Обзор расширений и компонентов виртуальной машины под управлением Windows](./extensions/overview.md). Вы также можете работать с расширениями для виртуальных машин, развернутых в рамках [классической модели развертывания](/previous-versions/azure/virtual-machines/windows/classic/agents-and-extensions-classic).

- **MPI** . размеры виртуальных машин с поддержкой SR-IOV в Azure поддерживают почти любую версию MPI для использования с Mellanox офед. На виртуальных машинах, не использующих SR-IOV, поддерживаемые реализации MPI используют интерфейс Microsoft Network Direct (ND) для обмена данными между виртуальными машинами. Таким образом, поддерживаются только версии Microsoft MPI (MS-MPI) 2012 R2 и более поздних версий и Intel MPI 5. x. Более поздние версии (2017, 2018) библиотеки Intel MPI Runtime могут быть несовместимыми с драйверами Azure RDMA. Дополнительные сведения о настройке MPI на виртуальных машинах HPC в Azure см. в статье [Настройка MPI для HPC](./workloads/hpc/setup-mpi.md) .

- **Адресное пространство сети RDMA.** Сеть RDMA в Azure резервирует адресное пространство 172.16.0.0/16. Чтобы выполнять приложения MPI в экземплярах, развернутых в виртуальной сети Azure, убедитесь, что адресное пространство виртуальной сети не пересекается с сетью RDMA.

## <a name="cluster-configuration-options"></a>Параметры конфигурации кластера

Azure предоставляет несколько вариантов для создания кластеров виртуальных машин для Windows HPC, которые могут взаимодействовать с помощью сети RDMA. 

- **Виртуальные машины**  . развертывайте виртуальные машины HPC с поддержкой RDMA в одном масштабируемом наборе или группе доступности (при использовании модели развертывания Azure Resource Manager). Если вы используете классическую модель развертывания, разверните виртуальные машины в одну облачную службу.

- **Масштабируемые наборы виртуальных машин** . в масштабируемом наборе виртуальных машин убедитесь, что развертывание выполняется в одну группу размещения для связи InfiniBand в масштабируемом наборе. Например, в шаблоне Resource Manager задайте значение `true` для свойства `singlePlacementGroup`. Обратите внимание, что максимальный размер масштабируемого набора, который можно установить с помощью свойства, ограничен `singlePlacementGroup` `true` на 100 виртуальных машинах по умолчанию. Если потребность в масштабе задания HPC превышает 100 виртуальных машин в одном клиенте, вы можете запросить увеличение, бесплатно [открыв запрос в службу поддержки клиентов](../azure-portal/supportability/how-to-create-azure-support-request.md) . Ограничение на количество виртуальных машин в одном масштабируемом наборе можно увеличить до 300. Обратите внимание, что при развертывании виртуальных машин с использованием групп доступности максимальное ограничение составляет 200 виртуальных машин на группу доступности.

- **MPI между виртуальными** машинами. Если между виртуальными машинами (например, с помощью MPI-соединения) требуется RDMA, убедитесь, что виртуальные машины находятся в одном масштабируемом наборе виртуальных машин или группе доступности.

- **Azure циклеклауд** . Создайте кластер HPC в [Azure циклеклауд](/azure/cyclecloud/) для запуска заданий MPI.

- **Пакетная служба Azure** . Создайте пул [пакетной службы Azure](../batch/index.yml) для выполнения рабочих нагрузок MPI. Сведения об использовании экземпляров для ресурсоемких вычислений при запуске приложений MPI с использованием пакетной службы Azure см. в статье [Использование задач с несколькими экземплярами для запуска приложений с интерфейсом передачи сообщений в пакетной службе](../batch/batch-mpi.md).

- **Пакет**  -  Microsoft HPC [Пакет HPC](/powershell/high-performance-computing/overview) включает среду выполнения для MS-MPI, которая использует сеть Azure RDMA при развертывании на виртуальных машинах Linux с поддержкой RDMA. Примеры развертываний см. [в разделе Настройка кластера RDMA Linux с пакетом HPC для запуска приложений MPI](/powershell/high-performance-computing/hpcpack-linux-openfoam).

## <a name="deployment-considerations"></a>Рекомендации по развертыванию

- **Подписка Azure.** Чтобы развернуть большое число экземпляров для ресурсоемких вычислений, рекомендуем подписку с оплатой по мере использования или другие варианты покупки. Если вы используете [бесплатную учетную запись Azure](https://azure.microsoft.com/free/), вам доступно ограниченное количество вычислительных ядер Azure.

- **Цены и доступность.** Виртуальные машины предлагаются только в ценовой категории уровня "Стандартный". Проверьте [доступность продуктов по регионам](https://azure.microsoft.com/global-infrastructure/services/) , чтобы узнать, в каких регионах Azure их можно использовать.

- **Квота ядер**. Вам может потребоваться увеличить стандартную квоту на число ядер в подписке Azure. Кроме того, количество ядер, которые можно развернуть для некоторых семейств размеров виртуальных машин (включая серию H), может быть ограничено условиями вашей подписки. Чтобы увеличить квоту, [отправьте запрос в службу поддержки](../azure-portal/supportability/how-to-create-azure-support-request.md). Это бесплатная услуга. (Ограничения по умолчанию могут быть разными в зависимости от категории подписки).

  > [!NOTE]
  > Если вам нужны ресурсы в очень большом объеме, обратитесь в службу поддержки Azure. Квоты Azure — это ограничения по кредитам, а не гарантированная емкость. Вне зависимости от квоты с вас будет взиматься плата только за используемые ядра.
  
- **Виртуальная сеть** — [виртуальная сеть](https://azure.microsoft.com/documentation/services/virtual-network/) Azure не требуется для использования ресурсоемких экземпляров. Но для нескольких развертываний вам потребуется по крайней мере облачная виртуальная сеть Azure или подключение типа "сеть — сеть", если нужен доступ к локальным ресурсам. При необходимости создайте виртуальную сеть, чтобы развернуть экземпляры. Добавление виртуальных машин для ресурсоемких вычислений в виртуальную сеть в территориальной группе не поддерживается.

- **Изменение размера** . из-за специального оборудования можно изменять только ресурсоемкие экземпляры в пределах одного семейства размеров (серии H или N). Например, можно изменить только размер виртуальной машины серии H (один размер из серии H на другой размер из этой же серии). Для некоторых виртуальных машин может потребоваться рассмотреть дополнительные рекомендации по поддержке драйвера InfiniBand и дисков NVMe.


## <a name="other-sizes"></a>Остальные размеры

- [Универсальные](sizes-general.md)
- [Оптимизированные для вычислений](sizes-compute.md)
- [Оптимизированные для памяти](sizes-memory.md)
- [Оптимизированные для хранилища](sizes-storage.md)
- [Оптимизированные для GPU](sizes-gpu.md)
- [Предыдущие поколения](sizes-previous-gen.md)

## <a name="next-steps"></a>Дальнейшие действия

- Узнайте больше о [настройке виртуальных машин](./workloads/hpc/configure.md), [включении INFINIBAND](./workloads/hpc/enable-infiniband.md), [настройке MPI](./workloads/hpc/setup-mpi.md) и оптимизации приложений HPC для [рабочих нагрузок](./workloads/hpc/overview.md)Azure в HPC.
- Ознакомьтесь с последними объявлениями и некоторыми примерами HPC, а также результатами в [блогах технического сообщества службы вычислений](https://techcommunity.microsoft.com/t5/azure-compute/bg-p/AzureCompute).
- Сведения о более высоком уровне архитектурного представления выполнения рабочих нагрузок HPC см. в статье [Высокопроизводительные вычисления (HPC) в Azure](/azure/architecture/topics/high-performance-computing/).
