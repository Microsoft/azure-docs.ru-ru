---
title: Оптимизация производительности виртуальных машин Azure серии Lsv2 — хранилище
description: Узнайте, как оптимизировать производительность решения на виртуальных машинах серии Lsv2 с использованием примера Linux.
services: virtual-machines-linux
author: laurenhughes
ms.service: virtual-machines
ms-subservice: vm-sizes-storage
ms.collection: linux
ms.topic: conceptual
ms.tgt_pltfrm: vm-linux
ms.workload: infrastructure-services
ms.date: 08/05/2019
ms.author: joelpell
ms.openlocfilehash: 99349654bb01f368a2a3a84c4ecc01f248b25175
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 03/20/2021
ms.locfileid: "102552768"
---
# <a name="optimize-performance-on-the-lsv2-series-linux-virtual-machines"></a>Оптимизация производительности виртуальных машин Linux серии Lsv2

Виртуальные машины серии Lsv2 поддерживают разнообразные рабочие нагрузки, требующие высокой скорости ввода-вывода и пропускной способности локального хранилища для самых разных применений и отраслей.  Серия Lsv2 идеально подходит для больших данных, баз данных SQL и NoSQL, хранилищ и больших транзакционных баз данных, в том числе Cassandra, MongoDB, Cloudera и Redis.

Архитектура виртуальных машин серии Lsv2 позволяет эффективно использовать процессор AMD EPYC™ 7551, чтобы обеспечить максимальную производительность процессора, памяти, устройств NVMe и виртуальных машин. Работая с партнерами в Linux, доступны несколько сборок Azure Marketplace, которые оптимизированы для производительности серии Lsv2 и в настоящее время включают:

- Ubuntu 18.04
- Ubuntu 16.04
- RHEL 8,0
- Debian 9
- Debian 10

В этой статье приводятся советы и рекомендации по обеспечению максимальной производительности рабочих нагрузок и приложений, рассчитанных на виртуальные машины. Сведения на этой странице будут постоянно обновляться по мере добавления более оптимизированных образов Lsv2 в Azure Marketplace.

## <a name="amd-eypc-chipset-architecture"></a>Архитектура микросхем AMD EYPC™

Виртуальные машины серии Lsv2 используют серверные процессоры AMD EYPC™ на основе микроархитектуры Zen. Платформа AMD разработала Infinity Fabric (IF) для EYPC™ в качестве масштабируемого соединения для своей модели NUMA, которая может использоваться для коммуникаций на кристалле, в пакете и в нескольких пакетах. По сравнению с QPI (Quick-Path Interconnect) и UPI (Ultra-Path Interconnect), которые используются в современных процессорах Intel с монолитным кристаллом, архитектура AMD с маленькими кристаллами и множеством NUMA может повысить производительность, но имеет недостатки. Фактическое влияние ограничений пропускной способности памяти и задержки может различаться в зависимости от типа выполняемых рабочих нагрузок.

## <a name="tips-to-maximize-performance"></a>Советы по повышению производительности

* Если вы отправляете настраиваемую Гуестос Linux для рабочей нагрузки, обратите внимание, что функция ускорения сети будет **отключена** по умолчанию. Если вы планируете включить функцию ускорения сети, включите ее во время создания виртуальной машины, чтобы обеспечить наилучшую производительность.

* Оборудование, которое лежит в основе виртуальных машин серии Lsv2, использует устройства NVMe с восемью парами очередей ввода-вывода (QP). Каждая очередь ввода-вывода устройств NVMe на самом деле представляет собой пару: очередь отправки и очередь выполнения. Драйвер NVMe настроен на оптимизацию использования этих восьми пар очередей ввода-вывода путем распределения операций ввода-вывода в расписании циклического перебора. Чтобы получить максимальную производительность, запускайте восемь заданий на каждом устройстве.

* Старайтесь не смешивать команды администрирования NVMe (например, запрос информации NVMe SMART и т. д.) с командами ввода-вывода NVMe во время активных рабочих нагрузок. Устройства Lsv2 NVMe поддерживаются технологией Hyper-V NVMe Direct, которая переключается в режим медленной работы каждый раз, когда команды администрирования NVMe ожидают выполнения. В таких случаях пользователи Lsv2 заметят значительное снижение производительности операций ввода-вывода NVMe.

* Пользователи Lsv2 не должны полагаться на сведения NUMA устройства (все 0), полученные от виртуальной машины о дисках данных, чтобы принять решение о сходстве NUMA для приложений. Для повышения производительности рекомендуется распределить рабочие нагрузки между процессорами, если это возможно.

* Максимальная поддерживаемая глубина очереди в паре очередей ввода-вывода для устройства NVMe на виртуальной машине Lsv2 — 1024 (лимит Amazon i3 — 32). Пользователи Lsv2 должны ограничить свои (искусственные) рабочие нагрузки сравнения производительности до глубины 1024 или менее, чтобы избежать запуска события полной очереди, что может снизить производительность.

## <a name="utilizing-local-nvme-storage"></a>Использование локального хранилища NVMe

Локальное хранилище на диске NVMe 1,92 ТБ на всех виртуальных машинах Lsv2 является эфемерным. Во время стандартной корректной перезагрузки виртуальной машины данные на локальном диске NVMe сохраняются. Данные не будут сохраняться в NVMe, если виртуальная машина повторно развернута, освобождена или удалена. Данные не сохраняются, если другая проблема вызывает неработоспособность виртуальной машины или оборудования, на котором она запущена. В этом случае любые данные на старом узле будут безопасно удалены.

Также возможны случаи, когда виртуальную машину необходимо переместить на другой хост-компьютер, например во время планового обслуживания. Плановое обслуживание и некоторые сбои оборудования можно запланировать с помощью функции [Запланированные события](scheduled-events.md). Запланированные события предоставляют актуальную информацию о плановом обслуживании и восстановлении.

Если запланированное событие обслуживания требует повторного создания виртуальной машины на новом узле с пустыми локальными дисками, необходимо повторно синхронизировать данные (при этом любые данные на старом узле также будут безопасно удалены). Это происходит потому, что виртуальные машины серии Lsv2 в настоящее время не поддерживают динамическую миграцию на локальном диске NVMe.

Существует два режима планового обслуживания.

### <a name="standard-vm-customer-controlled-maintenance"></a>Стандартное обслуживание виртуальной машины, контролируемое клиентом

- Виртуальная машина перемещается на обновленный узел в течение 30-дневного периода.
- Данные локального хранилища Lsv2 могут быть потеряны, поэтому рекомендуется выполнять резервное копирование данных до события.

### <a name="automatic-maintenance"></a>Автоматическое обслуживание

- Происходит, если клиент не выполняет обслуживание, контролируемое клиентом, или в случае аварийных процедур, таких как ошибка нулевого дня.
- Предназначено для сохранения данных клиентов, но существует небольшой риск заморозить или перезапустить виртуальную машину.
- Данные локального хранилища Lsv2 могут быть потеряны, поэтому рекомендуется выполнять резервное копирование данных до события.

Для всех ближайших событий обслуживания используйте контролируемый процесс, чтобы выбрать наиболее удобное для обновления время. Перед событием можно создать резервные копии данных в хранилище класса Premium. После события обслуживания можно вернуть данные в обновленное локальное хранилище NVMe на виртуальных машинах Lsv2.

Ниже перечислены сценарии хранения данных на локальных дисках NVMe:

- Виртуальная машина работает и работоспособна.
- Виртуальная машина перезагружается на месте (вами или Azure).
- Виртуальная машина приостановлена (остановлена без отмены выделения).
- Большая часть операций планового обслуживания.

Ниже перечислены сценарии безопасного удаления данных для защиты клиента.

- Виртуальная машина повторно развертывается, останавливается (освобождается) или удаляется (вами).
- Виртуальная машина становится неработоспособной и должна переместиться на другой узел из-за проблемы с оборудованием.
- Некоторые операции планового обслуживания, требующие повторного выделения виртуальной машины на другом узле для обслуживания.

Дополнительные сведения о вариантах резервного копирования данных в локальном хранилище см. в статье [Резервное копирование и аварийное восстановление для дисков IaaS Azure](../backup-and-disaster-recovery-for-azure-iaas-disks.md).

## <a name="frequently-asked-questions"></a>Часто задаваемые вопросы

* **Как начать развертывание виртуальных машин серии Lsv2?**  
   Как и для любой другой виртуальной машины, используйте [Портал](quick-create-portal.md), [Azure CLI](quick-create-cli.md) или [PowerShell](quick-create-powershell.md).

* **Сбой одного диска NVMe приведет к сбою всех виртуальных машин на узле?**  
   Если на узле оборудования обнаружен сбой диска, оборудование не работает. В этом случае все виртуальные машины на узле автоматически освобождаются и перемещаются на работоспособный узел. Для виртуальных машин серии Lsv2 это означает, что данные клиента на узле, где произошел сбой, также безопасно удаляются и должны быть созданы повторно клиентом на новом узле. Как уже отмечалось, до того, как динамическая миграция станет доступной в Lsv2, данные на узле, на котором происходит сбой, будут заблаговременно перемещаться вместе с виртуальными машинами по мере их передачи на другой узел.

* **Нужно ли вносить какие – либо изменения для rq_affinity производительности?**  
   Параметр rq_affinity является незначительной коррекцией при использовании абсолютного максимального числа операций ввода-вывода в секунду. Когда все остальное работает правильно, попытайтесь установить rq_affinity равным 0, чтобы увидеть, отличается ли это.

* **Нужно ли изменять параметры blk_mq?**  
   RHEL/CentOS 7. x автоматически использует BLK-MQ для устройств NVMe. Изменения конфигурации и параметры не требуются. Параметр scsi_mod. use _blk_mq предназначен только для SCSI и использовался во время предварительной версии Lsv2, так как устройства NVMe были видны на гостевых виртуальных машинах как устройства SCSI. В настоящее время устройства NVMe видимы в виде устройств NVMe, поэтому параметр SCSI BLK-MQ не важен.

* **Нужно ли изменить "FIO"?**  
   Чтобы получить максимальное число операций ввода-вывода в секунду с помощью средства измерения производительности, например "FIO" в размерах виртуальных машин L64v2 и L80v2, установите для параметра "rq_affinity" значение 0 на каждом устройстве NVMe.  Например, эта командная строка установит значение "rq_affinity" равным нулю для всех 10 устройств NVMe на виртуальной машине L80v2:

   ```console
   for i in `seq 0 9`; do echo 0 >/sys/block/nvme${i}n1/queue/rq_affinity; done
   ```

   Также обратите внимание, что оптимальная производительность достигается, когда ввод-вывод выполняется непосредственно на каждом необработанном устройстве NVMe без секционирования, без файловых систем, без конфигурации RAID 0 и т. д. Перед запуском тестового сеанса убедитесь, что конфигурация находится в известном состоянии "новое/чистое", выполнив `blkdiscard` на каждом устройстве NVMe.
   
## <a name="next-steps"></a>Дальнейшие действия

* См. спецификации для всех [виртуальных машин, оптимизированных для производительности хранилища](../sizes-storage.md), в Azure.
